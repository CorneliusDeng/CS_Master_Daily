{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10107,
     "status": "ok",
     "timestamp": 1650012696153,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "-_L_dhppItIk",
    "outputId": "6c1eecf0-fd72-4d13-ad05-192463636129"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import rl_utils\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "! git clone https://github.com/boyu-ai/ma-gym.git\n",
    "import sys\n",
    "sys.path.append(\"./ma-gym\")\n",
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdZSfYc7ItIn"
   },
   "outputs": [],
   "source": [
    "# 定义策略网络，其输入是状态，输出是动作的概率分布\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "# 定义价值网络，其输入是某个状态，输出则是状态的价值估计\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# PPO算法，采用截断方式\n",
    "class PPO_Clip:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) # 策略网络\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device) # 价值网络\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) # 策略网络优化器\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) # 价值网络优化器\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda # GAE（Generalized Advantage Estimation）中的参数，用于计算优势估计\n",
    "        self.eps = eps  # PPO_Clip中的超参数，表示进行截断的范围\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state) # 策略网络，对输入状态进行前向传播，得到动作的概率分布\n",
    "        action_dist = torch.distributions.Categorical(probs) # 生成动作的概率分布对象\n",
    "        action = action_dist.sample() # 从概率分布中采样得到动作\n",
    "        return action.item() # 返回采样动作\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -dones) # 计算时序差分目标，并且考虑终止状态的影响\n",
    "        td_delta = td_target - self.critic(states) # 时序差分误差\n",
    "        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device) # 计算优势估计\n",
    "        # 根据选择的动作从策略网络输出的概率分布中提取对应的对数概率，即计算在旧策略下选择当前动作的概率。在PPO算法中，需要计算新策略和旧策略的比例，用于计算损失函数\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        # 通过策略网络对状态进行前向传播，得到动作的概率分布。即当前策略下选择各个动作的概率\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions)) \n",
    "        ratio = torch.exp(log_probs - old_log_probs) # 新策略和旧策略的比例\n",
    "        # 计算第一项截断项（surr1）和第二项截断项（surr2），用于计算策略网络的损失函数\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage \n",
    "        # 基于截断项的最小值，计算策略网络的损失函数\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2)) \n",
    "        # 计算价值网络的损失函数，即计算当前状态下的价值估计与时序差分目标的均方误差\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        # 执行梯度优化的步骤：梯度清零、反向传播、参数更新\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2805926,
     "status": "ok",
     "timestamp": 1649963248923,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "t8FsMOFPItIp",
    "outputId": "2f453795-508c-45ff-91e1-fb8b81eb5e9c"
   },
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 100000\n",
    "hidden_dim = 64\n",
    "gamma = 0.99\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "team_size = 2\n",
    "grid_size = (15, 15)\n",
    "# 创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env = Combat(grid_shape=grid_size, n_agents=team_size, n_opponents=team_size)\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "action_dim = env.action_space[0].n\n",
    "# 两个智能体共享同一个策略\n",
    "agent = PPO_Clip(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device)\n",
    "\n",
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                # 从策略网络中获取动作\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                # 执行动作并获取下一个状态、奖励、是否终止等信息\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                # 将状态、动作、下一个状态、奖励、是否终止等信息添加到字典中\n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                transition_dict_1['rewards'].append(r[0] + 100 if info['win'] else r[0] - 0.1)\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(r[1] + 100 if info['win'] else r[1] - 0.1)\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                s = next_s\n",
    "                terminal = all(done)\n",
    "            # 将胜利或失败的标志添加到列表中\n",
    "            win_list.append(1 if info[\"win\"] else 0)\n",
    "            # 更新策略和价值网络的参数\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1649963248923,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "OT2mwoZdItIq",
    "outputId": "6ea70d1d-bb28-456e-ffca-fe4f8106e0b8"
   },
   "outputs": [],
   "source": [
    "win_array = np.array(win_list)\n",
    "# 每100条轨迹取一次平均\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "plt.plot(episodes_list, win_array)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Win rate')\n",
    "plt.title('IPPO on Combat')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第20章-多智能体强化学习入门.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
