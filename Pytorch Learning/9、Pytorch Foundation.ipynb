{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1：创建数据\n",
    "Tensors张量是一种特殊的数据结构，它和数组还有矩阵十分相似。在Pytorch中，Tensors可以在gpu或其他专用硬件上运行来加速计算之外，其他用法类似Numpy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# 直接从数据创建\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data.shape\n",
    "# 全为1\n",
    "x_ones = torch.ones_like(x_data)  # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "# 全为0\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)  # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "# 查看tensor类型\n",
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2：自动梯度计算\n",
    "\n",
    "在Pytorch中可以使用tensor进行计算，并最终可以从计算得到的tensor计算损失，并进行梯度信息。在Pytorch中主要关注正向传播的计算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(2, 2, requires_grad=True)\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=float, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)  # y就多了一个AddBackward\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)  # z多了MulBackward\n",
    "print(out)  # out多了MeanBackward\n",
    "\n",
    "# 计算公式：out = 0.25 ((x+2) * (x+2) * 3)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3：拟合曲线\n",
    "\n",
    "接下来我们将尝试使用Pytorch拟合一条曲线，我们首先的创建待你和的参数，并加载待训练的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要计算得到的参数\n",
    "w = torch.ones(1, requires_grad=True)\n",
    "b = torch.ones(1, requires_grad=True)\n",
    "\n",
    "# 数据\n",
    "x_tensor = torch.from_numpy(x)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "# 目标模型\n",
    "# y = wx + b\n",
    "# 定义损失\n",
    "def mse(label, pred):\n",
    "    diff = label - pred\n",
    "    return torch.sqrt((diff ** 2).mean())\n",
    "\n",
    "pred = x_tensor * w + b\n",
    "loss = mse(y_tensor, pred)\n",
    "# 执行20次参数更新\n",
    "for _ in range(20):\n",
    "\n",
    "    # 重新定义一下，梯度清空\n",
    "    w = w.clone().detach().requires_grad_(True)\n",
    "    b = b.clone().detach().requires_grad_(True)\n",
    "\n",
    "    # 正向传播\n",
    "    pred = x_tensor * w + b\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = mse(y_tensor, pred)\n",
    "    print(loss)\n",
    "\n",
    "    # 计算梯度\n",
    "    loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4：加载MNIST数据集\n",
    "\n",
    "torchvision是pytorch官方的用于视觉任务的库，这里我们加载最常见的MNST数据集。当然也可以自定义数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision 是pytorch官方的用于视觉任务的库\n",
    "import torchvision.datasets as datasets  # 内置的数据集读取\n",
    "import torchvision.transforms as transforms  # 内置的对图像的操作\n",
    "from torch import nn\n",
    "\n",
    "# 组合多个数据变换\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 图片的读取，（图片、类别）\n",
    "# 28 * 28，数字0、1、2、3、4、5、6、7、8、9\n",
    "dataset1 = datasets.MNIST('./', train=True, download=True)\n",
    "dataset2 = datasets.MNIST('./', train=False, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=40)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2,  batch_size=40)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5：定义全连接网络\n",
    "\n",
    "接下来我们定义网络结构，由于是图像分类任务，因此我们的节点维度使用逐步降低的定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Flatten(), # 将维度转换为二维\n",
    "    nn.Linear(784, 256), # 全连接层\n",
    "    nn.ReLU(), # 激活函数\n",
    "    nn.Linear(256, 10) # 全连接层\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤6：训练卷积神经网络\n",
    "\n",
    "如果需要定义CNN网络，则可以参考如下的方式。先定义卷积层，然后定义全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 28, 28)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    Reshape(),\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤7：模型训练\n",
    "\n",
    "定义训练时的超参数，如batch size、学习率和优化器。这里可以自定定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, lr, num_epochs = 256, 0.1, 10\n",
    "loss = nn.CrossEntropyLoss()\n",
    "updater = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train_acc, test_acc = [], []\n",
    "# epoch维度训练\n",
    "for _ in range(num_epochs):\n",
    "    acc = 0\n",
    "    \n",
    "    # 读取训练数据\n",
    "    # batch维度训练\n",
    "    for data in train_loader:\n",
    "        pred = net(data[0]) # 正向传播\n",
    "        pred_loss = loss(pred, data[1]) # 计算损失\n",
    "        updater.zero_grad() # 清空梯度\n",
    "        pred_loss.backward()  # 梯度计算\n",
    "        updater.step()  # 参数更新\n",
    "        \n",
    "        # 累计准确样本个数\n",
    "        acc += (pred.argmax(1) == data[1]).sum()\n",
    "    \n",
    "    # 计算准确率\n",
    "    acc = acc.float() / len(train_loader.dataset)\n",
    "    train_acc.append(acc)\n",
    "    \n",
    "    # 读取验证数据\n",
    "    # batch维度预测\n",
    "    with torch.no_grad(): # 不记录梯度信息\n",
    "        acc = 0\n",
    "        for data in test_loader:\n",
    "            pred = net(data[0]) # 正向传播\n",
    "            pred_loss = loss(pred, data[1]) # 累计梯度\n",
    "            acc += (pred.argmax(1) == data[1]).sum() # 累计准确样本个数\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = acc.float() / len(test_loader.dataset)\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    print(train_acc[-1], test_acc[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodjob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
