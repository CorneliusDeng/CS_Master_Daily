# 强化学习基础

## 强化学习概述

强化学习（reinforcement learning，RL）讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。强化学习由两部分组成：智能体和环境。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个动作 （action），这个动作也称为决策（decision）。然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

<img src="https://datawhalechina.github.io/easy-rl/img/ch1/1.1.png" style="zoom: 50%;" />

在监督学习过程中，有两个假设：

- 输入的数据（标注的数据）都应是没有关联的。因为如果输入的数据有关联，学习器（learner）是不好学习的。
- 需要告诉学习器正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。

在强化学习中，监督学习的两个假设其实都不能得到满足。强化学习之所以困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境中学习，延迟奖励（delayed reward）使得训练网络非常困难。

- 强化学习和监督学习的区别：
  - 强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的
  - 学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来最多的奖励，只能通过不停地尝试来发现最有利的动作
  - 智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探 索和利用之间进行权衡，这也是在监督学习里面没有的情况
  - 在强化学习过程中，没有非常强的监督者（supervisor），只有奖励信号（reward signal），并且奖励信号是延迟的，即环境会在很久以后告诉我们之前我们采取的动作到底是不是有效的。因为我们没有得 到即时反馈，所以智能体使用强化学习来学习就非常困难。当我们采取一个动作后，如果我们使用监督学习，我们就可以立刻获得一个指导，比如，我们现在采取了一个错误的动作，正确的动作应该是什么。而在强化学习里面，环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么。而且更困难的是，它可能是在一两分钟过后告诉我们这个动作是错误的
- 通过与监督学习的比较，总结出强化学习的一些特征：
  - 强化学习会试错探索，它通过探索环境来获取对环境的理解
  - 强化学习智能体会从环境里面获得延迟的奖励
  - 在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性
  - 智能体的动作会影响它随后得到的数据，这一点是非常重要的。在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升

深度强化学习（deep reinforcemet learning，DRL） = 深度学习 + 强化学习

标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15a.png)

深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个 端到端训练（end-to-end training）的过程。我们不需要设计特征，直接输入状 态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15b.png)

## 序列决策 (sequential decision making)

强化学习研究的问题是智能体与环境交互的问题，智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。

奖励是由环境给的一种标量的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。

在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测、动作、奖励的序列：$H_t=o_1,a_1,r_1,\cdots,o_t,a_t,r_t$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个环境的状态看成关于这个历史的函数：$S_t=f(H_t)$

环境有自己的函数 $s_t^e=f^e(H_t)$ 来更新状态，在智能体的内部也有一个函数 $s_t^a=f^a(H_t)$ 来更新状态。当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中，$o_t=s_t^e=s_t^a$

但是有一种情况是智能体得到的观测并不能包含环境运作的所有状态，因为在强化学习的设定里面，环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。 在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道 部分观测值。

## 动作空间（action space）

不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间。

## 智能体的组成成分和类型

部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP) 是一个马尔可夫决策过程的泛化。POMDP 依然具对应一个强化学习 agent，它可能有一个或多个如下的组成成分：

- 策略（policy）。智能体会用策略来选取下一步的动作
- 价值函数（value function）。我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进 入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
- 模型（model）。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。策略可分为两种：随机性策略和确定性策略。随机性策略（stochastic policy）就是 π 函数，即 $\pi(a|s)=p(a_t=a|s_t=s)$。输入一个状态，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。确定性策略（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^*=\underset{a}{arg\,max}(a|s)$。

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。

价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数定义为：
$$
V_\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s], \; \forall s \in S
$$
期望 $\mathbb{E}_\pi$ 的下标是 $\pi$ 函数，$\pi$ 函数的值可反映在我们使用策略 $\pi$ 的时候，到底可以得到多少奖励。

我们还有一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作。其定义为
$$
Q_\pi(s,a)=\mathbb{E}_\pi[G_t|s_t=s,a_t=a]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s,a_t=a]
$$
模型决定了下一步的状态，而下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即 $p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a)$，奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即 $R(s,a)=\mathbb{E}[r_{t+1}|s_t=s,a_t=a]$。

当我们有了策略、价值函数和模型3个组成部分后，就形成了一个马尔可夫决策过程（Markov decision process）

根据智能体学习的事物不同，我们可以把智能体进行归类。**基于价值的智能体（value-based agent）**显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。**基于策略的智能体（policy-based agent）**直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了**演员-评论员智能体（actor-critic agent）**。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。

对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。 基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

针对是否需要对真实环境建模，强化学习可以分为**有模型（model-based）**强化学习和**免模型（model-free）**强化学习。有模型强化学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。

## 学习与规划

学习（learning）和规划（planning）是序列决策的两个基本问题。 在强化学习中，环境初始时是未知的，智能体不知道环境如何工作，它通过不断地与环境交互，逐渐改进策略。

在规划中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

## 探索和利用

在强化学习里面，探索和利用是两个很核心的问题。探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。 利用即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。 在刚开始的时候，强化学习智能体不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。

与监督学习任务不同，强化学习任务的最终奖励在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。需注意的是，即便在这样的简单情形下，强化学习仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。

想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。然而，更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。

实际上，单步强化学习任务对应于一个理论模型，即**K臂赌博机（K-armed bandit）**。 K臂赌博机也被称为**多臂赌博机（multi-armed bandit）** 。K臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。 若仅为获知每个摇臂的期望奖励，则可采用**仅探索（exploration-only）法**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。若仅为执行奖励最大的动作，则可采用**仅利用（exploitation-only）法**：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖励，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖励，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖励最大化。事实上，探索（估计摇臂的优劣）和利用（选择当前最优摇臂)这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方，这就是强化学习所面临的**探索-利用窘境（exploration-exploitation dilemma）**。显然，想要累积奖励最大，则必须在探索与利用之间达成较好的折中。



# 多臂老虎机

## 问题介绍

在多臂老虎机（multi-armed bandit，MAB）问题中，有一个拥有 $K$ 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布  $R$。

<img src="https://hrl.boyuai.com/static/640.8908144b.png" style="zoom:50%;" />

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 $r$。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作 $T$ 次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

- 多臂老虎机问题可以表示为一个元组 $(A,R)$，其中
  - $A$ 为动作集合，其中一个动作表示拉动一个拉杆。若多臂老虎机一共有 $K$ 根根拉杆，那么动作空间就是集合 $\{a_1,\cdots,a_K\}$
  - $R$ 为奖励概率分布，拉动每一根拉杆的动作 $a$ 都对应一个奖励概率分布 $R(r|a)$，不同拉杆的奖励分布通常是不同的

假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间步 $T$ 内累积的奖励: $max\sum^T_{t=1}r_t,\;r_t\sim R(\cdot|a_t)$。其中 $a_t$ 表示在第时间步拉动某一拉杆的动作，$r_t$ 表示动作 $a_t$ 获得的奖励

对于每一个动作 $a$，我们定义其期望奖励为 $Q(a)=E_{r\sim R(\cdot|a)}[r]$。于是，至少存在一根拉杆，它的期望奖励不小于拉动其他任意一根拉杆，我们将该最优期望奖励表示为 $Q^*=max_{a\in A}Q(a)$。为了更加直观、方便地观察拉动一根拉杆的期望奖励离最优拉杆期望奖励的差距，引入**懊悔**（regret）概念。懊悔定义为拉动当前拉杆的动作 $a$ 与最优拉杆的期望奖励差，即$R(a)=Q^*-Q(a)$。**累积懊悔**（cumulative regret）即操作 $T$ 次拉杆后累积的懊悔总量，对于一次完整的 $T$ 步决策 $\{a_1,a_2,\cdots,a_T\}$，累积懊悔为 $\sigma_R=\sum^T_{t=1}R(a_t)$。MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。

------

为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望，其算法流程如下所示：

- 对于 $\forall a\in A$，初始化计数器 $N(a)=0$ 和期望奖励估值 $\widehat Q(a)=0$
- for t = 1 to T, do
  - 选取某根拉杆，该动作记为 $a_t$
  - 得到奖励 $r_t$
  - 更新计数器: $N(a_t) = N(a_t) + 1$
  - 更新期望奖励估值: $\widehat Q(a_t)= \widehat Q(a_t)+\frac{1}{N(a_t)}[r_t-\widehat Q(a_t)]$
- end for

------

以上 for 循环中的第四步如此更新估值，是因为这样可以进行增量式的期望更新，公式如下：
$$
\begin{align}
Q_k 
& = \frac{1}{k}\sum_{i=1}^k r_i \\
& = \frac{1}{k}(r_k + \sum_{i=1}^{k-1}r_i) \\
& = \frac{1}{k}[r_k+(k-1)Q_{k-1}] \\
& = \frac{1}{k}(r_k+kQ_{k-1}-Q_{k-1}) \\
& = Q_{k-1} + \frac{1}{k}(r_k-Q_{k-1})
\end{align}
$$
如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为 $O(n)$。而采用增量式更新，时间复杂度和空间复杂度均为 $O(1)$。

## 探索与利用的平衡

还没有一个策略告诉我们应该采取哪个动作，即拉动哪根拉杆，所以接下来考虑如何设计一个策略。

例如，一个最简单的策略就是一直采取第一个动作，但这就非常依赖运气的好坏。如果运气绝佳，可能拉动的刚好是能获得最大期望奖励的拉杆，即最优拉杆；但如果运气很糟糕，获得的就有可能是最小的期望奖励。在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。**探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。例如，对于一个 10 臂老虎机，我们只拉动过其中 3 根拉杆，接下来就一直拉动这 3 根拉杆中期望奖励最大的那根拉杆，但很有可能期望奖励最大的拉杆在剩下的 7 根当中，即使我们对 10 根拉杆各自都尝试了 20 次，发现 5 号拉杆的经验期望奖励是最高的，但仍然存在着微小的概率—另一根 6 号拉杆的真实期望奖励是比 5 号拉杆更高的。

于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如 $\epsilon$-贪婪算法、上置信界算法和汤普森采样算法等。

**Code is available at [K-armed bandit](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/K-armed%20bandit.ipynb)**

## $\epsilon$-贪婪算法

完全贪婪算法即在每一时刻采取期望奖励估值最大的动作（拉动拉杆），这就是纯粹的利用，而没有探索，所以我们通常需要对完全贪婪算法进行一些修改，其中比较经典的一种方法为 $\epsilon$-Greedy 算法。$\epsilon$-Greedy 算法在完全贪婪算法的基础上添加了噪声，每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索），公式如下：
$$
a_t = 
\begin{cases}
arg\; max_{a\in A}\widehat Q(a), & \text {采样概率：} 1-\epsilon \\
\text{从A中随机选择}, & \text{采样概率：}\epsilon
\end{cases}
$$
随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。

所以在 $\epsilon$-贪婪算法的具体实现中，我们可以令$\epsilon$  随时间衰减，即探索的概率将会不断降低。但是请注意，$\epsilon$不会在有限的步数内衰减至 0，因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。

实验证明，随时间做反比例衰减的 $\epsilon$-贪婪算法能够使累积懊悔与时间步的关系变成**次线性**（sublinear）的，这明显优于固定 $\epsilon$ 值的$\epsilon$ -贪婪算法

## 上置信界算法

设想这样一种情况：对于一台双臂老虎机，其中第一根拉杆只被拉动过一次，得到的奖励为 ；第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。这时你会怎么做？或许你会进一步尝试拉动第一根拉杆，从而更加确定其奖励分布。这种思路主要是基于不确定性，因为此时第一根拉杆只被拉动过一次，它的不确定性很高。一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。

我们在此引入不确定性度量 $U(a)$，它会随着一个动作被尝试次数的增加而减小。我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。

**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**（Hoeffding's inequality）。在霍夫丁不等式中，令 $X_1,\cdots,X_n$ 为$n$个独立同分布的随机变量，取值范围为 $[0,1]$，其经验期望为 $\overline{x}_n=\frac{1}{n}\sum^n_{j=1}X_j$，则有 $P\{E[X] \geq \overline{x}_n+u\}\leq e^{-2nu^2}$

现在我们将霍夫丁不等式运用于多臂老虎机问题中。将 $\widehat{Q}_t(a)$ 代入 $\overline{x}_t$，不等式中的参数 $u=\widehat{U}_t(a)$ 代表不确定性度量。给定一个概率 $p=e^{-2N_t(a)U_t(a)^2}$ ，根据上述不等式，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 至少以概率 $1-p$ 成立。当 $p$ 很小时，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 就以很大概率成立，$\widehat{Q}_t(a)+\widehat{U}_t(a)$ 便是期望奖励上界。此时，上置信界算法便选取期望奖励上界最大的动作，即 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+\widehat{U}(a)]$。其中 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)}}$ 。因此，设定一个概率 $p$ 后，就可以计算相应的不确定性度量 $\widehat{U}_t(a)$ 了。更直观地说，UCB 算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 $p$ 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

在具体的实现过程中，设置 $p = \frac{1}{t}$，并且在分母中为拉动每根拉杆的次数加上常数 1，以免出现分母为 0 的情形。即此时 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)+1}}$。同时，设定一个系数 $c$ 来控制不确定性的比重，此时 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+c \cdot \widehat{U}(a)]$

## 汤普森采样算法

MAB 中还有一种经典算法——**汤普森采样**（Thompson sampling），先假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作 $a$ 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。

了解了汤普森采样算法的基本思路后，我们需要解决另一个问题：怎样得到当前每个动作 $a$ 的奖励概率分布并且在过程中进行更新？在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择了 $k$ 次，其中 $m_1$ 次奖励为 1， $m_2$ 次奖励为 0，则该拉杆的奖励服从参数为 $m_1+1,m_2+1$ 的 Beta 分布。

## 总结

通过实验我们可以得到以下结论： $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。



# 马尔可夫决策过程

与多臂老虎机问题不同，**马尔可夫决策过程**（Markov decision process，MDP）包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

**Code is available at [MDP](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Markov%20decision%20process.ipynb)**

## 马尔可夫过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于 $t$ 时刻之前的状态。我们将已知历史信息 $S_1,\cdots,S_t$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示成 $P(S_{t+1}|S_1,\cdots,S_t)$

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为 $P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\cdots,S_t)$。也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然  $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。我们通常用元组 $<S,P>$ 描述一个马尔可夫过程，其中 $S$ 是有限数量的状态集合，$P$ 是**状态转移矩阵**（state transition matrix）。假设一共有 $n$ 个状态，此时 $S=\{s_1,s_2,\cdots,s_n\}$。状态转移矩阵 $P$ 定义了所有状态对之间的转移概率，即
$$
\boldsymbol{P}=\left(\begin{array}{cccc}
    p\left(s_{1} \mid s_{1}\right) & p\left(s_{2} \mid s_{1}\right) & \ldots & p\left(s_{n} \mid s_{1}\right) \\
    p\left(s_{1} \mid s_{2}\right) & p\left(s_{2} \mid s_{2}\right) & \ldots & p\left(s_{n} \mid s_{2}\right) \\
    \vdots & \vdots & \ddots & \vdots \\
    p\left(s_{1} \mid s_{n}\right) & p\left(s_{2} \mid s_{n}\right) & \ldots & p\left(s_{n} \mid s_{n}\right)
    \end{array}\right)
$$
矩阵 $P$  中的第 i 行和第 j 列元素 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率，我们称 $P(s'|s)$ 为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵 $P$ 的每一行的和为 1。

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**（episode），这个步骤也被叫做**采样**（sampling）。生成序列的概率和状态转移矩阵有关。

## 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$，就可以得到**马尔可夫奖励过程**（Markov reward process）。一个马尔可夫奖励过程由 $<S,P,r,\gamma>$ 构成，各个组成元素的含义如下所示：

- $S$ 是有限状态的集合
- $P$ 是状态转移矩阵
- $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 转移到该状态时可以获得奖励的期望
- $\gamma$ 是折扣因子（discount factor），$\gamma$ 的取值范围为 $[0,1]$。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励。

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报** $G_t$（Return），公式如下：
$$
G_t=R_t+\gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k}
$$
在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成 $V(s)=E[G_t|S_t=s]$，展开为：
$$
\begin{align}
V(s)
& = E[G_t|S_t=s] \\
& = E[R_t+\gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots |S_t=s] \\
& = E[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\cdots)|S_t=s] \\
& = E[R_t+\gamma G_{t+1}|S_t=s] \\
& = E[R_t+\gamma V(S_{t+1})|S_t=s]
\end{align}
$$
在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即 $E[R_t|S_t=s]=r(s)$；另一方面，等式中剩余部分 $E[\gamma V(S_{t+1})|S_t=s]$ 可以根据从状态 $s$ 出发的转移概率得到，即可以得到
$$
V(s)=r(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')
$$
上式就是马尔可夫奖励过程中的**贝尔曼方程**（Bellman equation），对每一个状态都成立。贝尔曼方程就是当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德 贝尔曼（Richard Bellman）而得名 ，也叫作“动态规划方程”。

若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S=\{s_1,s_2,\cdots,s_n\}$，将所有状态的价值表示成一个列向量 $V=[V(s_1),V(s_2),\cdots,V(s_n)]^T$，同理，将奖励函数写成一个列向量 $R=[r(s_1),r(s_2),\cdots,r(s_n)]^T$。于是可以将贝尔曼方程写成矩阵的形式：
$$
V = R+\gamma PV \\

\left(\begin{array}{c}
    V\left(s_{1}\right) \\
    V\left(s_{2}\right) \\
    \vdots \\
    V\left(s_{N}\right)
    \end{array}\right)=\left(\begin{array}{c}
    R\left(s_{1}\right) \\
    R\left(s_{2}\right) \\
    \vdots \\
    R\left(s_{N}\right)
    \end{array}\right)+\gamma\left(\begin{array}{cccc}
    p\left(s_{1} \mid s_{1}\right) & p\left(s_{2} \mid s_{1}\right) & \ldots & p\left(s_{n} \mid s_{1}\right) \\
    p\left(s_{1} \mid s_{2}\right) & p\left(s_{2} \mid s_{2}\right) & \ldots & p\left(s_{n} \mid s_{2}\right) \\
    \vdots & \vdots & \ddots & \vdots \\
    p\left(s_{1} \mid s_{n}\right) & p\left(s_{2} \mid s_{n}\right) & \ldots & p\left(s_{n} \mid s_{n}\right)
    \end{array}\right)\left(\begin{array}{c}
    V\left(s_{1}\right) \\
    V\left(s_{2}\right) \\
    \vdots \\
    V\left(s_{n}\right)
    \end{array}\right) \\
    
\text{把贝尔曼方程写成矩阵形式后，可以直接求解} \\
\begin{aligned}
V &= R+ \gamma PV \\
IV &= R+ \gamma PV \\
(I-\gamma P)V &= R \\
V &= (I-\gamma P)^{-1} R
\end{aligned}
$$
以上解析解的计算复杂度是 $O(n^3)$，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用**动态规划**（dynamic programming）算法、**蒙特卡洛方法**（Monte-Carlo method）和**时序差分**（temporal difference）。

## 马尔可夫决策过程

马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程，而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**（Markov decision process，MDP）。我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。马尔可夫决策过程由元组 $<S,A,P,r,\gamma>$构成，其中：

- $S$ 是有限状态的集合
- $A$ 是动作的集合
- $P(s'|s,a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s'$ 的概率
- $r(s,a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$，在奖励函数只取决于状态 $s$ 时，则退化为 $r(s)$
- $\gamma$ 是折扣因子

MDP 与 MRP 非常相像，主要区别为 MDP 中的状态转移函数和奖励函数都比 MRP 多了动作 $a$ 作为自变量。注意，在上面 MDP 的定义中，我们不再使用类似 MRP 定义中的状态转移矩阵方式，而是直接表示成了状态转移函数。这样做一是因为此时状态转移与动作也有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义，例如，如果状态集合不是有限的，就无法用数组表示，但仍然可以用状态转移函数表示。

不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。例如，一艘小船在大海中随着水流自由飘荡的过程就是一个马尔可夫奖励过程，它如果凭借运气漂到了一个目的地，就能获得比较大的奖励；如果有个水手在控制着这条船往哪个方向前进，就可以主动选择前往目的地获得比较大的奖励。马尔可夫决策过程是一个与时间相关的不断进行的过程，在智能体和环境 MDP 之间存在一个不断交互的过程。

它们之间的交互如下图所示，智能体根据当前状态 $S_t$ 选择动作 $A_t$；对于状态 $S_t$ 和动作 $A_t$，MDP 根据奖励函数和状态转移函数得到 $S_{t+1}$ 和 $R_t$ 并反馈给智能体。智能体的目标是最大化得到的累计奖励。智能体根据当前状态从动作的集合 $A$ 中选择一个动作的函数，被称为策略。

<img src="https://hrl.boyuai.com/static/rl-process.723b4a67.png" style="zoom:67%;" />

智能体的**策略**（Policy）通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_t=a|S_t=s)$ 是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率。当一个策略是**确定性策略**（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；当一个策略是**随机性策略**（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。

用 $V^{\pi}(s)$ 表示在 MDP 中基于策略 $\pi$ 的**状态价值函数**（state-value function），定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为：$V^{\pi}(s)=E_{\pi}[G_t|S_t=s]$

不同于 MRP，在 MDP 中，由于动作的存在，额外定义一个**动作价值函数**（action-value function），用 $Q^{\pi}(s,a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报：$Q^{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]$

状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果：$V^{\pi}(s)=\sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)$

使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：$Q^{\pi}(s,a)=r(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V^{\pi}(s')$

通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）：
$$
\begin{align}
V^{\pi}(s)
& = E_{\pi}[G_t|S_t=s] \\
& = E_{\pi}[R_t+\gamma V^{\pi}(S_{t+1})|S_t=s] \\
& = \sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')] \\

Q^{\pi}(s,a)
& = E_{\pi}[G_t|S_t=s,A_t=a] \\
& = E_{\pi}[R_t+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
& = r(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi (a'|s')Q^{\pi}(s',a')
\end{align}
$$
价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的！

如果想要计算某 MDP 下，一个策略 $\pi$ 的状态价值函数。我们现在有的工具是 MRP 的解析解方法。于是，一个很自然的想法是：给定一个 MDP 和一个策略 $\pi$ ，我们是否可以将其转化为一个 MRP？答案是肯定的。我们可以将策略的动作选择进行**边缘化**（marginalization)，就可以得到没有动作的 MRP 了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个 MRP 在该状态下的奖励，即：$r'(s)=\sum_{a \in A}\pi(a|s)r(s,a)$

同理，我们计算采取动作的概率与使 $s$ 转移到 $s'$ 的概率的乘积，再将这些乘积相加，其和就是一个 MRP 的状态从 $s$ 转移到 $s'$ 的概率：$P'(s'|s)=\sum_{a \in A}\pi(a|s)P(s'|s,a)$

于是，我们构建得到了一个 MRP: $<S,P',r',\gamma>$。根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。

## 蒙特卡洛方法

**蒙特卡洛方法**（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。如下图所示的正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。

$\frac{\text{圆的面积}}{\text{正方形的面积}}=\frac{\text{圆中点的个数}}{\text{正方形中点的个数}}$<img src="https://hrl.boyuai.com/static/mc.c89f09b0.png" style="zoom:50%;" />

一个状态的价值是它的期望回报，那么一个很直观的想法就是用蒙特卡洛策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望，公式如下：$V^{\pi}(s)=E_{\pi}[G_t|S_t=s] \approx \frac{1}{N}\sum_{i=1}^NG_t^{(i)}$

在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略 $\pi$ 从状态 $s$ 开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示：

- 使用策略 $\pi$ 采样若干条序列：$s_0^{(i)} \stackrel{a_0^{{(i)}}}{\longrightarrow} r_0^{(i)},\;s_1^{(i)} \stackrel{a_1^{{(i)}}}{\longrightarrow} r_1^{(i)},\;s_2^{(i)} \stackrel{a_2^{{(i)}}}{\longrightarrow}\cdots \stackrel{a_{T-1}^{{(i)}}}{\longrightarrow}r_{T-1}^{(i)},\;s_T^{(i)}$
- 对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作
  - 更新状态 $s$ 的计数器 $N(s)\leftarrow N(s)+1$
  - 更新状态 $s$ 的总回报 $M(s) \leftarrow M(s)+G_t$
- 每一个状态的价值被估计为回报的平均值 $V(s)=\frac{M(s)}{N(s)}$

根据大数定律，当 $N(s)\to \infty$，有 $V(s)\to V^{\pi}(s)$ 。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态 $s$ 和对应回报 $G$ ，进行如下计算：

- $N(s)\leftarrow N(s)+1$
- $V(s) \leftarrow V(s)+\frac{1}{N(s)}(G-V(s))$ 

实验证明，使用蒙特卡洛方法估计得到的状态价值和使用 MRP 解析解得到的状态价值是很接近的。

## 占用度量

易知，不同策略的价值函数是不一样的，这是因为对于同一个 MDP，不同策略会访问到的状态的概率分布是不同的。因此我们需要理解不同策略会使智能体访问到不同概率分布的状态这个事实，这会影响到策略的价值函数。

定义 MDP 的初始状态分布为 $v_0(s)$，用 $P^{\pi}_t(s)$ 表示采取策略 $\pi$ 使得智能体在 $t$ 时刻状态为 $s$ 的概率，所以有 $P^{\pi}_0(s)=v_0(s)$，然后就可以定义一个策略的**状态访问分布**（state visitation distribution）：$v^{\pi}(s)=(1-\gamma)\sum_{t=0}^{\infty}\gamma ^tP_t^{\pi}(s)$。

其中，$1-\gamma$ 是用来使得概率加和为 1 的归一化因子。状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：$v^{\pi}(s')=(1-\gamma)v_0(s')+\gamma\int{P(s'|s,a)\pi(a|s)v^{\pi}(s)dsda}$

此外，还可以定义策略的**占用度量**（occupancy measure）：$\rho^{\pi}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma ^tP_t^{\pi}(s)\pi(a,s)$

它表示动作状态对 $(s,a)$ 被访问到的概率。二者之间存在如下关系：$\rho^{\pi}(s,a)=v^{\pi}(s)\pi(a,s)$

- 进一步得出如下两个定理:
  - **定理 1**：智能体分别以策略 $\pi_1$ 和 $\pi_2$ 和同一个 MDP 交互得到的占用度量 $\rho^{\pi_1}(s,a)$ 和 $\rho^{\pi_2}(s,a)$ 满足：$\rho^{\pi_1} = \rho^{\pi_2}\Longleftrightarrow \pi_1=\pi_2$
  - **定理 2**：给定一合法占用度量 $\rho$，可生成该占用度量的唯一策略是 $\pi_\rho=\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')}$

注意：以上提到的“合法”占用度量是指存在一个策略使智能体与 MDP 交互产生的状态动作对被访问到的概率。

实验证明，不同策略对于同一个状态动作对的占用度量是不一样的。

## 最优策略

强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。

首先定义策略之间的偏序关系：当且仅当对于任意的状态 $s$ 都有 $V^{\pi}(s)\geq V^{\pi'}(s)$，记 $\pi > \pi'$

于是在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是**最优策略**（optimal policy）。最优策略可能有很多个，都将其表示为 $\pi^*(s)$

最优策略都有相同的状态价值函数，称之为**最优状态价值函数**，表示为：$V^*(s)=\underset{\pi}{max}V^{\pi}(s), \forall s \in S$

同理，定义**最优动作价值函数**：$Q^*(s,a)=\underset{\pi}{max}Q^{\pi}(s,a), \forall s \in S, \forall a \in A$

为了使 $Q^{\pi}(s,a)$ 最大，我们需要在当前的状态动作对 $(s,a)$ 之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：$Q^*(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^*(s')$

这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：$V^*(s)=\underset{a\in A}{max}Q^*(s,a)$

根据 $V^*(s)$ 和 $Q^*(s,a)$ 的关系，得到**贝尔曼最优方程**（Bellman optimality equation）：
$$
V^*(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^*(s')\} \\
Q^*(s,a)=r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)\underset{a'\in A}{max}Q^*(s',a')
$$


# 动态规划算法

**动态规划**（dynamic programming）是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

利用用动态规划的思想来可以求解在马尔可夫决策过程中的最优策略。

基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，**策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的**。

**Code is available at [DP](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Dynamic%20programming.ipynb)**

使用策略迭代和价值迭代来求解**悬崖漫步**（Cliff Walking）这个环境中的最优策略。悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。

## 策略迭代算法

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 策略评估

策略评估这一过程用来计算一个策略的状态价值函数。

由贝尔曼期望方程 $V^{\pi}(s)=\sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')]$

其中，$\pi(a|s)$ 是策略 $\pi$ 在状态 $s$ 下采取动作 $a$ 的概率。可以看到，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。

更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即 $V^{k+1}(s)=\sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{k}(s')]$

我们可以选定任意初始值 $V^0$。根据贝尔曼期望方程，可以得知 $V^k=V^{\pi}$  是以上更新公式的一个**不动点**（fixed point）。事实上，可以证明当 $k\to \infty$ 时，序列 $\{V^k\}$ 会收敛到 $V^{\pi}$，所以可以据此来计算得到一个策略的状态价值函数。

可以看到，由于需要不断做贝尔曼期望方程迭代，策略评估其实会耗费很大的计算代价。在实际的实现过程中，如果某一轮 $max_{s\in S}|V^{k+1}(s)-V^k(s)|$ 的值非常小，可以提前结束策略评估。这样做可以提升效率，并且得到的价值也非常接近真实的价值。

### 策略提升

使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。

假设此时对于策略 $\pi$， 我们已经知道其价值 $V^{\pi}$ ，也就是知道了在策略 $\pi$ 下从每一个状态 $s$ 出发最终得到的期望回报。

假设智能体在状态 $s$ 下采取动作 $a$，之后的动作依旧遵循策略 $\pi$，此时得到的期望回报其实就是动作价值 $Q^{\pi}(s,a)$。如果有 $Q^{\pi}(s,a) > V^{\pi}(s)$ ，则说明在状态 $s$ 下采取动作 $a$ 会比原来的策略 $\pi(a|s)$ 得到更高的期望回报。

以上假设只是针对一个状态，现在假设存在一个确定性策略 $\pi'$，在任意一个状态 $s$ 下，都满足：$Q^{\pi}(s,\pi'(s)) > V^{\pi}(s)$ 。于是在任意状态 $s$ 下，有 $V^{\pi'}(s) \geq V^{\pi}(s)$

这便是**策略提升定理**（policy improvement theorem）。于是我们可以直接贪心地在每一个状态选择动作价值最大的动作，也就是
$$
\pi'(s)=arg\;\underset{a}{max}Q^{\pi}(s,a)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')\}
$$
我们发现构造的贪心策略 $\pi'$ 满足策略提升定理的条件，所以策略 $\pi'$ 能够比策略 $\pi$ 更好或者至少与其一样好。这个根据贪心法选取动作从而得到新的策略的过程称为策略提升。当策略提升之后得到的策略 $\pi'$ 和之前的策略 $\pi$ 一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi'$ 就是最优策略。

### 策略迭代算法

总体来说，策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略。

结合策略评估和策略提升，得到以下策略迭代算法：

- 随机初始化策略 $\pi(s)$ 和价值函数 $V(s)$
- while $\Delta > \theta$ do:
  - $\Delta \leftarrow 0$
  - 对于每一个状态 $s \in S$:
    - $v \leftarrow V(s)$
    - $V(s)\leftarrow r(s,\pi(s))+\gamma \sum_{s'}p(s'|s,\pi(s))V(s')$
    - $\Delta \leftarrow max(\Delta,|v-V(s)|)$
- end while
- $\pi_{old}\leftarrow \pi$
- 对于每一个状态 $s \in S$:
  - $\pi(s)\leftarrow arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$
- 若 $\pi_{old}= \pi$, 则停止算法并返回 $V$ 和 $\pi$ ; 否则转到策略评估循环

## 价值迭代算法

从策略迭代算法的实验结果可以发现，策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。

可能出现这样的情况：虽然状态价值函数还没有收敛，但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略。

价值迭代算法就是只在策略评估中进行一轮价值更新，然后直接根据更新后的价值进行策略提升。它可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数。

价值迭代可以看成一种动态规划过程，它利用的是贝尔曼最优方程：$V^*(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^*(s')\}$

将其写成迭代更新的方式为：$V^{k+1}(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^k(s')\}$

价值迭代便是按照以上更新方式进行的。等到 $V^{k+1}$ 和 $V^{k}$ 相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数 $V^*$。然后利用 $\pi(s)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{k+1}(s')\}$，从中恢复出最优策略即可。

价值迭代算法流程如下：

- 随机初始化价值函数 $V(s)$
- while $\Delta > \theta$ do:
  - $\Delta \leftarrow 0$
  - 对于每一个状态 $s \in S$:
    - $v \leftarrow V(s)$
    - $V(s)\leftarrow \underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$
    - $\Delta \leftarrow max(\Delta,|v-V(s)|)$
- end while
- 返回一个确定性策略 $\pi(s)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$

实验结果说明，解决同样的训练任务，价值迭代总共进行了数十轮，而策略迭代中的策略评估总共进行了数百轮，价值迭代中的循环次数远少于策略迭代。



# 时序差分算法

动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。

但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习**（model-free reinforcement learning）。

不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。

无模型的强化学习中有两大经典算法：Sarsa 和 Q-learning，它们都是基于**时序差分**（temporal difference，TD）的强化学习算法。

此外，还存在一组概念：在线策略学习和离线策略学习。通常来说，在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。

**Code is available at [TD](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Temporal%20difference.ipynb)**

## 时序差分方法

时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。

蒙特卡洛方法对价值函数的增量更新方式：$V(s_t) \leftarrow V(s_t)+\alpha(G_t-V(s_t))$ 。将原始的 $\frac{1}{N(s)}$ 替换成了 $\alpha$，表示对价值估计更新的步长。可以将 $\alpha$ 取为一个常数，此时更新方式不再像蒙特卡洛方法那样严格地取期望。

蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报 $G_t$，而时序差分方法只需要当前步结束即可进行计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：
$$
V(s_t) \leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]
$$
其中 $r_t+\gamma V(s_{t+1})-V(s_t)$ 通常被称为**时序差分**（temporal difference，TD）**误差**（error），时序差分算法将其与步长的乘积作为状态价值的更新量。可以用 $r_t+\gamma V(s_{t+1})$ 来代替 $G_t$ 的原因是：
$$
\begin{align}
V_{\pi}(s) 
& = E_{\pi}[G_t|S_t=s] \\
& = E_{\pi}[\sum_{k=0}^{\infty}\gamma^kr_{t+k}|S_t=s] \\
& = E_{\pi}[r_t+\gamma\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}|S_t=s] \\
& = E_{\pi}[r_t+\gamma V_{\pi}(S_{t+1})|S_t=s]
\end{align}
$$
因此蒙特卡洛方法将上式第一行作为更新的目标，而时序差分算法将上式最后一行作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了 $V(s_{t+1})$ 的估计值，可以证明它最终收敛到策略 $\pi$ 的价值函数。

**时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。**

## Sarsa 算法

既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是时可以直接用时序差分算法来估计动作价值函数 $Q$：$Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$

然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即 $\underset{a}{arg\;max}\;Q(s,a)$。这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。

然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们需要用极大量的样本来进行更新。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了，我们可以这么做的原因是策略提升可以在策略评估未完全进行的情况进行，这其实是**广义策略迭代**（generalized policy iteration）的思想。第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对 $(s,a)$ 永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。

简单常用的解决方案是不再一味使用贪婪算法，而是采用一个 $\epsilon$-贪婪策略：有 $1-\epsilon$ 的概率采用动作价值最大的那个动作，另外有 $\epsilon$ 的概率从动作空间中随机采取一个动作，其公式表示为：
$$
\pi(a,s) = 
\begin{cases}
\frac{\epsilon}{|A|}+1-\epsilon, & \text {如果} a = arg\; max_{a'}Q(s,a') \\
\frac{\epsilon}{|A|}, & \text{其他动作}
\end{cases}
$$
现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态 $s$、当前动作 $a$、获得的奖励 $r$、下一个状态 $s'$ 和下一个动作 $a'$，将这些符号拼接后就得到了算法名称。Sarsa 的具体算法如下：

![](https://datawhalechina.github.io/easy-rl/img/ch3/3.15.png)

## 多步 Sarsa 算法

蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是**无偏**（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。

结合二者优势的方法：**多步时序差分**。多步时序差分的意思是使用步的奖励，然后使用之后状态的价值估计。

多步（n步）Sarsa 算法的公式表示：
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+ \gamma r_{t+1} + \cdots+\gamma^n Q(s_{t+n},a_{t+n})-Q(s_t,a_t)]
$$

## Q-learning 算法

除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为：
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+ \gamma\;\underset{a}{max} Q(s_{t+1},a)-Q(s_t,a_t)]
$$
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.18.png)

我们可以用价值迭代的思想来理解 Q-learning，即 Q-learning 是直接在估计 $Q^*$，因为动作价值函数的贝尔曼最优方程是 $Q^*(s,a)=r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)\underset{a'\in A}{max}Q^*(s',a')$

而 Sarsa 估计当前 $\epsilon$-贪婪策略的动作价值函数。需要强调的是，Q-learning 的更新并非必须使用当前贪心策略 $arg\;max_a\;Q(s,a)$ 采样得到的数据，因为给定任意 $(s,a,r,s')$都可以直接根据更新公式来更新 $Q$，为了探索，我们通常使用一个 $\epsilon$-贪婪策略来与环境交互。Sarsa 必须使用当前 $\epsilon$-贪婪策略采样得到的数据，因为它的更新中用到的 $Q(s',a')$ 的 $a'$ 是当前策略在 $s'$ 下的动作。

我们称 Sarsa 是**在线策略**（on-policy）算法，称 Q-learning 是**离线策略**（off-policy）算法。

我们称采样数据的策略为**行为策略**（behavior policy），称用这些数据来更新的策略为**目标策略**（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。

判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，具体而言：

- 对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组 $(s,a,r,s',a')$，因此它是在线策略学习方法
- 对于 Q-learning，它的更新公式使用的是四元组 $(s,a,r,s')$ 来更新当前状态动作对的价值 $Q(s,a)$，数据中的 $s$ 和 $a$ 是给定的条件，$r$ 和 $s'$ 皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法

![](https://hrl.boyuai.com/static/400.78f393db.png)

比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q-learning 是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。

值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为**离线强化学习**（offline reinforcement learning）

![](https://datawhalechina.github.io/easy-rl/img/ch3/3.21.png)

# Dyna-Q 算法

在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：**基于模型的强化学习**（model-based reinforcement learning）和**无模型的强化学习**（model-free reinforcement learning）。

无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，Sarsa 和 Q-learning 算法便是两种无模型的强化学习方法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。两种动态规划算法，策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。

强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。

**Dyna-Q 算法是一个经典的基于模型的强化学习算法。**Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态 $s$，采取一个曾经在该状态下执行过的动作 $a$，通过模型得到转移后的状态 $s'$ 以及奖励 $r$ ，并根据这个模拟数据 $(s,a,r,s')$，用 Q-learning 的更新方式来更新动作价值函数。

![](https://hrl.boyuai.com/static/480.25b67b37.png)

Dyna-Q 算法的具体流程：

- 初始化 $Q(s,a)$，初始化模型 $M(s,a)$
- for 序列 $e=1 \rightarrow E$ do:
  - 得到初始状态 $s$
  - for $t=1 \rightarrow T$ do:
    - 用 $\epsilon$-贪婪策略根据 $Q$ 选择当前状态 $s$ 下的动作 $a$
    - 得到环境反馈的 $r,s'$
    - $Q(s,a) \leftarrow Q(s,a)+\alpha[r+ \gamma\;\underset{a'}{max} Q(s',a')-Q(s,a)]$
    - $M(s,a) \leftarrow r,s'$
    - for 次数 $n=1 \rightarrow N$ do:
      - 随机选择一个曾经访问过的状态 $s_m$
      - 采取一个曾经在状态 $s_m$ 下执行过的动作 $a_m$
      - $r_m,s'_m \leftarrow M(s_m,a_m)$
      - $Q(s_m,a_m) \leftarrow Q(s_m,a_m)+\alpha[r_m+ \gamma\;\underset{a'}{max} Q(s'_m,a')-Q(s_m,a_m)]$
    - end for
    - $s \leftarrow s'$
  - end for
- end for

可以看到，在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做 $N$ 次 Q-planning。其中 Q-planning 的次数 $N$ 是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据 $(s,a,r,s')$ 时，可以直接对模型做出更新，即 $M(s,a) \leftarrow r,s'$

**Code is available at [Dyna-Q](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Dyna-Q.ipynb)**

实验证明，随着 Q-planning 步数的增多，Dyna-Q 算法的收敛速度也随之变快。当然，并不是在所有的环境中，都是 Q-planning 步数越大则算法收敛越快，这取决于环境是否是确定性的，以及环境模型的精度。在悬崖漫步环境中，状态的转移是完全确定性的，构建的环境模型的精度是最高的，所以可以通过增加 Q-planning 步数来直接降低算法的样本复杂度。



# DQN 算法

在 Q-learning 算法中，以矩阵的方式建立了一张存储每个状态下所有动作 $Q$ 值的表格。表格中的每一个动作价值 $Q(s,a)$ 表示在状态 $s$ 下选择动作 $a$ 然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，当状态或者动作数量非常大的时候，这种做法就不适用了。更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的 $Q$ 值。

对于这种情况，我们需要用函数拟合的方法来估计 $Q$ 值，即将这个复杂的 $Q$ 值表格视作数据，使用一个参数化的函数 $Q_{\theta}$ 来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。而DQN 算法便可以用来解决连续状态下离散动作的问题。

以 [CartPole](https://github.com/openai/gym/wiki/CartPole-v0) 问题为例，它的状态值就是连续的，动作值是离散的。

类似车杆的环境中得到动作价值函数 $Q(s,a)$，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用**函数拟合**（function approximation）的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数 $Q$。若动作是连续（无限）的，神经网络的输入是状态 $s$ 和动作 $a$，然后输出一个标量，表示在状态 $s$ 下采取动作 $a$ 能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，我们还可以只将状态 $s$ 输入到神经网络中，使其同时输出每一个动作的 $Q$ 值。

通常 DQN（以及 Q-learning）只能处理动作离散的情况，因为在函数 $Q$ 的更新过程中有 $max_a$这一操作。假设神经网络用来拟合函数的参数是 $w$，即每一个状态 $s$ 下所有可能动作 $a$ 的 $Q$ 值我们都能表示为 $Q_w(s,a)$。我们将用于拟合函数函数的神经网络称为**Q 网络**

<img src="https://hrl.boyuai.com/static/640.46b13e89.png" style="zoom: 67%;" />

回顾 Q-learning 的更新规则：$Q(s,a) \leftarrow Q(s,a)+\alpha[r+ \gamma\;\underset{a'\in A}{max} Q(s',a')-Q(s,a)]$
上述公式用**时序差分**（temporal difference，TD）学习目标 $r+ \gamma\;\underset{a'\in A}{max} Q(s',a')$ 来增量式更新 $Q(s,a)$，也就是说要使 $Q(s,a)$ 和 TD 目标 $r+ \gamma\;\underset{a'\in A}{max} Q(s',a')$ 靠近。

于是，对于一组数据 $\{(s_i,a_i,r_i,s'_i)\}$，我们可以很自然地将 **Q 网络的损失函数**构造为**均方误差**的形式：
$$
w^*=\underset{w}{arg\;min\;}\frac{1}{2N}\sum_{i=1}^N[Q_w(s_i,a_i)-(r_i+\gamma\; \underset{a'}{max}\;Q_w(s'_i,a'))]^2
$$
至此，我们就可以将 Q-learning 扩展到神经网络形式——**深度 Q 网络**（deep Q network，DQN）算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个 $\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——**经验回放**和**目标网络**，它们能够帮助 DQN 取得稳定、出色的性能。

## 经验回放

在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次 $Q$ 值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用:

- 使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。
- 提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

<img src="https://datawhalechina.github.io/easy-rl/img/ch6/6.18.png" style="zoom:25%;" />

## 目标网络

DQN 算法最终更新的目标是让 $Q_w(s,a)$ 逼近 $r+ \gamma\;\underset{a'\in A}{max}\;Q_w(s',a')$ ，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。

为了解决这一问题，DQN 便使用了**目标网络**（target network）的思想：既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。为了实现这一思想，我们需要利用两套 Q 网络。

- 原来的训练网络 $Q_w(s,a)$ ，用于计算原来的损失函数 $\frac{1}{2}[Q_w(s,a)-(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))]^2$ 中的 $Q_w(s,a)$ 项，并且使用正常梯度下降方法来进行更新
- 目标网络 $Q_{w^{-}}(s,a)$，用于计算原先损失函数 $\frac{1}{2}[Q_w(s,a)-(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))]^2$ 中的 $(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))$ 项，其中 $w^{-}$ 表示目标网络中的参数。如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套较旧的参数，训练网络 $Q_w(s,a)$ 在训练中的每一步都会更新，而目标网络的参数每隔 $C$ 步才会与训练网络同步一次。这样做使得目标网络相对于训练网络更加稳定。

<img src="https://datawhalechina.github.io/easy-rl/img/ch6/6.12.png" style="zoom: 25%;" />

------

**DQN 算法流程如下：**

- 用随机的网络参数 $w$ 初始化网络 $Q_w(s,a)$
- 复制相同的参数 $w^- \leftarrow w$ 来初始化目标网络 $Q_{w'}$
- 初始化经验回放池 $R$
- for 序列 $e = 1 \rightarrow E$ do
  -  获取环境初始状态 $s_1$
  -  for 时间步 $t=1 \rightarrow T$ do
    -  根据当前网络 $Q_w(s,a)$以 $\epsilon$-贪婪策略选择动作 $a_t$
    -  执行动作 $a_t$，获得回报 $r_t$，环境状态变为 $s_{t+1}$
    -  将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池中
    -  若 $R$ 中数据足够，从中采样 $N$ 个数据 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
    -  对每个数据，用目标网络计算 $y_i=r_i+\gamma\; \underset{a}{max}\;Q_{w^{-}}(s_{i+1},a))$
    -  最小化目标损失 $L=\frac{1}{N}\sum_i(y_i-Q_w(s_i,a_i))^2$，以此更新当前网络 $Q_w$
    -  更新目标网络
  - end for
- end for

**Code is available at [DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Deep%20Q%20network.ipynb)**

## Double DQN

普通的 DQN 算法通常会导致对 $Q$ 值的过高估计（overestimation）。传统 DQN 优化的 TD 误差目标为：$r_i+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))$

其中 $\underset{a'}{max}\;Q_{w^{-}}(s',a'))$ 由目标网络（参数为$w^-$）计算得出，我们还可以将其写成如下形式：$Q_{w^{-}}(s',\underset{a'}{arg\;max\;}Q_{w^-}(s',a'))$

- 换句话说，$max$ 操作实际可以被拆解为两部分
  - 首先选取状态 $s'$ 下的最优动作 $a^*=\underset{a'}{arg\;max\;}Q_{w^-}(s',a')$
  - 接着计算该动作对应的价值 $Q_{w^-}(s',a^*)$

当这两部分采用同一套 Q 网络进行计算时，每次得到的都是神经网络当前估算的所有动作价值中的最大值。考虑到通过神经网络估算的 $Q$ 值本身在某些时候会产生正向或负向的误差，在 DQN 的更新方式下神经网络会将正向误差累积。对于动作空间较大的任务，DQN 中的过高估计问题会非常严重，造成 DQN 无法有效工作的后果。

为了解决这一问题，Double DQN 算法提出利用两个独立训练的神经网络估算 $\underset{a'}{max}\;Q_*(s',a')$。具体做法是将原有的 $\underset{a'}{max}\;Q_{w^{-}}(s',a')$ 更改为 $Q_{w^{-}}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))$，即利用一套神经网络 $Q_w$ 的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络 $Q_w^-$ 计算该动作的价值。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的 $Q$ 值不会存在很大的过高估计问题。

在传统的 DQN 算法中，本来就存在两套 $Q$ 函数的神经网络——目标网络和训练网络，只不过 $\underset{a'}{max}\;Q_{w^{-}}(s',a')$的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为 Double DQN 算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络计算 $Q$ 值，这便是 Double DQN 的主要思想。由于在 DQN 算法中将训练网络的参数记为 $w$ ，将目标网络的参数记为 $w^-$，这与 Double DQN 的两套神经网络的参数是统一的，因此，我们可以直接写出如下 Double DQN 的优化目标：
$$
r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))
$$
显然，DQN 与 Double DQN 的差别只是在于计算状态 $s'$ 下 $Q$ 值时如何选取动作：

- DQN 的优化目标可以写为 $r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_{w^-}(s',a'))$，动作的选取依靠目标网络 $Q_{w^-}$
- Double DQN 的优化目标为 $r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))$ ，动作的选取依靠训练网络 $Q_w$

所以 Double DQN 的代码实现可以直接在 DQN 的基础上进行，无须做过多修改。

**Code is available at [Double DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Double%20Deep%20Q%20network.ipynb)**

实验证明，与普通的 DQN 相比，Double DQN 比较少出现 $Q$ 值大于 0 的情况，说明 $Q$ 值过高估计的问题得到了很大缓解。

## Dueling DQN

Dueling DQN 是 DQN 另一种的改进算法，它在传统 DQN 的基础上只进行了微小的改动，但却能大幅提升 DQN 的表现。在强化学习中，我们将状态动作价值函数 $Q$ 减去状态价值函数 $V$ 的结果定义为优势函数 $A$，即 $A(s,a)=Q(s,a)-V(s)$。

在同一个状态下，所有动作的优势值之和为 0，因为所有动作的动作价值的期望就是这个状态的状态价值。据此，在 Dueling DQN 中，Q 网络被建模为：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)
$$
其中，$V_{\eta,\alpha}(s)$ 为状态价值函数，而 $A_{\eta,\beta}(s,a)$ 则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；$\eta$ 是状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而 $\alpha,\beta$ 分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出 $Q$ 值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到 $Q$ 值。Dueling DQN 的网络结构如下图所示：

<img src="https://hrl.boyuai.com/static/640.455bc383.png" style="zoom: 67%;" />

**将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。**

对于 Dueling DQN 中的公式 $Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)$ ，它存在对于 $V$ 值和 $A$ 值建模不唯一性的问题。例如，对于同样的 $Q$ 值，如果将 $V$ 值加上任意大小的常数 $C$ ，再将所有 $A$ 值减去 $C$，则得到的 $Q$ 值依然不变，这就导致了训练的不稳定性。为了解决这一问题，Dueling DQN 强制最优动作的优势函数的实际输出为 0，即：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\underset{a'}{max}\;A_{\eta,\beta}(s,a')
$$
此时 $V(s)=\underset{a}{max}\;A_{\eta,\beta}(s,a)$ ，可以确保 $V$​ 值建模的唯一性。在实现过程中，我们还可以用平均代替最大化操作，即：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\frac{1}{|A|}\sum_{a'}A_{\eta,\beta}(s,a')
$$
此时 $V(s)=\frac{1}{|A|}\sum_{a'}A_{\eta,\beta}(s,a')$ 

Dueling DQN 会比 DQN 好的部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数 $V$ 都会被更新，这也会影响到其他动作的 $Q$ 值。而传统的 DQN 只会更新某个动作的 $Q$ 值，其他动作的 $Q$ 值就不会更新。因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数。

**Code is available at [Dueling DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Double%20Deep%20Q%20network.ipynb)**

实验证明，相比于传统的 DQN，Dueling DQN 在多个动作选择下的学习更加稳定，得到的回报最大值也更大。由 Dueling DQN 的原理可知，随着动作空间的增大，Dueling DQN 相比于 DQN 的优势更为明显。之前我们在环境中设置的离散动作数为 11，我们可以增加离散动作数（例如 15、25 等），继续进行对比实验。



# 策略梯度算法

Q-learning、DQN 及 DQN 改进算法都是**基于价值**（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是**基于策略**（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。策略梯度是基于策略的方法的基础。

## 策略梯度

基于策略的方法首先需要将策略参数化。假设目标策略 $\pi_{\theta}$ 是一个随机性策略，并且处处可微，其中 $\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为
$$
J(\theta)=E_{s_0}[V^{\pi_{\theta}}(s_0)]
$$
其中，$s_0$ 表示初始状态。现在有了目标函数，我们将目标函数对策略 $\theta$ 求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。

$v^{\pi}$ 表示策略 $\pi$ 下的状态访问分布。然后我们对目标函数求梯度，可以得到如下式子，
$$
\begin{align}
\nabla_{\theta} J(\theta) 
& \propto \sum_{s\in S}v^{\pi_{\theta}}(s)\sum_{a\in A}Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\pi_{\theta}(a|s) \\
& = \sum_{s\in S}v^{\pi_{\theta}}(s)\sum_{a\in A}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a)\frac{\nabla_{\theta}\pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \\
& = E_{\pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla_{\theta}log\;\pi_{\theta}(a|s)]
\end{align}
$$
这个梯度可以用来更新策略。需要注意的是，因为上式中期望 $E$ 的下标是 $\pi_{\theta}$ ，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略 $\pi_{\theta}$ 采样得到的数据来计算梯度。直观理解一下策略梯度这个公式，可以发现在每一个状态下，梯度的修改是让策略更多地去采样到带来较高 $Q$ 值的动作，更少地去采样到带来较低值的动作。

在计算策略梯度的公式中，我们需要用到 $Q^{\pi_{\theta}}(s,a)$ ，可以用多种方式对它进行估计。

REINFORCE 算法便是采用了蒙特卡洛方法来估计 $Q^{\pi_{\theta}}(s,a)$ ，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为：
$$
\nabla_{\theta} J(\theta) = 
E_{\pi_{\theta}}
[\sum_{t=0}^T(\sum_{t'=t}^T\gamma^{t'-t}r_{t'})
\nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)
]
$$
其中，$T$ 是和环境交互的最大步数。

## REINFORCE

REINFORCE 算法的具体算法流程如下：

- 初始化策略参数 $\theta$
- for 序列 $e=1 \rightarrow E$ do :
  -  用当前策略 $\pi_{\theta}$ 采样轨迹 $\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_T,a_T,r_T\}$
  -  计算当前轨迹每个时刻 $t$ 往后的回报 $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$，记为 $\psi_{t}$
  -  对 $\theta$ 进行更新，$\theta=\theta+\alpha\sum_{t=0}^T\psi_{t}
    \nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)$
- end for

**Code is available at [REINFORCE](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/REINFORCE.ipynb)**

实验证明，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。



# Actor-Critic 算法

基于值函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。**Actor-Critic 方法既学习价值函数，又学习策略函数 。**Actor-Critic 是囊括一系列算法的整体架构，目前很多高效的前沿算法都属于 Actor-Critic 算法。需要明确的是，**Actor-Critic 算法本质上是基于策略的算法**，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。

在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE 算法用蒙特卡洛方法来估计 $Q(s,a)$ ，**Actor-Critic 算法所做的就是拟合一个值函数来指导策略进行学习**

在策略梯度中，可以把梯度写成下面这个更加一般的形式：$g=E[\sum_{t=0}^T\psi_{t}
\nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)]$

- 其中，$\psi_{t}$ 可以有很多种形式：
  - $\sum_{t'=0}^T\gamma^{t'}r_{t'}$：轨迹的总回报
  - $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$：动作 $a_t$ 之后的回报
  - $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}-b(s_t)$：基准线版本的改进
  - $Q^{\pi_\theta}(s_t,a_t)$：动作价值函数
  - $A^{\pi_\theta}(s_t,a_t)$：优势函数
  - $r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t})$：时序差分残差

REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用形式(3)引入**基线函数**（baseline function）$b(s_t)$ 来减小方差。

此外，我们也可以采用 Actor-Critic 算法估计一个动作价值函数 $Q$，代替蒙特卡洛采样得到的回报，这便是形式(4)

这个时候，我们可以把状态价值函数 $V$ 作为基线，从 $Q$ 函数减去这个 $V$ 函数则得到了 $A$ 函数，我们称之为**优势函数**（advantage function），这便是形式(5)

更进一步，我们可以利用 $Q=r+\gamma V$ 等式得到形式(6)

着重看形式(6)，即通过时序差分残差 $\psi_{t}=r_t+\gamma V^{\pi}(s_{t+1})-V^{\pi}(s_{t})$ 来指导策略梯度进行学习。事实上，用 $Q$ 值或者 $V$ 值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。除此之外，REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。

- **我们将 Actor-Critic 分为两个部分：Actor（策略网络）和 Critic（价值网络）** 
  - Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略
  - Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新

<img src="https://hrl.boyuai.com/static/640.5dee5ab0.jpg" style="zoom:50%;" />

Actor 的更新采用策略梯度的原则，Critic 如何更新：将 Critic 价值网络表示为 $V_w$，参数为 $w$ 。于是，我们可以采取时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数：$L(w)=\frac{1}{2}(r+\gamma V_w(s_{t+1})-V_w(s_t))^2$

与 DQN 中一样，我们采取类似于目标网络的方法，将上式中 $r+\gamma V_w(s_{t+1})$ 作为时序差分目标，不会产生梯度来更新价值函数。因此，价值函数的梯度为：
$$
\nabla_wL(w)=-[r+\gamma V_w(s_{t+1})-V_w(s_{t})]\nabla_wV_w(s_t)
$$
然后使用梯度下降方法来更新 Critic 价值网络参数即可。

Actor-Critic 算法的具体流程如下：

- 初始化策略网络参数，价值网络参数
- for 序列 $e=1\rightarrow E$ do :
  -  用当前策略 $\pi_\theta$ 采样轨迹 $\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots\}$
  -  为每一步数据计算: $\delta_t=r_t+\gamma V_w(s_{t+1})-V_w(s_{t})$
  -  更新价值参数 $w=w+\alpha_w\sum_t\delta_t\nabla_wV_w(s_t)$
  -  更新策略参数 $\theta=\theta+\alpha_\theta\sum_t\delta_t\nabla_\theta log\;\pi_\theta(a_t|s_t)$
- end for

**Code is available at [Actor-Critic](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Actor-Critic.ipynb)**

实验证明，Actor-Critic 算法很快便能收敛到最优策略，并且训练过程非常稳定，抖动情况相比 REINFORCE 算法有了明显的改进，这说明价值函数的引入减小了方差。



# TRPO 算法

策略梯度算法和 Actor-Critic 算法这两种基于策略的方法虽然简单、直观，但在实际应用过程中会遇到训练不稳定的情况。这样的算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。

针对以上问题，我们考虑在更新时找到一块**信任区域**（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是**信任区域策略优化**（trust region policy optimization，TRPO）算法的主要思想。

## 策略目标

假设当前策略为 $\pi_\theta$ ，参数为 $\theta$ 。我们考虑如何借助当前的 $\theta$ 找到一个更优的参数 $\theta'$，使得 $J(\theta')\geq J(\theta)$。具体来说，由于初始状态 $s_0$ 的分布和策略无关，因此上述策 $\pi_\theta$ 下的优化目标 $J(\theta)$ 可以写成在新策略 $\pi_{\theta'}$ 的期望形式：
$$
\begin{align}
J(\theta) 
& = E_{s_0}[V^{\pi_\theta}(s_0)] \\
& = E_{\pi_{\theta'}} [\sum_{t=0}^{\infty}\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^{\infty}\gamma^tV^{\pi_\theta}(s_t)] \\
& = -E_{\pi_{\theta'}} [\sum_{t=0}^{\infty}\gamma^t(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t))]
\end{align}
$$
（......）

TRPO 算法属于在线策略学习方法，每次策略训练仅使用上一轮策略采样的数据，是基于策略的深度强化学习算法中十分有代表性的工作之一。直觉性地理解，TRPO 给出的观点是：由于策略的改变导致数据分布的改变，这大大影响深度模型实现的策略网络的学习效果，所以通过划定一个可信任的策略学习区域，保证策略学习的稳定性和有效性。

# PPO 算法

TRPO 算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是，TRPO 算法的改进版——**近端策略优化（proximal policy optimization，PPO）**算法被提出，PPO 基于 TRPO 的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法。如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。

TRPO 的优化目标：
$$
\underset{\theta}{max}\;E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)] \\
s.t. \quad E_{s\sim v^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta}(\cdot|s))] \leq \delta
$$
TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断。

PPO 是 TRPO 的一种改进算法，它在实现上简化了 TRPO 中的复杂计算，并且它在实验中的性能大多数情况下会比 TRPO 更好，因此目前常被用作一种常用的基准算法。需要注意的是，TRPO 和 PPO 都属于在线策略学习算法，即使优化目标中包含重要性采样的过程，但其只是用到了上一轮策略的数据，而不是过去所有策略的数据。

## PPO-惩罚

PPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数。即：
$$
\underset{\theta}{arg\;max\;}E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}
[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)
-\beta D_{KL}[(\pi_{\theta_k}(\cdot|s),\pi_{\theta}(\cdot|s))]]
$$
令 $d_k=D^{v^{\pi_{\theta_k}}}_{KL}(\pi_{\theta_k},\pi_{\theta})$， $\beta$ 的更新规则如下：

- 如果 $d_k < \frac{\delta}{1.5}$，那么 $\beta_{k+1}=\frac{\beta_k}{2}$
- 如果 $d_k > \frac{\delta}{1.5}$，那么 $\beta_{k+1}=\beta_k \times 2$
- 否则 $\beta_{k+1}=\beta_k$

其中，$\delta$ 是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距

## PPO-截断

PPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即：
$$
\underset{\theta}{arg\;max\;}E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}
[min(
\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)
,clip(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon
)A^{\pi_{\theta_k}}(s,a)
)]
$$
其中 $clip(x,l,r):=max(min(x,r),l)$，即把 $x$ 限制在 $[l,r]$内。上式中 $\epsilon$ 是一个超参数，表示进行截断（clip）的范围

如果 $A^{\pi_{\theta_k}}(s,a) > 0$，说明这个动作的价值高于平均，最大化这个式子会增大 $\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}$，但不会让其超过 $1+\epsilon$。反之，如果 $A^{\pi_{\theta_k}}(s,a) < 0$ ，最大化这个式子会减小 $\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}$，但不会让其超过 $1-\epsilon$。如下所示。

<img src="https://hrl.boyuai.com/static/640.1ddeb598.png" style="zoom:67%;" />

**Code is available at [PPO](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/PPO.ipynb)**

大量实验表明，PPO-截断总是比 PPO-惩罚表现得更好。



# DDPG 算法

基于策略梯度的算法 REINFORCE、Actor-Critic 以及两个改进算法——TRPO 和 PPO，它们都是在线策略算法，这意味着它们的**样本效率**（sample efficiency）比较低。

DQN 算法直接估计最优函数 Q，可以做到离线策略学习，但是它只能处理动作空间有限的环境，这是因为它需要从所有动作中挑选一个 $Q$ 值最大的动作。如果动作个数是无限的，虽然我们可以将动作空间离散化，但这比较粗糙，无法精细控制。

**深度确定性策略梯度**（deep deterministic policy gradient，DDPG）算法就是处理动作空间无限的环境并且使用的是离线策略算法，它构造一个确定性策略，用梯度上升的方法来最大化 $Q$ 值。

DDPG 也属于一种 Actor-Critic 算法。REINFORCE、TRPO 和 PPO 学习随机性策略，而 DDPG 则学习一个确定性策略。

深度确定性策略梯度算法（DDPG），它是面向连续动作空间的深度确定性策略训练的典型算法。相比于它的先期工作，即确定性梯度算法（DPG），DDPG 加入了目标网络和软更新的方法，这对深度模型构建的价值网络和策略网络的稳定学习起到了关键的作用。DDPG 算法也被引入了多智能体强化学习领域，催生了 MADDPG 算法。

随机性策略可以表示为 $a\sim \pi_\theta(\cdot|s)$；而如果策略是确定性的，则可以记为 $a=\mu_\theta(s)$​。与策略梯度定理类似，可以推导出**确定性策略梯度定理**（deterministic policy gradient theorem）：
$$
\nabla_\theta J(\pi_\theta)=E_{s\sim v^{\pi_\beta}}[\nabla_\theta \mu_\theta(s)\nabla_a Q_w^\mu(s,a)|_{a=\mu_{\theta}(s)}]
$$
其中，$\pi_\beta$ 是用来收集数据的行为策略。我们可以这样理解这个定理：假设现在已经有函数 $Q$ ，给定一个状态 $s$，但由于现在动作空间是无限的，无法通过遍历所有动作来得到 $Q$ 值最大的动作，因此我们想用策略 $\mu$ 找到使 $Q(s,a)$ 值最大的动作 $a$，即 $\mu(s)=\underset{a}{arg\;max\;}Q(s,a)$。此时，$Q$ 就是 Critic，$\mu$ 就是 Actor，这是一个 Actor-Critic 的框架

![](https://hrl.boyuai.com/static/640.a3d586c4.png)

要想得到 $\mu$，首先用 $Q$ 对 $\mu_\theta$ 求导 $\nabla_\theta Q(s,\mu_\theta(s))$，其中会用到梯度的链式法则，先对 $a$ 求导，再对 $\theta$ 求导。然后通过梯度上升的方法来最大化函数 $Q$，得到 $Q$ 值最大的动作

DDPG 要用到4个神经网络，其中 Actor 和 Critic 各用一个网络，此外它们都各自有一个目标网络。DDPG 中 Actor 也需要目标网络因为目标网络也会被用来计算目标 $Q$  值。DDPG 中目标网络的更新与 DQN 中略有不同：在 DQN 中，每隔一段时间将 $Q$  网络直接复制给目标 $Q$  网络；而在 DDPG 中，目标 $Q$  网络的更新采取的是一种软更新的方式，即让目标 $Q$  网络缓慢更新，逐渐接近 $Q$  网络，其公式为：$w^- \leftarrow\tau w+(1-\tau)w^-$。通常 $\tau$ 是一个比较小的数，当 $\tau = 1$ 时，就和 DQN 的更新方式一致了。而目标 $\mu$ 网络也使用这种软更新的方式。

另外，由于函数 $Q$ 存在 $Q$ 值过高估计的问题，DDPG 采用了 Double DQN 中的技术来更新 $Q$ 网络。但是，由于 DDPG 采用的是确定性策略，它本身的探索仍然十分有限。 DQN 算法的探索主要由 $\epsilon$-贪婪策略的行为策略产生。同样作为一种离线策略的算法，DDPG 在行为策略上引入一个随机噪声 $N$ 来进行探索。

DDPG 的算法流程如下:

- 随机噪声可以用 $N$ 来表示，用随机的网络参数 $w$ 和 $\theta$ 分别初始化 Critic 网络 $Q_w(s,a)$ 和 Actor 网络 $\mu_\theta(s)$
- 复制相同的参数 $w^- \leftarrow w$ 和 $\theta^- \leftarrow \theta$，分别初始化目标网络 $Q_{w^-}$ 和 $\mu_{\theta^-}$
- 初始化经验回放池 $R$
- for 序列 $e=1 \to E$ do :
  - 初始化随机过程 $N$ 用于动作探索
  - 获取环境初始状态 $s_1$ 
  - for 时间步 $t=1\to T$ do :
    - 根据当前策略和噪声选择动作 $a_t=\mu_\theta(s_t)+N$
    - 执行动作 $a_t$，获得奖励 $r_t$，环境状态变为 $s_{t+1}$
    - 将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池 $R$
    - 从 $R$ 中采样 $N$ 个元组 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
    - 对每个元组，用目标网络计算 $y_i=r_i+\gamma Q_{w^-}(s_{i+1},\mu_{\theta^-}(s_{i+1}))$
    - 最小化目标损失 $L=\frac{1}{N}\sum_{i=1}^N(y_i-Q_w(s_i,a_i))^2$，以此更新当前 Critic 网络
    - 计算采样的策略梯度，以此更新当前 Actor 网络：$\nabla_\theta J=\frac{1}{N}\sum_{i=1}^N[\nabla_\theta \mu_\theta(s_i)\nabla_a Q_w(s_i,a)|_{a=\mu_{\theta}(s_i)}]$
    - 更新目标网络：$w^- \leftarrow \tau w+(1-\tau)w^- \leftarrow \tau \theta +(1-\tau)\theta^-$
  - end for
- end for

在 DDPG 的原始论文中，添加的噪声符合奥恩斯坦-乌伦贝克（Ornstein-Uhlenbeck，OU）随机过程：
$$
\Delta x_t=\theta(\mu-x_{t-1})+\sigma W
$$
其中，$\mu$ 是均值，$W$ 是符合布朗运动的随机噪声，$\theta$ 和 $\sigma$ 是比例参数。可以看出，当 $x(t-1)$ 偏离均值时，$x_t$ 的值会向均值靠拢。OU 随机过程的特点是在均值附近做出线性负反馈，并有额外的干扰项。OU 随机过程是与时间相关的，适用于有惯性的系统。在 DDPG 的实践中，不少地方仅使用正态分布的噪声。

**Code is available at [DDPG](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/DDPG.ipynb)**

# SAC 算法

在线策略算法的采样效率比较低，我们通常更倾向于使用离线策略算法。然而，虽然 DDPG 是离线策略算法，但是它的训练非常不稳定，收敛性较差，对超参数比较敏感，也难以适应不同的复杂环境。2018 年，一个更加稳定的离线策略算法 **Soft Actor-Critic（SAC）**被提出。SAC 的前身是 Soft Q-learning，它们都属于**最大熵强化学习**的范畴。Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数 $Q$ 的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。

## 最大熵强化学习

**熵**（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果 $X$ 是一个随机变量，且它的概率密度函数为 $p$，那么它的熵 $H$ 就被定义为：$H(X)=E_{x\sim p}[-log\; p(x)]$

在强化学习中，我们可以使用 $H(\pi(\cdot|s))$ 来表示策略 $\pi$ 在状态 $s$ 下的随机程度

**大熵强化学习**（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为
$$
\pi^*=\underset{\pi}{arg\;max\;}E_{\pi}[\sum_tr(s_t,a_t)+\alpha H(\pi(\cdot|s_t))]
$$
其中，$\alpha$ 是一个正则化的系数，用来控制熵的重要程度。熵正则化增加了强化学习算法的探索程度，$\alpha$ 越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。

## Soft 策略迭代

在最大熵强化学习框架中，由于目标函数发生了变化，其他的一些定义也有相应的变化。首先，我们看一下 Soft 贝尔曼方程：
$$
Q(s_t,a_t)=r(s_t,a_t)+\gamma E_{s_{t+1}}[V(s_{t+1})]
$$
其中，状态价值函数被写为
$$
V(s_t)=E_{a_t\sim \pi}[Q(s_t,a_t)-\alpha log \pi (a_t|s_t)]=E_{a_t\sim \pi}[Q(s_t,a_t)]+H(\pi(\cdot|s_t))
$$
于是，根据该 Soft 贝尔曼方程，在有限的状态和动作空间情况下，Soft 策略评估可以收敛到策略 $\pi$​ 的 Soft Q 函数。然后，根据如下 Soft 策略提升公式可以改进策略：
$$
\pi_{new}=\underset{\pi'}{arg\;min;}D_{KL}(\pi'(\cdot|s),\frac{exp(\frac{1}{\alpha}Q^{\pi_{old}}(s,\cdot))}
{Z^{\pi_{old}}(s,\cdot)})
$$
重复交替使用 Soft 策略评估和 Soft 策略提升，最终策略可以收敛到最大熵强化学习目标中的最优策略。但该 Soft 策略迭代方法只适用于**表格型**（tabular）设置的情况，即状态空间和动作空间是有限的情况。在连续空间下，我们需要通过参数化函数 $Q$ 和策略 $\pi$ 来近似这样的迭代。

## SAC

在 SAC 算法中，我们为两个动作价值函数 $Q$（参数分别为 $w_1$ 和 $w_2$）和一个策略函数 $\pi$（参数为 $\theta$）建模。基于 Double DQN 的思想，SAC 使用两个 $Q$ 网络，但每次用 $Q$ 网络时会挑选一个 $Q$ 值小的网络，从而缓解 $Q$ 值过高估计的问题。任意一个函数 $Q$ 的损失函数为：
$$
\begin{align}
L_Q(w) 
& = E_{(s_t,a_t,r_t,s_{t+1})\sim R} [\frac{1}{2}(Q_w(s_t,a_t)-(r_t+\gamma V_{w^-}(s_{t+1})))^2] \\
& = E_{(s_t,a_t,r_t,s_{t+1})\sim R,\;a_{t+1}\sim \pi_\theta(\cdot|s_{t+1})}
[\frac{1}{2}(Q_w(s_t,a_t)-(r_t+\gamma (\underset{j=1,2}{min\;}Q_{w_j^-}(s_{t+1},a_{t+1}) - \alpha log \pi(a_{t+1}|s_{t+1})))
)^2]
\end{align}
$$
其中，$R$ 是策略过去收集的数据，因为 SAC 是一种离线策略算法。为了让训练更加稳定，这里使用了目标 $Q$ 网络 $Q_{w^-}$，同样是两个目标 $Q$ 网络，与两个 $Q$ 网络一一对应。SAC 中目标 $Q$ 网络的更新方式与 DDPG 中的更新方式一样。

策略 $\pi$ 的损失函数由 KL 散度得到，化简后为：
$$
L_{\pi}(\theta)=E_{s_t\sim R,\;a_t\sim \pi_\theta}[\alpha\;log(\pi_\theta(a_t|s_t))-Q_w(s_t,a_t)]
$$
可以理解为最大化函数 $V$，因为有 $V(s_t)=E_{a_t\sim\pi}[Q(s_t,a_t)-\alpha \; log\; \pi(a_t|s_t)]$

对连续动作空间的环境，SAC 算法的策略输出高斯分布的均值和标准差，但是根据高斯分布来采样动作的过程是不可导的。因此，我们需要用到**重参数化技巧**（reparameterization trick）。重参数化的做法是先从一个单位高斯分布 $N$ 采样，再把采样值乘以标准差后加上均值。这样就可以认为是从策略高斯分布采样，并且这样对于策略函数是可导的。我们将其表示为 $a_t=f_\theta(\epsilon_t;s_t)$，其中 $\epsilon_t$ 是一个噪声随机变量。同时考虑到两个函数 $Q$​，重写策略的损失函数：
$$
L_\pi(\theta)=E_{s_t\sim R,\epsilon_t\sim N}[\alpha\;log(\pi_\theta(f_\theta(\epsilon_t;s_t)|s_t))-\underset{j=1,2}{min\;}Q_{w_j}(s_t,f_\theta(\epsilon_t;s_t))]
$$
在 SAC 算法中，如何选择熵正则项的系数非常重要。在不同的状态下需要不同大小的熵：在最优动作不确定的某个状态下，熵的取值应该大一点；而在某个最优动作比较确定的状态下，熵的取值可以小一点。为了自动调整熵正则项，SAC 将强化学习的目标改写为一个带约束的优化问题：
$$
\underset{\pi}{max\;}E_\pi[\sum_tr(s_t,a_t)] \quad s.t. \; E_{(s_t,a_t)\sim \rho_\pi}[-log(\pi_t(a_t|s_t))] \geq \Eta_0
$$
也就是最大化期望回报，同时约束熵的均值大于 $\Eta_0$。通过一些数学技巧化简后，得到 $\alpha$​ 的损失函数：
$$
L(\alpha)=E_{s_t\sim R,a_t\sim \pi(\cdot|s_t)}[-\alpha\; log\; \pi(a_t|s_t)-\alpha \Eta_0]
$$
即当策略的熵低于目标值 $\Eta_0$时，训练目标 $L(\alpha)$ 会使 $\alpha$ 的值增大，进而在上述最小化损失函数 $L_\pi(\theta)$ 的过程中增加了策略熵对应项的重要性；而当策略的熵高于目标值 $\Eta_0$时，训练目标 $L(\alpha)$ 会使 $\alpha$ 的值减小，进而使得策略训练时更专注于价值提升。

 SAC的具体算法流程如下：

- 用随机的网络参数 $w_1$, 和 $w_2$ 分别初始化 Critic 网络 $Q_{w_1}(s,a)$, 和 Actor 网络 $\pi_\theta(s)$
- 复制相同的参数 $w^-_1 \leftarrow w_1$ 和 $w^-_2 \leftarrow w_2$，分别初始化目标网络 $Q_{w^-_1}$ 和 $Q_{w^-_2}$
- 初始化经验回放池 $R$
- for 序列 $e=1 \to E$ do :
  - 初始化环境初始状态 $s_1$ 
  - for 时间步 $t=1\to T$ do :
    - 根据当前策略选择动作 $a_t=\mu_\theta(s_t)$
    - 执行动作 $a_t$，获得奖励 $r_t$，环境状态变为 $s_{t+1}$
    - 将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池 $R$
    - for 训练轮数 $k=1\to K$ do:
      - 从 $R$ 中采样 $N$ 个元组 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
      - 对每个元组，用目标网络计算 $y_i=r_i+\gamma\;\underset{j=1,2}{min}\; Q_{w_j^-}(s_{i+1},a_{i+1})-\alpha\;log\;\pi_\theta(a_{i+1}|s_{i+1})$，其中 $a_{i+1}\sim \pi_\theta(\cdot|s_{i+1})$
      - 对两个 Critic 网络都进行如下更新：对 $j=1,2$，最小化损失函数 $L=\frac{1}{N}\sum_{i=1}^N(y_i-Q_{w_j}(s_i,a_i))^2$
      - 用重参数化技巧采样动作 $\widetilde{a}_i$，然后用以下损失函数更新当前 Actor 网络：$L_\pi(\theta)=\frac{1}{N}\sum_{i=1}^N[\alpha\;log\pi_\theta(\widetilde{a}_i|s_i)-\underset{j=1,2}{min\;}Q_{w_j}(s_i,\widetilde{a}_i)]$
      - 更新熵正则项的系数 $\alpha$
      - 更新目标网络：$w^-_1 \leftarrow \tau w_1+(1-\tau)w^-_1 \leftarrow \tau w_2 +(1-\tau)w^-_2$
    - end for
  - end for
- end for

**Code is available at [SAC](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/SAC.ipynb)**
