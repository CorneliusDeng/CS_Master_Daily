# 强化学习基础

## 强化学习概述

强化学习（reinforcement learning，RL）讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。强化学习由两部分组成：智能体和环境。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个动作 （action），这个动作也称为决策（decision）。然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

<img src="https://datawhalechina.github.io/easy-rl/img/ch1/1.1.png" style="zoom: 50%;" />

在监督学习过程中，有两个假设：

- 输入的数据（标注的数据）都应是没有关联的。因为如果输入的数据有关联，学习器（learner）是不好学习的。
- 需要告诉学习器正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。

在强化学习中，监督学习的两个假设其实都不能得到满足。强化学习之所以困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境中学习，延迟奖励（delayed reward）使得训练网络非常困难。

- 强化学习和监督学习的区别：
  - 强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的
  - 学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来最多的奖励，只能通过不停地尝试来发现最有利的动作
  - 智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探 索和利用之间进行权衡，这也是在监督学习里面没有的情况
  - 在强化学习过程中，没有非常强的监督者（supervisor），只有奖励信号（reward signal），并且奖励信号是延迟的，即环境会在很久以后告诉我们之前我们采取的动作到底是不是有效的。因为我们没有得 到即时反馈，所以智能体使用强化学习来学习就非常困难。当我们采取一个动作后，如果我们使用监督学习，我们就可以立刻获得一个指导，比如，我们现在采取了一个错误的动作，正确的动作应该是什么。而在强化学习里面，环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么。而且更困难的是，它可能是在一两分钟过后告诉我们这个动作是错误的
- 通过与监督学习的比较，总结出强化学习的一些特征：
  - 强化学习会试错探索，它通过探索环境来获取对环境的理解
  - 强化学习智能体会从环境里面获得延迟的奖励
  - 在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性
  - 智能体的动作会影响它随后得到的数据，这一点是非常重要的。在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升

深度强化学习（deep reinforcemet learning，DRL） = 深度学习 + 强化学习

标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15a.png)

深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个 端到端训练（end-to-end training）的过程。我们不需要设计特征，直接输入状 态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15b.png)

## 序列决策 (sequential decision making)

强化学习研究的问题是智能体与环境交互的问题，智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。

奖励是由环境给的一种标量的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。

在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测、动作、奖励的序列：$H_t=o_1,a_1,r_1,\cdots,o_t,a_t,r_t$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个环境的状态看成关于这个历史的函数：$S_t=f(H_t)$

环境有自己的函数 $s_t^e=f^e(H_t)$ 来更新状态，在智能体的内部也有一个函数 $s_t^a=f^a(H_t)$ 来更新状态。当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中，$o_t=s_t^e=s_t^a$

但是有一种情况是智能体得到的观测并不能包含环境运作的所有状态，因为在强化学习的设定里面，环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。 在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道 部分观测值。

## 动作空间（action space）

不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间。

## 智能体的组成成分和类型

部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP) 是一个马尔可夫决策过程的泛化。POMDP 依然具对应一个强化学习 agent，它可能有一个或多个如下的组成成分：

- 策略（policy）。智能体会用策略来选取下一步的动作
- 价值函数（value function）。我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进 入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
- 模型（model）。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。策略可分为两种：随机性策略和确定性策略。随机性策略（stochastic policy）就是 π 函数，即 $\pi(a|s)=p(a_t=a|s_t=s)$。输入一个状态，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。确定性策略（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^*=\underset{a}{arg\,max}(a|s)$。

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。

价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数定义为：
$$
V_\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s], \; \forall s \in S
$$
期望 $\mathbb{E}_\pi$ 的下标是 $\pi$ 函数，$\pi$ 函数的值可反映在我们使用策略 $\pi$ 的时候，到底可以得到多少奖励。

我们还有一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作。其定义为
$$
Q_\pi(s,a)=\mathbb{E}_\pi[G_t|s_t=s,a_t=a]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s,a_t=a]
$$
模型决定了下一步的状态，而下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即 $p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a)$，奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即 $R(s,a)=\mathbb{E}[r_{t+1}|s_t=s,a_t=a]$。

当我们有了策略、价值函数和模型3个组成部分后，就形成了一个马尔可夫决策过程（Markov decision process）

根据智能体学习的事物不同，我们可以把智能体进行归类。**基于价值的智能体（value-based agent）**显式地学习价值函数，隐式地学习它的策略。策略是其从学到的价值函数里面推算出来的。**基于策略的智能体（policy-based agent）**直接学习策略，我们给它一个状态，它就会输出对应动作的概率。基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了**演员-评论员智能体（actor-critic agent）**。这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。

对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。 基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

针对是否需要对真实环境建模，强化学习可以分为**有模型（model-based）**强化学习和**免模型（model-free）**强化学习。有模型强化学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。

## 学习与规划

学习（learning）和规划（planning）是序列决策的两个基本问题。 在强化学习中，环境初始时是未知的，智能体不知道环境如何工作，它通过不断地与环境交互，逐渐改进策略。

在规划中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

## 探索和利用

在强化学习里面，探索和利用是两个很核心的问题。探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。 利用即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。 在刚开始的时候，强化学习智能体不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。

与监督学习任务不同，强化学习任务的最终奖励在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。需注意的是，即便在这样的简单情形下，强化学习仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。

想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。然而，更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。

实际上，单步强化学习任务对应于一个理论模型，即**K臂赌博机（K-armed bandit）**。 K臂赌博机也被称为**多臂赌博机（multi-armed bandit）** 。K臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。 若仅为获知每个摇臂的期望奖励，则可采用**仅探索（exploration-only）法**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。若仅为执行奖励最大的动作，则可采用**仅利用（exploitation-only）法**：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖励，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖励，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖励最大化。事实上，探索（估计摇臂的优劣）和利用（选择当前最优摇臂)这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方，这就是强化学习所面临的**探索-利用窘境（exploration-exploitation dilemma）**。显然，想要累积奖励最大，则必须在探索与利用之间达成较好的折中。



# 多臂老虎机

## 问题介绍

在多臂老虎机（multi-armed bandit，MAB）问题中，有一个拥有 $K$ 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布  $R$。

<img src="https://hrl.boyuai.com/static/640.8908144b.png" style="zoom:50%;" />

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 $r$。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作 $T$ 次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

- 多臂老虎机问题可以表示为一个元组 $(A,R)$，其中
  - $A$ 为动作集合，其中一个动作表示拉动一个拉杆。若多臂老虎机一共有 $K$ 根根拉杆，那么动作空间就是集合 $\{a_1,\cdots,a_K\}$
  - $R$ 为奖励概率分布，拉动每一根拉杆的动作 $a$ 都对应一个奖励概率分布 $R(r|a)$，不同拉杆的奖励分布通常是不同的

假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间步 $T$ 内累积的奖励: $max\sum^T_{t=1}r_t,\;r_t\sim R(\cdot|a_t)$。其中 $a_t$ 表示在第时间步拉动某一拉杆的动作，$r_t$ 表示动作 $a_t$ 获得的奖励

对于每一个动作 $a$，我们定义其期望奖励为 $Q(a)=E_{r\sim R(\cdot|a)}[r]$。于是，至少存在一根拉杆，它的期望奖励不小于拉动其他任意一根拉杆，我们将该最优期望奖励表示为 $Q^*=max_{a\in A}Q(a)$。为了更加直观、方便地观察拉动一根拉杆的期望奖励离最优拉杆期望奖励的差距，引入**懊悔**（regret）概念。懊悔定义为拉动当前拉杆的动作 $a$ 与最优拉杆的期望奖励差，即$R(a)=Q^*-Q(a)$。**累积懊悔**（cumulative regret）即操作 $T$ 次拉杆后累积的懊悔总量，对于一次完整的 $T$ 步决策 $\{a_1,a_2,\cdots,a_T\}$，累积懊悔为 $\sigma_R=\sum^T_{t=1}R(a_t)$。MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。

------

为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望，其算法流程如下所示：

- 对于 $\forall a\in A$，初始化计数器 $N(a)=0$ 和期望奖励估值 $\widehat Q(a)=0$
- for t = 1 to T, do
  - 选取某根拉杆，该动作记为 $a_t$
  - 得到奖励 $r_t$
  - 更新计数器: $N(a_t) = N(a_t) + 1$
  - 更新期望奖励估值: $\widehat Q(a_t)= \widehat Q(a_t)+\frac{1}{N(a_t)}[r_t-\widehat Q(a_t)]$
- end for

------

以上 for 循环中的第四步如此更新估值，是因为这样可以进行增量式的期望更新，公式如下：
$$
\begin{align}
Q_k 
& = \frac{1}{k}\sum_{i=1}^k r_i \\
& = \frac{1}{k}(r_k + \sum_{i=1}^{k-1}r_i) \\
& = \frac{1}{k}[r_k+(k-1)Q_{k-1}] \\
& = \frac{1}{k}(r_k+kQ_{k-1}-Q_{k-1}) \\
& = Q_{k-1} + \frac{1}{k}(r_k-Q_{k-1})
\end{align}
$$
如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为 $O(n)$。而采用增量式更新，时间复杂度和空间复杂度均为 $O(1)$。

## 探索与利用的平衡

还没有一个策略告诉我们应该采取哪个动作，即拉动哪根拉杆，所以接下来考虑如何设计一个策略。

例如，一个最简单的策略就是一直采取第一个动作，但这就非常依赖运气的好坏。如果运气绝佳，可能拉动的刚好是能获得最大期望奖励的拉杆，即最优拉杆；但如果运气很糟糕，获得的就有可能是最小的期望奖励。在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。**探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。例如，对于一个 10 臂老虎机，我们只拉动过其中 3 根拉杆，接下来就一直拉动这 3 根拉杆中期望奖励最大的那根拉杆，但很有可能期望奖励最大的拉杆在剩下的 7 根当中，即使我们对 10 根拉杆各自都尝试了 20 次，发现 5 号拉杆的经验期望奖励是最高的，但仍然存在着微小的概率—另一根 6 号拉杆的真实期望奖励是比 5 号拉杆更高的。

于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如 $\epsilon$-贪婪算法、上置信界算法和汤普森采样算法等。

下述算法的代码见：[K-armed bandit](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/K-armed%20bandit.ipynb)

## $\epsilon$-贪婪算法

完全贪婪算法即在每一时刻采取期望奖励估值最大的动作（拉动拉杆），这就是纯粹的利用，而没有探索，所以我们通常需要对完全贪婪算法进行一些修改，其中比较经典的一种方法为 $\epsilon$-Greedy 算法。$\epsilon$-Greedy 算法在完全贪婪算法的基础上添加了噪声，每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索），公式如下：
$$
a_t = 
\begin{cases}
arg\; max_{a\in A}\widehat Q(a), & \text {采样概率：} 1-\epsilon \\
\text{从A中随机选择}, & \text{采样概率：}\epsilon
\end{cases}
$$
随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。

所以在 $\epsilon$-贪婪算法的具体实现中，我们可以令$\epsilon$  随时间衰减，即探索的概率将会不断降低。但是请注意，$\epsilon$不会在有限的步数内衰减至 0，因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。

实验证明，随时间做反比例衰减的 $\epsilon$-贪婪算法能够使累积懊悔与时间步的关系变成**次线性**（sublinear）的，这明显优于固定 $\epsilon$ 值的$\epsilon$ -贪婪算法

## 上置信界算法

设想这样一种情况：对于一台双臂老虎机，其中第一根拉杆只被拉动过一次，得到的奖励为 ；第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。这时你会怎么做？或许你会进一步尝试拉动第一根拉杆，从而更加确定其奖励分布。这种思路主要是基于不确定性，因为此时第一根拉杆只被拉动过一次，它的不确定性很高。一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。

我们在此引入不确定性度量 $U(a)$，它会随着一个动作被尝试次数的增加而减小。我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。

**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**（Hoeffding's inequality）。在霍夫丁不等式中，令 $X_1,\cdots,X_n$ 为$n$个独立同分布的随机变量，取值范围为 $[0,1]$，其经验期望为 $\overline{x}_n=\frac{1}{n}\sum^n_{j=1}X_j$，则有 $P\{E[X] \geq \overline{x}_n+u\}\leq e^{-2nu^2}$

现在我们将霍夫丁不等式运用于多臂老虎机问题中。将 $\widehat{Q}_t(a)$ 代入 $\overline{x}_t$，不等式中的参数 $u=\widehat{U}_t(a)$ 代表不确定性度量。给定一个概率 $p=e^{-2N_t(a)U_t(a)^2}$ ，根据上述不等式，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 至少以概率 $1-p$ 成立。当 $p$ 很小时，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 就以很大概率成立，$\widehat{Q}_t(a)+\widehat{U}_t(a)$ 便是期望奖励上界。此时，上置信界算法便选取期望奖励上界最大的动作，即 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+\widehat{U}(a)]$。其中 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)}}$ 。因此，设定一个概率 $p$ 后，就可以计算相应的不确定性度量 $\widehat{U}_t(a)$ 了。更直观地说，UCB 算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 $p$ 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

在具体的实现过程中，设置 $p = \frac{1}{t}$，并且在分母中为拉动每根拉杆的次数加上常数 1，以免出现分母为 0 的情形。即此时 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)+1}}$。同时，设定一个系数 $c$ 来控制不确定性的比重，此时 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+c \cdot \widehat{U}(a)]$

## 汤普森采样算法

MAB 中还有一种经典算法——**汤普森采样**（Thompson sampling），先假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作 $a$ 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。

了解了汤普森采样算法的基本思路后，我们需要解决另一个问题：怎样得到当前每个动作 $a$ 的奖励概率分布并且在过程中进行更新？在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择了 $k$ 次，其中 $m_1$ 次奖励为 1， $m_2$ 次奖励为 0，则该拉杆的奖励服从参数为 $m_1+1,m_2+1$ 的 Beta 分布。

## 总结

通过实验我们可以得到以下结论： $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。



# 马尔可夫决策过程

与多臂老虎机问题不同，**马尔可夫决策过程**（Markov decision process，MDP）包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

## 马尔可夫过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于 $t$ 时刻之前的状态。我们将已知历史信息 $S_1,\cdots,S_t$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示成 $P(S_{t+1}|S_1,\cdots,S_t)$

## 马尔可夫奖励过程

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。我们通常用元组 $<S,P>$ 描述一个马尔可夫过程，其中 $S$ 是有限数量的状态集合，$P$ 是**状态转移矩阵**（state transition matrix）。假设一共有 $n$ 个状态，此时 $S=\{s_1,s_2,\cdots,s_n\}$。状态转移矩阵 $P$ 定义了所有状态对之间的转移概率，即
$$
\boldsymbol{P}=\left(\begin{array}{cccc}
    p\left(s_{1} \mid s_{1}\right) & p\left(s_{2} \mid s_{1}\right) & \ldots & p\left(s_{n} \mid s_{1}\right) \\
    p\left(s_{1} \mid s_{2}\right) & p\left(s_{2} \mid s_{2}\right) & \ldots & p\left(s_{n} \mid s_{2}\right) \\
    \vdots & \vdots & \ddots & \vdots \\
    p\left(s_{1} \mid s_{n}\right) & p\left(s_{2} \mid s_{n}\right) & \ldots & p\left(s_{n} \mid s_{n}\right)
    \end{array}\right)
$$
矩阵 $P$  中的第 i 行和第 j 列元素 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率，我们称 $P(s'|s)$ 为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵 $P$ 的每一行的和为 1。

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**（episode），这个步骤也被叫做**采样**（sampling）。生成序列的概率和状态转移矩阵有关。

## 马尔可夫决策过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$，就可以得到**马尔可夫奖励过程**（Markov reward process）。一个马尔可夫奖励过程由 $<S,P,r,\gamma>$ 构成，各个组成元素的含义如下所示：

- $S$ 是有限状态的集合
- $P$ 是状态转移矩阵
- $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 转移到该状态时可以获得奖励的期望
- $\gamma$ 是折扣因子（discount factor），$\gamma$ 的取值范围为 $[0,1]$。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励。
