# 强化学习基础

## 强化学习概述

强化学习（reinforcement learning，RL）讨论的问题是智能体（agent）怎么在复杂、不确定的环境（environment）中最大化它能获得的奖励。强化学习由两部分组成：智能体和环境。在强化学习过程中，智能体与环境一直在交互。智能体在环境中获取某个状态后，它会利用该状态输出一个动作 （action），这个动作也称为决策（decision）。然后这个动作会在环境中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

<img src="https://datawhalechina.github.io/easy-rl/img/ch1/1.1.png" style="zoom: 50%;" />

在监督学习过程中，有两个假设：

- 输入的数据（标注的数据）都应是没有关联的。因为如果输入的数据有关联，学习器（learner）是不好学习的。
- 需要告诉学习器正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。

在强化学习中，监督学习的两个假设其实都不能得到满足。强化学习之所以困难，是因为智能体不能得到即时的反馈，然而我们依然希望智能体在这个环境中学习，延迟奖励（delayed reward）使得训练网络非常困难。

- 强化学习和监督学习的区别：
  - 强化学习输入的样本是序列数据，而不像监督学习里面样本都是独立的
  - 学习器并没有告诉我们每一步正确的动作应该是什么，学习器需要自己去发现哪些动作可以带来最多的奖励，只能通过不停地尝试来发现最有利的动作
  - 智能体获得自己能力的过程，其实是不断地试错探索（trial-and-error exploration）的过程。探索 （exploration）和利用（exploitation）是强化学习里面非常核心的问题。其中，探索指尝试一些新的动作， 这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”；利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。因此，我们需要在探 索和利用之间进行权衡，这也是在监督学习里面没有的情况
  - 在强化学习过程中，没有非常强的监督者（supervisor），只有奖励信号（reward signal），并且奖励信号是延迟的，即环境会在很久以后告诉我们之前我们采取的动作到底是不是有效的。因为我们没有得 到即时反馈，所以智能体使用强化学习来学习就非常困难。当我们采取一个动作后，如果我们使用监督学习，我们就可以立刻获得一个指导，比如，我们现在采取了一个错误的动作，正确的动作应该是什么。而在强化学习里面，环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么。而且更困难的是，它可能是在一两分钟过后告诉我们这个动作是错误的
- 通过与监督学习的比较，总结出强化学习的一些特征：
  - 强化学习会试错探索，它通过探索环境来获取对环境的理解
  - 强化学习智能体会从环境里面获得延迟的奖励
  - 在强化学习的训练过程中，时间非常重要。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性
  - 智能体的动作会影响它随后得到的数据，这一点是非常重要的。在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升

深度强化学习（deep reinforcemet learning，DRL） = 深度学习 + 强化学习

标准强化学习：比如 TD-Gammon 玩 Backgammon 游戏的过程，其实就是设计特征，然后训练价值函数的过程。标准强化学习先设计很多特征，这些特征可以描述现在整个状态。 得到这些特征后，我们就可以通过训练一个分类网络或者分别训练一个价值估计函数来采取动作。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15a.png)

深度强化学习：自从我们有了深度学习，有了神经网络，就可以把智能体玩游戏的过程改进成一个 端到端训练（end-to-end training）的过程。我们不需要设计特征，直接输入状 态就可以输出动作。我们可以用一个神经网络来拟合价值函数或策略网络，省去特征工程（feature engineering）的过程。![](https://datawhalechina.github.io/easy-rl/img/ch1/1.15b.png)

## 序列决策 (sequential decision making)

强化学习研究的问题是智能体与环境交互的问题，智能体把它的动作输出给环境，环境取得这个动作后会进行下一步，把下一步的观测与这个动作带来的奖励返还给智能体。这样的交互会产生很多观测，智能体的目的是从这些观测之中学到能最大化奖励的策略。

奖励是由环境给的一种标量的反馈信号（scalar feedback signal），这种信号可显示智能体在某一步采取某个策略的表现如何。强化学习的目的就是最大化智能体可以获得的奖励，智能体在环境里面存在的目的就是最大化它的期望的累积奖励（expected cumulative reward）。

在一个强化学习环境里面，智能体的目的就是选取一系列的动作来最大化奖励，所以这些选取的动作必须有长期的影响。但在这个过程里面，智能体的奖励其实是被延迟了的，就是我们现在选取的某一步动作，可能要等到很久后才知道这一步到底产生了什么样的影响。强化学习里面一个重要的课题就是近期奖励和远期奖励的权衡 （trade-off），研究怎么让智能体取得更多的远期奖励。

在与环境的交互过程中，智能体会获得很多观测。针对每一个观测，智能体会采取一个动作，也会得到一个奖励。所以历史是观测、动作、奖励的序列：$H_t=o_1,a_1,r_1,\cdots,o_t,a_t,r_t$

智能体在采取当前动作的时候会依赖于它之前得到的历史，所以我们可以把整个环境的状态看成关于这个历史的函数：$S_t=f(H_t)$

环境有自己的函数 $s_t^e=f^e(H_t)$ 来更新状态，在智能体的内部也有一个函数 $s_t^a=f^a(H_t)$ 来更新状态。当智能体的状态与环境的状态等价的时候，即当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的（fully observed）。在这种情况下面，强化学习通常被建模成一个马尔可夫决策过程 （Markov decision process，MDP）的问题。在马尔可夫决策过程中，$o_t=s_t^e=s_t^a$

但是有一种情况是智能体得到的观测并不能包含环境运作的所有状态，因为在强化学习的设定里面，环境的状态才是真正的所有状态。当智能体只能看到部分的观测，我们就称这个环境是部分可观测的（partially observed）。 在这种情况下，强化学习通常被建模成部分可观测马尔可夫决策过程（partially observable Markov decision process, POMDP）的问题。部分可观测马尔可夫决策过程是马尔可夫决策过程的一种泛化。 部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是假设智能体无法感知环境的状态，只能知道 部分观测值。

## 动作空间（action space）

不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间。

## 智能体的组成成分和类型

部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP) 是一个马尔可夫决策过程的泛化。POMDP 依然具对应一个强化学习 agent，它可能有一个或多个如下的组成成分：

- 策略（policy）。智能体会用策略来选取下一步的动作
- 价值函数（value function）。我们用价值函数来对当前状态进行评估。价值函数用于评估智能体进 入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
- 模型（model）。模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式

策略是智能体的动作模型，它决定了智能体的动作。它其实是一个函数，用于把输入的状态变成动作。策略可分为两种：随机性策略和确定性策略。随机性策略（stochastic policy）就是 π 函数，即 $\pi(a|s)=p(a_t=a|s_t=s)$。输入一个状态，输出一个概率。这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将采取的动作。确定性策略（deterministic policy）就是智能体直接采取最有可能的动作，即 $a^*=\underset{a}{arg\,max}(a|s)$。

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。

价值函数的值是对未来奖励的预测，我们用它来评估状态的好坏。价值函数里面有一个折扣因子（discount factor），我们希望在尽可能短的时间里面得到尽可能多的奖励。价值函数定义为：
$$
V_\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s], \; \forall s \in S
$$
期望 $\mathbb{E}_\pi$ 的下标是 $\pi$ 函数，$\pi$ 函数的值可反映在我们使用策略 $\pi$ 的时候，到底可以得到多少奖励。

我们还有一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作。其定义为
$$
Q_\pi(s,a)=\mathbb{E}_\pi[G_t|s_t=s,a_t=a]=\mathbb{E}_\pi[\sum_{k=0}^\infty\gamma^kr_t+k+1|s_t=s,a_t=a]
$$
模型决定了下一步的状态，而下一步的状态取决于当前的状态以及当前采取的动作。它由状态转移概率和奖励函数两个部分组成。状态转移概率即 $p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a)$，奖励函数是指我们在当前状态采取了某个动作，可以得到多大的奖励，即 $R(s,a)=\mathbb{E}[r_{t+1}|s_t=s,a_t=a]$。

当我们有了策略、价值函数和模型3个组成部分后，就形成了一个马尔可夫决策过程（Markov decision process）

根据智能体学习的事物不同，我们可以把智能体进行归类。**基于价值的智能体（value-based agent）**显式地学习价值函数，隐式地学习它的策略，策略是其从学到的价值函数里面推算出来的。**基于策略的智能体（policy-based agent）**直接学习策略，我们给它一个状态，它就会输出对应动作的概率，基于策略的智能体并没有学习价值函数。把基于价值的智能体和基于策略的智能体结合起来就有了**演员-评论员智能体（actor-critic agent）**，这一类智能体把策略和价值函数都学习了，然后通过两者的交互得到最佳的动作。

对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解。从决策方式来看，强化学习又可以划分为基于策略的方法和基于价值的方法。决策方式是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。 而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于动作集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作）。 基于价值的强化学习算法有Q学习（Q-learning）、 Sarsa 等，而基于策略的强化学习算法有策略梯度（Policy Gradient，PG）算法等。此外，演员-评论员算法同时使用策略和价值评估来做出决策。其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

针对是否需要对真实环境建模，强化学习可以分为**有模型（model-based）**强化学习和**无模型（model-free）**强化学习。有模型强化学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型强化学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。

## 学习与规划

学习（learning）和规划（planning）是序列决策的两个基本问题。 在强化学习中，环境初始时是未知的，智能体不知道环境如何工作，它通过不断地与环境交互，逐渐改进策略。

在规划中，环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

## 探索和利用

在强化学习里面，探索和利用是两个很核心的问题。探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）。 利用即我们不去尝试新的动作，而是采取已知的可以带来很大奖励的动作。 在刚开始的时候，强化学习智能体不知道它采取了某个动作后会发生什么，所以它只能通过试错去探索，所以探索就是通过试错来理解采取的动作到底可不可以带来好的奖励。利用是指我们直接采取已知的可以带来很好奖励的动作。所以这里就面临一个权衡问题，即怎么通过牺牲一些短期的奖励来理解动作，从而学习到更好的策略。

与监督学习任务不同，强化学习任务的最终奖励在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。需注意的是，即便在这样的简单情形下，强化学习仍与监督学习有显著不同，因为智能体需通过试错来发现各个动作产生的结果，而没有训练数据告诉智能体应当采取哪个动作。

想要最大化单步奖励需考虑两个方面：一是需知道每个动作带来的奖励，二是要执行奖励最大的动作。若每个动作对应的奖励是一个确定值，那么尝试遍所有的动作便能找出奖励最大的动作。然而，更一般的情形是，一个动作的奖励值是来自一个概率分布，仅通过一次尝试并不能确切地获得平均奖励值。

实际上，单步强化学习任务对应于一个理论模型，即**K臂赌博机（K-armed bandit）**。 K臂赌博机也被称为**多臂赌博机（multi-armed bandit）** 。K臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。 若仅为获知每个摇臂的期望奖励，则可采用**仅探索（exploration-only）法**：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。若仅为执行奖励最大的动作，则可采用**仅利用（exploitation-only）法**：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖励，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖励，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖励最大化。事实上，探索（估计摇臂的优劣）和利用（选择当前最优摇臂)这两者是矛盾的，因为尝试次数（总投币数）有限，加强了一方则自然会削弱另一方，这就是强化学习所面临的**探索-利用窘境（exploration-exploitation dilemma）**。显然，想要累积奖励最大，则必须在探索与利用之间达成较好的折中。



# 多臂老虎机

## 问题介绍

在多臂老虎机（multi-armed bandit，MAB）问题中，有一个拥有 $K$ 根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布  $R$。

<img src="https://hrl.boyuai.com/static/640.8908144b.png" style="zoom:50%;" />

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 $r$。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作 $T$ 次拉杆后获得尽可能高的累积奖励。由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

- 多臂老虎机问题可以表示为一个元组 $(A,R)$，其中
  - $A$ 为动作集合，其中一个动作表示拉动一个拉杆。若多臂老虎机一共有 $K$ 根根拉杆，那么动作空间就是集合 $\{a_1,\cdots,a_K\}$
  - $R$ 为奖励概率分布，拉动每一根拉杆的动作 $a$ 都对应一个奖励概率分布 $R(r|a)$，不同拉杆的奖励分布通常是不同的

假设每个时间步只能拉动一个拉杆，多臂老虎机的目标为最大化一段时间步 $T$ 内累积的奖励: $max\sum^T_{t=1}r_t,\;r_t\sim R(\cdot|a_t)$。其中 $a_t$ 表示在第时间步拉动某一拉杆的动作，$r_t$ 表示动作 $a_t$ 获得的奖励

对于每一个动作 $a$，我们定义其期望奖励为 $Q(a)=E_{r\sim R(\cdot|a)}[r]$。于是，至少存在一根拉杆，它的期望奖励不小于拉动其他任意一根拉杆，我们将该最优期望奖励表示为 $Q^*=max_{a\in A}Q(a)$。为了更加直观、方便地观察拉动一根拉杆的期望奖励离最优拉杆期望奖励的差距，引入**懊悔**（regret）概念。懊悔定义为拉动当前拉杆的动作 $a$ 与最优拉杆的期望奖励差，即$R(a)=Q^*-Q(a)$。**累积懊悔**（cumulative regret）即操作 $T$ 次拉杆后累积的懊悔总量，对于一次完整的 $T$ 步决策 $\{a_1,a_2,\cdots,a_T\}$，累积懊悔为 $\sigma_R=\sum^T_{t=1}R(a_t)$。MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。

------

为了知道拉动哪一根拉杆能获得更高的奖励，我们需要估计拉动这根拉杆的期望奖励。由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望，其算法流程如下所示：

- 对于 $\forall a\in A$，初始化计数器 $N(a)=0$ 和期望奖励估值 $\widehat Q(a)=0$
- for t = 1 to T, do
  - 选取某根拉杆，该动作记为 $a_t$
  - 得到奖励 $r_t$
  - 更新计数器: $N(a_t) = N(a_t) + 1$
  - 更新期望奖励估值: $\widehat Q(a_t)= \widehat Q(a_t)+\frac{1}{N(a_t)}[r_t-\widehat Q(a_t)]$
- end for

------

以上 for 循环中的第四步如此更新估值，是因为这样可以进行增量式的期望更新，公式如下：
$$
\begin{align}
Q_k 
& = \frac{1}{k}\sum_{i=1}^k r_i \\
& = \frac{1}{k}(r_k + \sum_{i=1}^{k-1}r_i) \\
& = \frac{1}{k}[r_k+(k-1)Q_{k-1}] \\
& = \frac{1}{k}(r_k+kQ_{k-1}-Q_{k-1}) \\
& = Q_{k-1} + \frac{1}{k}(r_k-Q_{k-1})
\end{align}
$$
如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为 $O(n)$。而采用增量式更新，时间复杂度和空间复杂度均为 $O(1)$。

## 探索与利用的平衡

还没有一个策略告诉我们应该采取哪个动作，即拉动哪根拉杆，所以接下来考虑如何设计一个策略。

例如，一个最简单的策略就是一直采取第一个动作，但这就非常依赖运气的好坏。如果运气绝佳，可能拉动的刚好是能获得最大期望奖励的拉杆，即最优拉杆；但如果运气很糟糕，获得的就有可能是最小的期望奖励。在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。**探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。例如，对于一个 10 臂老虎机，我们只拉动过其中 3 根拉杆，接下来就一直拉动这 3 根拉杆中期望奖励最大的那根拉杆，但很有可能期望奖励最大的拉杆在剩下的 7 根当中，即使我们对 10 根拉杆各自都尝试了 20 次，发现 5 号拉杆的经验期望奖励是最高的，但仍然存在着微小的概率—另一根 6 号拉杆的真实期望奖励是比 5 号拉杆更高的。

于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如 $\epsilon$-贪婪算法、上置信界算法和汤普森采样算法等。

**Code is available at [K-armed bandit](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/K-armed%20bandit.ipynb)**

## $\epsilon$-贪婪算法

完全贪婪算法即在每一时刻采取期望奖励估值最大的动作（拉动拉杆），这就是纯粹的利用，而没有探索，所以我们通常需要对完全贪婪算法进行一些修改，其中比较经典的一种方法为 $\epsilon$-Greedy 算法。$\epsilon$-Greedy 算法在完全贪婪算法的基础上添加了噪声，每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索），公式如下：
$$
a_t = 
\begin{cases}
arg\; max_{a\in A}\widehat Q(a), & \text {采样概率：} 1-\epsilon \\
\text{从A中随机选择}, & \text{采样概率：}\epsilon
\end{cases}
$$
随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。

所以在 $\epsilon$-贪婪算法的具体实现中，我们可以令$\epsilon$  随时间衰减，即探索的概率将会不断降低。但是请注意，$\epsilon$不会在有限的步数内衰减至 0，因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。

实验证明，随时间做反比例衰减的 $\epsilon$-贪婪算法能够使累积懊悔与时间步的关系变成**次线性**（sublinear）的，这明显优于固定 $\epsilon$ 值的$\epsilon$ -贪婪算法

## 上置信界算法

设想这样一种情况：对于一台双臂老虎机，其中第一根拉杆只被拉动过一次，得到的奖励为 ；第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。这时你会怎么做？或许你会进一步尝试拉动第一根拉杆，从而更加确定其奖励分布。这种思路主要是基于不确定性，因为此时第一根拉杆只被拉动过一次，它的不确定性很高。一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。

我们在此引入不确定性度量 $U(a)$，它会随着一个动作被尝试次数的增加而减小。我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。

**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**（Hoeffding's inequality）。在霍夫丁不等式中，令 $X_1,\cdots,X_n$ 为$n$个独立同分布的随机变量，取值范围为 $[0,1]$，其经验期望为 $\overline{x}_n=\frac{1}{n}\sum^n_{j=1}X_j$，则有 $P\{E[X] \geq \overline{x}_n+u\}\leq e^{-2nu^2}$

现在我们将霍夫丁不等式运用于多臂老虎机问题中。将 $\widehat{Q}_t(a)$ 代入 $\overline{x}_t$，不等式中的参数 $u=\widehat{U}_t(a)$ 代表不确定性度量。给定一个概率 $p=e^{-2N_t(a)U_t(a)^2}$ ，根据上述不等式，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 至少以概率 $1-p$ 成立。当 $p$ 很小时，$Q_t(a) < \widehat{Q}_t(a)+\widehat{U}_t(a)$ 就以很大概率成立，$\widehat{Q}_t(a)+\widehat{U}_t(a)$ 便是期望奖励上界。此时，上置信界算法便选取期望奖励上界最大的动作，即 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+\widehat{U}(a)]$。其中 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)}}$ 。因此，设定一个概率 $p$ 后，就可以计算相应的不确定性度量 $\widehat{U}_t(a)$ 了。更直观地说，UCB 算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 $p$ 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

在具体的实现过程中，设置 $p = \frac{1}{t}$，并且在分母中为拉动每根拉杆的次数加上常数 1，以免出现分母为 0 的情形。即此时 $\widehat{U}_t(a)=\sqrt{\frac{-log\;p}{2N_t(a)+1}}$。同时，设定一个系数 $c$ 来控制不确定性的比重，此时 $a=\underset{a \in A}{arg\;max}[\widehat{Q}(a)+c \cdot \widehat{U}(a)]$

## 汤普森采样算法

MAB 中还有一种经典算法——**汤普森采样**（Thompson sampling），先假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作 $a$ 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。

了解了汤普森采样算法的基本思路后，我们需要解决另一个问题：怎样得到当前每个动作 $a$ 的奖励概率分布并且在过程中进行更新？在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择了 $k$ 次，其中 $m_1$ 次奖励为 1， $m_2$ 次奖励为 0，则该拉杆的奖励服从参数为 $m_1+1,m_2+1$ 的 Beta 分布。

## 总结

通过实验我们可以得到以下结论： $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。

多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。



# 马尔可夫决策过程

与多臂老虎机问题不同，**马尔可夫决策过程**（Markov decision process，MDP）包含状态信息以及状态之间的转移机制。如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

**Code is available at [MDP](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Markov%20decision%20process.ipynb)**

## 马尔可夫过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于 $t$ 时刻之前的状态。我们将已知历史信息 $S_1,\cdots,S_t$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示成 $P(S_{t+1}|S_1,\cdots,S_t)$

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为 $P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\cdots,S_t)$。也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然  $t+1$ 时刻的状态只与 $t$ 时刻的状态有关，但是 $t$ 时刻的状态其实包含了 $t-1$ 时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。我们通常用元组 $<S,P>$ 描述一个马尔可夫过程，其中 $S$ 是有限数量的状态集合，$P$ 是**状态转移矩阵**（state transition matrix）。假设一共有 $n$ 个状态，此时 $S=\{s_1,s_2,\cdots,s_n\}$。状态转移矩阵 $P$ 定义了所有状态对之间的转移概率，即
$$
\boldsymbol{P}=
\left(\begin{array}{cccc}
    p\left(s_{1} \mid s_{1}\right) & p\left(s_{2} \mid s_{1}\right) & \ldots & p\left(s_{n} \mid s_{1}\right) \\
    p\left(s_{1} \mid s_{2}\right) & p\left(s_{2} \mid s_{2}\right) & \ldots & p\left(s_{n} \mid s_{2}\right) \\
    \vdots & \vdots & \ddots & \vdots \\
    p\left(s_{1} \mid s_{n}\right) & p\left(s_{2} \mid s_{n}\right) & \ldots & p\left(s_{n} \mid s_{n}\right)
    \end{array}\right)
$$
矩阵 $P$  中的第 i 行和第 j 列元素 $P(s_j|s_i)=P(S_{t+1}=s_j|S_t=s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率，我们称 $P(s'|s)$ 为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵 $P$ 的每一行的和为 1。

给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**（episode），这个步骤也被叫做**采样**（sampling）。生成序列的概率和状态转移矩阵有关。

## 马尔可夫奖励过程

在马尔可夫过程的基础上加入奖励函数 $r$ 和折扣因子 $\gamma$，就可以得到**马尔可夫奖励过程**（Markov reward process）。一个马尔可夫奖励过程由 $<S,P,r,\gamma>$ 构成，各个组成元素的含义如下所示：

- $S$ 是有限状态的集合
- $P$ 是状态转移矩阵
- $r$ 是奖励函数，某个状态 $s$ 的奖励 $r(s)$ 指转移到该状态时可以获得奖励的期望
- $\gamma$ 是折扣因子（discount factor），$\gamma$ 的取值范围为 $[0,1]$。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励。

在一个马尔可夫奖励过程中，从第 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为**回报** $G_t$（Return），公式如下：
$$
G_t=R_t+\gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k}
$$
在马尔可夫奖励过程中，**一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）**被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成 $V(s)=E[G_t|S_t=s]$，展开为：
$$
\begin{align}
V(s)
& = E[G_t|S_t=s] \\
& = E[R_t+\gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots |S_t=s] \\
& = E[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\cdots)|S_t=s] \\
& = E[R_t+\gamma G_{t+1}|S_t=s] \\
& = E[R_t+\gamma V(S_{t+1})|S_t=s]
\end{align}
$$
在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即 $E[R_t|S_t=s]=r(s)$；另一方面，等式中剩余部分 $E[\gamma V(S_{t+1})|S_t=s]$ 可以根据从状态 $s$ 出发的转移概率得到，即可以得到
$$
V(s)=r(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')
$$
上式就是马尔可夫奖励过程中的**贝尔曼方程**（Bellman equation），对每一个状态都成立。贝尔曼方程就是当前状态与未来状态的迭代关系，表示**当前状态的价值函数可以通过下个状态的价值函数来计算**。贝尔曼方程因其提出者、动态规划创始人理查德 贝尔曼（Richard Bellman）而得名 ，也叫作“动态规划方程”。

若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S=\{s_1,s_2,\cdots,s_n\}$，将所有状态的价值表示成一个列向量 $V=[V(s_1),V(s_2),\cdots,V(s_n)]^T$，同理，将奖励函数写成一个列向量 $R=[r(s_1),r(s_2),\cdots,r(s_n)]^T$。于是可以将贝尔曼方程写成矩阵的形式：
$$
V = R+\gamma PV \\

\left(\begin{array}{c}
    V\left(s_{1}\right) \\
    V\left(s_{2}\right) \\
    \vdots \\
    V\left(s_{N}\right)
    \end{array}\right)=\left(\begin{array}{c}
    R\left(s_{1}\right) \\
    R\left(s_{2}\right) \\
    \vdots \\
    R\left(s_{N}\right)
    \end{array}\right)+\gamma\left(\begin{array}{cccc}
    p\left(s_{1} \mid s_{1}\right) & p\left(s_{2} \mid s_{1}\right) & \ldots & p\left(s_{n} \mid s_{1}\right) \\
    p\left(s_{1} \mid s_{2}\right) & p\left(s_{2} \mid s_{2}\right) & \ldots & p\left(s_{n} \mid s_{2}\right) \\
    \vdots & \vdots & \ddots & \vdots \\
    p\left(s_{1} \mid s_{n}\right) & p\left(s_{2} \mid s_{n}\right) & \ldots & p\left(s_{n} \mid s_{n}\right)
    \end{array}\right)\left(\begin{array}{c}
    V\left(s_{1}\right) \\
    V\left(s_{2}\right) \\
    \vdots \\
    V\left(s_{n}\right)
    \end{array}\right) \\
    
\text{把贝尔曼方程写成矩阵形式后，可以直接求解} \\
\begin{aligned}
V &= R+ \gamma PV \\
IV &= R+ \gamma PV \\
(I-\gamma P)V &= R \\
V &= (I-\gamma P)^{-1} R
\end{aligned}
$$
以上解析解的计算复杂度是 $O(n^3)$，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用**动态规划**（dynamic programming）算法、**蒙特卡洛方法**（Monte-Carlo method）和**时序差分**（temporal difference）。

## 马尔可夫决策过程

马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程，而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**（Markov decision process，MDP）。我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。马尔可夫决策过程由元组 $<S,A,P,r,\gamma>$构成，其中：

- $S$ 是有限状态的集合
- $A$ 是动作的集合
- $P(s'|s,a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s'$ 的概率
- $r(s,a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$，在奖励函数只取决于状态 $s$ 时，则退化为 $r(s)$
- $\gamma$ 是折扣因子

MDP 与 MRP 非常相像，主要区别为 MDP 中的状态转移函数和奖励函数都比 MRP 多了动作 $a$ 作为自变量。

注意，在上面 MDP 的定义中，我们不再使用类似 MRP 定义中的状态转移矩阵方式，而是直接表示成了状态转移函数。这样做一是因为此时状态转移与动作也有关，变成了一个三维数组，而不再是一个矩阵（二维数组）；二是因为状态转移函数更具有一般意义，例如，如果状态集合不是有限的，就无法用数组表示，但仍然可以用状态转移函数表示。

不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。例如，一艘小船在大海中随着水流自由飘荡的过程就是一个马尔可夫奖励过程，它如果凭借运气漂到了一个目的地，就能获得比较大的奖励；如果有个水手在控制着这条船往哪个方向前进，就可以主动选择前往目的地获得比较大的奖励。马尔可夫决策过程是一个与时间相关的不断进行的过程，在智能体和环境 MDP 之间存在一个不断交互的过程。

它们之间的交互如下图所示，智能体根据当前状态 $S_t$ 选择动作 $A_t$；对于状态 $S_t$ 和动作 $A_t$，MDP 根据奖励函数和状态转移函数得到 $S_{t+1}$ 和 $R_t$ 并反馈给智能体。智能体的目标是最大化得到的累计奖励。智能体根据当前状态从动作的集合 $A$ 中选择一个动作的函数，被称为策略。

<img src="https://hrl.boyuai.com/static/rl-process.723b4a67.png" style="zoom:67%;" />

智能体的**策略**（Policy）通常用字母 $\pi$ 表示。策略 $\pi(a|s)=P(A_t=a|S_t=s)$ 是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率。当一个策略是**确定性策略**（deterministic policy）时，它在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0；当一个策略是**随机性策略**（stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。

用 $V^{\pi}(s)$ 表示在 MDP 中**基于策略 $\pi$ 的状态价值函数**（state-value function），定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为：$V^{\pi}(s)=E_{\pi}[G_t|S_t=s]$

不同于 MRP，在 MDP 中，由于动作的存在，额外定义一个**动作价值函数**（action-value function），用 $Q^{\pi}(s,a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报：$Q^{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]$

**状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果：$V^{\pi}(s)=\sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)$**

使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：$Q^{\pi}(s,a)=r(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V^{\pi}(s')$

通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）：
$$
\begin{align}
V^{\pi}(s)
& = E_{\pi}[G_t|S_t=s] \\
& = E_{\pi}[R_t+\gamma V^{\pi}(S_{t+1})|S_t=s] \\
& = \sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')] \\

Q^{\pi}(s,a)
& = E_{\pi}[G_t|S_t=s,A_t=a] \\
& = E_{\pi}[R_t+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
& = r(s,a) + \gamma \sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi (a'|s')Q^{\pi}(s',a')
\end{align}
$$
价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的！

如果想要计算某 MDP 下，一个策略 $\pi$ 的状态价值函数。我们现在有的工具是 MRP 的解析解方法。于是，一个很自然的想法是：给定一个 MDP 和一个策略 $\pi$ ，我们是否可以将其转化为一个 MRP？答案是肯定的。我们可以将策略的动作选择进行**边缘化**（marginalization)，就可以得到没有动作的 MRP 了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个 MRP 在该状态下的奖励，即：$r'(s)=\sum_{a \in A}\pi(a|s)r(s,a)$

同理，我们计算采取动作的概率与使 $s$ 转移到 $s'$ 的概率的乘积，再将这些乘积相加，其和就是一个 MRP 的状态从 $s$ 转移到 $s'$ 的概率：$P'(s'|s)=\sum_{a \in A}\pi(a|s)P(s'|s,a)$

于是，我们构建得到了一个 MRP: $<S,P',r',\gamma>$。根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。

## 蒙特卡洛方法

**蒙特卡洛方法**（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。如下图所示的正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。

$\frac{\text{圆的面积}}{\text{正方形的面积}}=\frac{\text{圆中点的个数}}{\text{正方形中点的个数}}$<img src="https://hrl.boyuai.com/static/mc.c89f09b0.png" style="zoom:50%;" />

一个状态的价值是它的期望回报，那么一个很直观的想法就是用蒙特卡洛策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望，公式如下：$V^{\pi}(s)=E_{\pi}[G_t|S_t=s] \approx \frac{1}{N}\sum_{i=1}^NG_t^{(i)}$

在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略 $\pi$ 从状态 $s$ 开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示：

- 使用策略 $\pi$ 采样若干条序列：$s_0^{(i)} \stackrel{a_0^{{(i)}}}{\longrightarrow} r_0^{(i)},\;s_1^{(i)} \stackrel{a_1^{{(i)}}}{\longrightarrow} r_1^{(i)},\;s_2^{(i)} \stackrel{a_2^{{(i)}}}{\longrightarrow}\cdots \stackrel{a_{T-1}^{{(i)}}}{\longrightarrow}r_{T-1}^{(i)},\;s_T^{(i)}$
- 对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作
  - 更新状态 $s$ 的计数器 $N(s)\leftarrow N(s)+1$
  - 更新状态 $s$ 的总回报 $M(s) \leftarrow M(s)+G_t$
- 每一个状态的价值被估计为回报的平均值 $V(s)=\frac{M(s)}{N(s)}$

根据大数定律，当 $N(s)\to \infty$，有 $V(s)\to V^{\pi}(s)$ 。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态 $s$ 和对应回报 $G$ ，进行如下计算：

- $N(s)\leftarrow N(s)+1$
- $V(s) \leftarrow V(s)+\frac{1}{N(s)}(G-V(s))$ 

实验证明，使用蒙特卡洛方法估计得到的状态价值和使用 MRP 解析解得到的状态价值是很接近的。

## 占用度量

易知，不同策略的价值函数是不一样的，这是因为对于同一个 MDP，不同策略会访问到的状态的概率分布是不同的。因此我们需要理解不同策略会使智能体访问到不同概率分布的状态这个事实，这会影响到策略的价值函数。

定义 MDP 的初始状态分布为 $v_0(s)$，用 $P^{\pi}_t(s)$ 表示采取策略 $\pi$ 使得智能体在 $t$ 时刻状态为 $s$ 的概率，所以有 $P^{\pi}_0(s)=v_0(s)$，然后就可以定义一个策略的**状态访问分布**（state visitation distribution）：$v^{\pi}(s)=(1-\gamma)\sum_{t=0}^{\infty}\gamma ^tP_t^{\pi}(s)$。

其中，$1-\gamma$ 是用来使得概率加和为 1 的归一化因子。状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：$v^{\pi}(s')=(1-\gamma)v_0(s')+\gamma\int{P(s'|s,a)\pi(a|s)v^{\pi}(s)dsda}$

此外，还可以定义策略的**占用度量**（occupancy measure）：$\rho^{\pi}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma ^tP_t^{\pi}(s)\pi(a,s)$

它表示动作状态对 $(s,a)$ 被访问到的概率。二者之间存在如下关系：$\rho^{\pi}(s,a)=v^{\pi}(s)\pi(a,s)$

- 进一步得出如下两个定理:
  - **定理 1**：智能体分别以策略 $\pi_1$ 和 $\pi_2$ 和同一个 MDP 交互得到的占用度量 $\rho^{\pi_1}(s,a)$ 和 $\rho^{\pi_2}(s,a)$ 满足：$\rho^{\pi_1} = \rho^{\pi_2}\Longleftrightarrow \pi_1=\pi_2$
  - **定理 2**：给定一合法占用度量 $\rho$，可生成该占用度量的唯一策略是 $\pi_\rho=\frac{\rho(s,a)}{\sum_{a'}\rho(s,a')}$

注意：以上提到的“合法”占用度量是指存在一个策略使智能体与 MDP 交互产生的状态动作对被访问到的概率。

实验证明，不同策略对于同一个状态动作对的占用度量是不一样的。

## 最优策略

强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。

首先定义策略之间的偏序关系：当且仅当对于任意的状态 $s$ 都有 $V^{\pi}(s)\geq V^{\pi'}(s)$，记 $\pi > \pi'$

于是在有限状态和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是**最优策略**（optimal policy）。最优策略可能有很多个，都将其表示为 $\pi^*(s)$

最优策略都有相同的状态价值函数，称之为**最优状态价值函数**，表示为：$V^*(s)=\underset{\pi}{max}V^{\pi}(s), \forall s \in S$

同理，定义**最优动作价值函数**：$Q^*(s,a)=\underset{\pi}{max}Q^{\pi}(s,a), \forall s \in S, \forall a \in A$

为了使 $Q^{\pi}(s,a)$ 最大，我们需要在当前的状态动作对 $(s,a)$ 之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：$Q^*(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^*(s')$

这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：$V^*(s)=\underset{a\in A}{max}Q^*(s,a)$

根据 $V^*(s)$ 和 $Q^*(s,a)$ 的关系，得到**贝尔曼最优方程**（Bellman optimality equation）：
$$
V^*(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^*(s')\} \\
Q^*(s,a)=r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)\underset{a'\in A}{max}Q^*(s',a')
$$


# 动态规划算法

**动态规划**（dynamic programming）是程序设计算法中非常重要的内容，能够高效解决一些经典问题，例如背包问题和最短路径规划。动态规划的基本思想是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到目标问题的解。动态规划会保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。

利用用动态规划的思想来可以求解在马尔可夫决策过程中的最优策略。

基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。具体来说，策略迭代中的策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程；而价值迭代直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。

不同于蒙特卡洛方法和时序差分算法，基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。另外，**策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的**。

**Code is available at [DP](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Dynamic%20programming.ipynb)**

使用策略迭代和价值迭代来求解**悬崖漫步**（Cliff Walking）这个环境中的最优策略。悬崖漫步是一个非常经典的强化学习环境，它要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。

## 策略迭代算法

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 策略评估

策略评估这一过程用来计算一个策略的状态价值函数。

由贝尔曼期望方程 $V^{\pi}(s)=\sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')]$

其中，$\pi(a|s)$ 是策略 $\pi$ 在状态 $s$ 下采取动作 $a$ 的概率。可以看到，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。

更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即 $V^{k+1}(s)=\sum_{a \in A}\pi(a|s)[r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{k}(s')]$

我们可以选定任意初始值 $V^0$。根据贝尔曼期望方程，可以得知 $V^k=V^{\pi}$  是以上更新公式的一个**不动点**（fixed point）。事实上，可以证明当 $k\to \infty$ 时，序列 $\{V^k\}$ 会收敛到 $V^{\pi}$，所以可以据此来计算得到一个策略的状态价值函数。

可以看到，由于需要不断做贝尔曼期望方程迭代，策略评估其实会耗费很大的计算代价。在实际的实现过程中，如果某一轮 $max_{s\in S}|V^{k+1}(s)-V^k(s)|$ 的值非常小，可以提前结束策略评估。这样做可以提升效率，并且得到的价值也非常接近真实的价值。

### 策略提升

使用策略评估计算得到当前策略的状态价值函数之后，我们可以据此来改进该策略。

假设此时对于策略 $\pi$， 我们已经知道其价值 $V^{\pi}$ ，也就是知道了在策略 $\pi$ 下从每一个状态 $s$ 出发最终得到的期望回报。

假设智能体在状态 $s$ 下采取动作 $a$，之后的动作依旧遵循策略 $\pi$，此时得到的期望回报其实就是动作价值 $Q^{\pi}(s,a)$。如果有 $Q^{\pi}(s,a) > V^{\pi}(s)$ ，则说明在状态 $s$ 下采取动作 $a$ 会比原来的策略 $\pi(a|s)$ 得到更高的期望回报。

以上假设只是针对一个状态，现在假设存在一个确定性策略 $\pi'$，在任意一个状态 $s$ 下，都满足：$Q^{\pi}(s,\pi'(s)) > V^{\pi}(s)$ 。于是在任意状态 $s$ 下，有 $V^{\pi'}(s) \geq V^{\pi}(s)$

这便是**策略提升定理**（policy improvement theorem）。于是我们可以直接贪心地在每一个状态选择动作价值最大的动作，也就是
$$
\pi'(s)=arg\;\underset{a}{max}Q^{\pi}(s,a)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{\pi}(s')\}
$$
我们发现构造的贪心策略 $\pi'$ 满足策略提升定理的条件，所以策略 $\pi'$ 能够比策略 $\pi$ 更好或者至少与其一样好。这个根据贪心法选取动作从而得到新的策略的过程称为策略提升。当策略提升之后得到的策略 $\pi'$ 和之前的策略 $\pi$ 一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi'$ 就是最优策略。

### 策略迭代算法

总体来说，策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略。

结合策略评估和策略提升，得到以下策略迭代算法：

- 随机初始化策略 $\pi(s)$ 和价值函数 $V(s)$
- while $\Delta > \theta$ do:
  - $\Delta \leftarrow 0$
  - 对于每一个状态 $s \in S$:
    - $v \leftarrow V(s)$
    - $V(s)\leftarrow r(s,\pi(s))+\gamma \sum_{s'}p(s'|s,\pi(s))V(s')$
    - $\Delta \leftarrow max(\Delta,|v-V(s)|)$
- end while
- $\pi_{old}\leftarrow \pi$
- 对于每一个状态 $s \in S$:
  - $\pi(s)\leftarrow arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$
- 若 $\pi_{old}= \pi$, 则停止算法并返回 $V$ 和 $\pi$ ; 否则转到策略评估循环

## 价值迭代算法

从策略迭代算法的实验结果可以发现，策略迭代中的策略评估需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下。

可能出现这样的情况：虽然状态价值函数还没有收敛，但是不论接下来怎么更新状态价值，策略提升得到的都是同一个策略。

价值迭代算法就是只在策略评估中进行一轮价值更新，然后直接根据更新后的价值进行策略提升。它可以被认为是一种策略评估只进行了一轮更新的策略迭代算法。需要注意的是，价值迭代中不存在显式的策略，我们只维护一个状态价值函数。

价值迭代可以看成一种动态规划过程，它利用的是贝尔曼最优方程：$V^*(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^*(s')\}$

将其写成迭代更新的方式为：$V^{k+1}(s)=\underset{a\in A}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^k(s')\}$

价值迭代便是按照以上更新方式进行的。等到 $V^{k+1}$ 和 $V^{k}$ 相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数 $V^*$。然后利用 $\pi(s)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s' \in S}p(s'|s,a)V^{k+1}(s')\}$，从中恢复出最优策略即可。

价值迭代算法流程如下：

- 随机初始化价值函数 $V(s)$
- while $\Delta > \theta$ do:
  - $\Delta \leftarrow 0$
  - 对于每一个状态 $s \in S$:
    - $v \leftarrow V(s)$
    - $V(s)\leftarrow \underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$
    - $\Delta \leftarrow max(\Delta,|v-V(s)|)$
- end while
- 返回一个确定性策略 $\pi(s)=arg\;\underset{a}{max}\{r(s,a)+\gamma \sum_{s'}p(s'|s,a)V(s')\}$

实验结果说明，解决同样的训练任务，价值迭代总共进行了数十轮，而策略迭代中的策略评估总共进行了数百轮，价值迭代中的循环次数远少于策略迭代。



# 时序差分算法

动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。

但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习**（model-free reinforcement learning）。

不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。

无模型的强化学习中有两大经典算法：Sarsa 和 Q-learning，它们都是基于**时序差分**（temporal difference，TD）的强化学习算法。

此外，还存在一组概念：在线策略学习和离线策略学习。通常来说，在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。

**Code is available at [TD](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Temporal%20difference.ipynb)**

## 时序差分方法

时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。

蒙特卡洛方法对价值函数的增量更新方式：$V(s_t) \leftarrow V(s_t)+\alpha(G_t-V(s_t))$ 。将原始的 $\frac{1}{N(s)}$ 替换成了 $\alpha$，表示对价值估计更新的步长。可以将 $\alpha$ 取为一个常数，此时更新方式不再像蒙特卡洛方法那样严格地取期望。

蒙特卡洛方法必须要等整个序列结束之后才能计算得到这一次的回报 $G_t$，而时序差分方法只需要当前步结束即可进行计算。具体来说，**时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报**，即：
$$
V(s_t) \leftarrow V(s_t)+\alpha[r_t+\gamma V(s_{t+1})-V(s_t)]
$$
其中 $r_t+\gamma V(s_{t+1})-V(s_t)$ 通常被称为**时序差分**（temporal difference，TD）**误差**（error），时序差分算法将其与步长的乘积作为状态价值的更新量。可以用 $r_t+\gamma V(s_{t+1})$ 来代替 $G_t$ 的原因是：
$$
\begin{align}
V_{\pi}(s) 
& = E_{\pi}[G_t|S_t=s] \\
& = E_{\pi}[\sum_{k=0}^{\infty}\gamma^kr_{t+k}|S_t=s] \\
& = E_{\pi}[r_t+\gamma\sum_{k=0}^{\infty}\gamma^kr_{t+k+1}|S_t=s] \\
& = E_{\pi}[r_t+\gamma V_{\pi}(S_{t+1})|S_t=s]
\end{align}
$$
因此蒙特卡洛方法将上式第一行作为更新的目标，而时序差分算法将上式最后一行作为更新的目标。于是，在用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了 $V(s_{t+1})$ 的估计值，可以证明它最终收敛到策略 $\pi$ 的价值函数。

**时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。**

## Sarsa 算法

既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是时可以直接用时序差分算法来估计动作价值函数 $Q$：$Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$

然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即 $\underset{a}{arg\;max}\;Q(s,a)$。这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。

然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们需要用极大量的样本来进行更新。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了，我们可以这么做的原因是策略提升可以在策略评估未完全进行的情况进行，这其实是**广义策略迭代**（generalized policy iteration）的思想。第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对 $(s,a)$ 永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。

简单常用的解决方案是不再一味使用贪婪算法，而是采用一个 $\epsilon$-贪婪策略：有 $1-\epsilon$ 的概率采用动作价值最大的那个动作，另外有 $\epsilon$ 的概率从动作空间中随机采取一个动作，其公式表示为：
$$
\pi(a,s) = 
\begin{cases}
\frac{\epsilon}{|A|}+1-\epsilon, & \text {如果} a = arg\; max_{a'}Q(s,a') \\
\frac{\epsilon}{|A|}, & \text{其他动作}
\end{cases}
$$
现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态 $s$、当前动作 $a$、获得的奖励 $r$、下一个状态 $s'$ 和下一个动作 $a'$，将这些符号拼接后就得到了算法名称。Sarsa 的具体算法如下：

![](https://datawhalechina.github.io/easy-rl/img/ch3/3.15.png)

## 多步 Sarsa 算法

蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是**无偏**（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。

结合二者优势的方法：**多步时序差分**。多步时序差分的意思是使用步的奖励，然后使用之后状态的价值估计。

多步（n步）Sarsa 算法的公式表示：
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+ \gamma r_{t+1} + \cdots+\gamma^n Q(s_{t+n},a_{t+n})-Q(s_t,a_t)]
$$

## Q-learning 算法

除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为：
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t)+\alpha[r_t+ \gamma\;\underset{a}{max} Q(s_{t+1},a)-Q(s_t,a_t)]
$$
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.18.png)

我们可以用价值迭代的思想来理解 Q-learning，即 Q-learning 是直接在估计 $Q^*$，因为动作价值函数的贝尔曼最优方程是 $Q^*(s,a)=r(s,a) + \gamma \sum_{s' \in S}p(s'|s,a)\underset{a'\in A}{max}Q^*(s',a')$

而 Sarsa 估计当前 $\epsilon$-贪婪策略的动作价值函数。需要强调的是，Q-learning 的更新并非必须使用当前贪心策略 $arg\;max_a\;Q(s,a)$ 采样得到的数据，因为给定任意 $(s,a,r,s')$都可以直接根据更新公式来更新 $Q$，为了探索，我们通常使用一个 $\epsilon$-贪婪策略来与环境交互。Sarsa 必须使用当前 $\epsilon$-贪婪策略采样得到的数据，因为它的更新中用到的 $Q(s',a')$ 的 $a'$ 是当前策略在 $s'$ 下的动作。

我们称 Sarsa 是**在线策略**（on-policy）算法，称 Q-learning 是**离线策略**（off-policy）算法。

我们称采样数据的策略为**行为策略**（behavior policy），称用这些数据来更新的策略为**目标策略**（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。

判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，具体而言：

- 对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组 $(s,a,r,s',a')$，因此它是在线策略学习方法
- 对于 Q-learning，它的更新公式使用的是四元组 $(s,a,r,s')$ 来更新当前状态动作对的价值 $Q(s,a)$，数据中的 $s$ 和 $a$ 是给定的条件，$r$ 和 $s'$ 皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法

![](https://hrl.boyuai.com/static/400.78f393db.png)

比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q-learning 是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。

值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为**离线强化学习**（offline reinforcement learning）

![](https://datawhalechina.github.io/easy-rl/img/ch3/3.21.png)

# Dyna-Q 算法

在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：**基于模型的强化学习**（model-based reinforcement learning）和**无模型的强化学习**（model-free reinforcement learning）。

无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，Sarsa 和 Q-learning 算法便是两种无模型的强化学习方法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。两种动态规划算法，策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。

强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。

**Dyna-Q 算法是一个经典的基于模型的强化学习算法。**Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态 $s$，采取一个曾经在该状态下执行过的动作 $a$，通过模型得到转移后的状态 $s'$ 以及奖励 $r$ ，并根据这个模拟数据 $(s,a,r,s')$，用 Q-learning 的更新方式来更新动作价值函数。

![](https://hrl.boyuai.com/static/480.25b67b37.png)

Dyna-Q 算法的具体流程：

- 初始化 $Q(s,a)$，初始化模型 $M(s,a)$
- for 序列 $e=1 \rightarrow E$ do:
  - 得到初始状态 $s$
  - for $t=1 \rightarrow T$ do:
    - 用 $\epsilon$-贪婪策略根据 $Q$ 选择当前状态 $s$ 下的动作 $a$
    - 得到环境反馈的 $r,s'$
    - $Q(s,a) \leftarrow Q(s,a)+\alpha[r+ \gamma\;\underset{a'}{max} Q(s',a')-Q(s,a)]$
    - $M(s,a) \leftarrow r,s'$
    - for 次数 $n=1 \rightarrow N$ do:
      - 随机选择一个曾经访问过的状态 $s_m$
      - 采取一个曾经在状态 $s_m$ 下执行过的动作 $a_m$
      - $r_m,s'_m \leftarrow M(s_m,a_m)$
      - $Q(s_m,a_m) \leftarrow Q(s_m,a_m)+\alpha[r_m+ \gamma\;\underset{a'}{max} Q(s'_m,a')-Q(s_m,a_m)]$
    - end for
    - $s \leftarrow s'$
  - end for
- end for

可以看到，在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做 $N$ 次 Q-planning。其中 Q-planning 的次数 $N$ 是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据 $(s,a,r,s')$ 时，可以直接对模型做出更新，即 $M(s,a) \leftarrow r,s'$

**Code is available at [Dyna-Q](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Dyna-Q.ipynb)**

实验证明，随着 Q-planning 步数的增多，Dyna-Q 算法的收敛速度也随之变快。当然，并不是在所有的环境中，都是 Q-planning 步数越大则算法收敛越快，这取决于环境是否是确定性的，以及环境模型的精度。在悬崖漫步环境中，状态的转移是完全确定性的，构建的环境模型的精度是最高的，所以可以通过增加 Q-planning 步数来直接降低算法的样本复杂度。



# DQN 算法

在 Q-learning 算法中，以矩阵的方式建立了一张存储每个状态下所有动作 $Q$ 值的表格。表格中的每一个动作价值 $Q(s,a)$ 表示在状态 $s$ 下选择动作 $a$ 然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，当状态或者动作数量非常大的时候，这种做法就不适用了。更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的 $Q$ 值。

对于这种情况，我们需要用函数拟合的方法来估计 $Q$ 值，即将这个复杂的 $Q$ 值表格视作数据，使用一个参数化的函数 $Q_{\theta}$ 来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。而**深度Q网络(deep Q-network，DQN)**算法便可以用来解决连续状态下离散动作的问题。

以 [CartPole](https://github.com/openai/gym/wiki/CartPole-v0) 问题为例，它的状态值就是连续的，动作值是离散的。

类似车杆的环境中得到动作价值函数 $Q(s,a)$，由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是使用**函数拟合**（function approximation）的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数 $Q$。若动作是连续（无限）的，神经网络的输入是状态 $s$ 和动作 $a$，然后输出一个标量，表示在状态 $s$ 下采取动作 $a$ 能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，我们还可以只将状态 $s$ 输入到神经网络中，使其同时输出每一个动作的 $Q$ 值。

通常 DQN（以及 Q-learning）只能处理动作离散的情况，因为在函数 $Q$ 的更新过程中有 $max_a$ 这一操作。假设神经网络用来拟合函数的参数是 $w$，即每一个状态 $s$ 下所有可能动作 $a$ 的 $Q$ 值我们都能表示为 $Q_w(s,a)$。我们将用于拟合函数的神经网络称为 **Q 网络**

<img src="https://hrl.boyuai.com/static/640.46b13e89.png" style="zoom: 67%;" />

回顾 Q-learning 的更新规则：$Q(s,a) \leftarrow Q(s,a)+\alpha[r+ \gamma\;\underset{a'\in A}{max} Q(s',a')-Q(s,a)]$
上述公式用**时序差分**（temporal difference，TD）学习目标 $r+ \gamma\;\underset{a'\in A}{max} Q(s',a')$ 来增量式更新 $Q(s,a)$，也就是说要使 $Q(s,a)$ 和 TD 目标 $r+ \gamma\;\underset{a'\in A}{max} Q(s',a')$ 靠近。

于是，对于一组数据 $\{(s_i,a_i,r_i,s'_i)\}$，我们可以很自然地将 **Q 网络的损失函数**构造为**均方误差**的形式：
$$
w^*=\underset{w}{arg\;min\;}\frac{1}{2N}\sum_{i=1}^N[Q_w(s_i,a_i)-(r_i+\gamma\; \underset{a'}{max}\;Q_w(s'_i,a'))]^2
$$
至此，我们就可以将 Q-learning 扩展到神经网络形式——**深度 Q 网络**（deep Q network，DQN）算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个 $\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——**经验回放**和**目标网络**，它们能够帮助 DQN 取得稳定、出色的性能。

## 经验回放

在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次 $Q$ 值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用:

- 使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。
- 提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

<img src="https://datawhalechina.github.io/easy-rl/img/ch6/6.18.png" style="zoom:10%;" />

## 目标网络

**DQN 算法最终更新的目标是让 $Q_w(s,a)$ 逼近 $r+ \gamma\;\underset{a'\in A}{max}\;Q_w(s',a')$** ，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。

为了解决这一问题，DQN 便使用了**目标网络**（target network）的思想：**既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住**。为了实现这一思想，我们需要利用两套 Q 网络。

- 原来的训练网络 $Q_w(s,a)$ ，用于计算原来的损失函数 $\frac{1}{2}[Q_w(s,a)-(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))]^2$ 中的 $Q_w(s,a)$ 项，并且使用正常**梯度下降**方法来进行更新
- 目标网络 $Q_{w^{-}}(s,a)$，用于计算原先损失函数 $\frac{1}{2}[Q_w(s,a)-(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))]^2$ 中的 $(r+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))$ 项，其中 $w^{-}$ 表示目标网络中的参数。如果两套网络的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套较旧的参数，训练网络 $Q_w(s,a)$ 在训练中的每一步都会更新，而目标网络的参数每隔 $C$ 步才会与训练网络同步一次。这样做使得目标网络相对于训练网络更加稳定。

<img src="https://datawhalechina.github.io/easy-rl/img/ch6/6.12.png" style="zoom: 10%;" />

------

**DQN 算法流程如下：**

- 用随机的网络参数 $w$ 初始化网络 $Q_w(s,a)$
- 复制相同的参数 $w^- \leftarrow w$ 来初始化目标网络 $Q_{w'}$
- 初始化经验回放池 $R$
- for 序列 $e = 1 \rightarrow E$ do
  -  获取环境初始状态 $s_1$
  -  for 时间步 $t=1 \rightarrow T$ do
    -  根据当前网络 $Q_w(s,a)$以 $\epsilon$-贪婪策略选择动作 $a_t$
    -  执行动作 $a_t$，获得回报 $r_t$，环境状态变为 $s_{t+1}$
    -  将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池中
    -  若 $R$ 中数据足够，从中采样 $N$ 个数据 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
    -  对每个数据，用目标网络计算 $y_i=r_i+\gamma\; \underset{a}{max}\;Q_{w^{-}}(s_{i+1},a))$
    -  最小化目标损失 $L=\frac{1}{N}\sum_i(y_i-Q_w(s_i,a_i))^2$，以此更新当前网络 $Q_w$
    -  更新目标网络
  - end for
- end for

**Code is available at [DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Deep%20Q%20network.ipynb)**

## Double DQN

- **普通的 DQN 算法通常会导致对 $Q$ 值的过高估计（overestimation）**，原因是
  - 采样不一致性（Sampling Inconsistency）：DQN 使用经验回放（experience replay）来训练深度神经网络。在经验回放中，DQN 从经验池中随机采样一批数据进行训练。由于网络的参数在训练过程中是不断更新的，不同时间点的网络参数会导致在采样过程中出现不一致性。这就意味着，当估计目标值时，DQN 使用当前网络参数和下一个状态的最大估计值，但这两者可能是不同时间点的估计
  - 动作选择偏差（Action Selection Bias）：在 DQN 中，为了探索环境并平衡探索与利用的权衡，通常使用 ε-贪心（ε-greedy）策略进行动作选择。这意味着以 ε 的概率选择一个随机动作，以 1-ε 的概率选择当前估计值最大的动作。然而，在估计值存在过高估计的情况下，ε-贪心策略可能会导致对过高估计值的过度偏好。因为即使对于真实价值较低的动作，由于过高估计的存在，它们在选择时也有较高的概率被选中

传统 DQN 优化的 TD 误差目标为：$r_i+\gamma\; \underset{a'}{max}\;Q_{w^{-}}(s',a'))$

其中 $\underset{a'}{max}\;Q_{w^{-}}(s',a'))$ 由目标网络（参数为$w^-$）计算得出，我们还可以将其写成如下形式：$Q_{w^{-}}(s',\underset{a'}{arg\;max\;}Q_{w^-}(s',a'))$

- 换句话说，$max$ 操作实际可以被拆解为两部分
  - 首先选取状态 $s'$ 下的最优动作 $a^*=\underset{a'}{arg\;max\;}Q_{w^-}(s',a')$
  - 接着计算该动作对应的价值 $Q_{w^-}(s',a^*)$

当这两部分采用同一套 Q 网络进行计算时，每次得到的都是神经网络当前估算的所有动作价值中的最大值。考虑到通过神经网络估算的 $Q$ 值本身在某些时候会产生正向或负向的误差，在 DQN 的更新方式下神经网络会将正向误差累积。对于动作空间较大的任务，DQN 中的过高估计问题会非常严重，造成 DQN 无法有效工作的后果。

为了解决这一问题，**双深度Q网络(Double DQN)** 算法提出利用两个独立训练的神经网络估算 $\underset{a'}{max}\;Q_*(s',a')$。具体做法是将原有的 $\underset{a'}{max}\;Q_{w^{-}}(s',a')$ 更改为 $Q_{w^{-}}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))$，**即利用一套神经网络 $Q_w$ 的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络 $Q_w^-$ 计算该动作的价值**。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的 $Q$ 值不会存在很大的过高估计问题。

在传统的 DQN 算法中，本来就存在两套 $Q$ 函数的神经网络——目标网络和训练网络，只不过 $\underset{a'}{max}\;Q_{w^{-}}(s',a')$的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为 Double DQN 算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络计算 $Q$ 值，这便是 Double DQN 的主要思想。由于在 DQN 算法中将训练网络的参数记为 $w$ ，将目标网络的参数记为 $w^-$，这与 Double DQN 的两套神经网络的参数是统一的，因此，我们可以直接写出如下 Double DQN 的优化目标：
$$
r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))
$$
显然，DQN 与 Double DQN 的差别只是在于计算状态 $s'$ 下 $Q$ 值时如何选取动作：

- DQN 的优化目标可以写为 $r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_{w^-}(s',a'))$，动作的选取依靠目标网络 $Q_{w^-}$
- Double DQN 的优化目标为 $r+\gamma Q_{w^-}(s',\underset{a'}{arg\;max\;}Q_w(s',a'))$ ，动作的选取依靠训练网络 $Q_w$

所以 Double DQN 的代码实现可以直接在 DQN 的基础上进行，无须做过多修改。

**Code is available at [Double DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Double%20Deep%20Q%20network.ipynb)**

实验证明，与普通的 DQN 相比，Double DQN 比较少出现 $Q$ 值大于 0 的情况，说明 $Q$ 值过高估计的问题得到了很大缓解

## Dueling DQN

**竞争深度Q网络(Dueling DQN)** 是 DQN 另一种的改进算法，它在传统 DQN 的基础上只进行了微小的改动，但却能大幅提升 DQN 的表现。在强化学习中，我们将**动作价值函数 $Q$ 减去状态价值函数 $V$ 的结果定义为优势函数 $A$**，即 $A(s,a)=Q(s,a)-V(s)$。

- **在同一个状态下，所有动作的优势值之和为 0，因为所有动作的动作价值的期望就是这个状态的状态价值**。推导如下：
  - 对于任意状态 $s$，我们可以将动作价值函数表示为每个动作的期望加上其相对于平均值的优势值之和：$Q(s,a)=V(s)+A(s,a)$
  - 如果我们将所有动作的优势值相加：$\sum_aA(s,a)=\sum_a[Q(s,a)-V(s)]$
  - 根据加法的结合律：$\sum_aA(s,a)=\sum_aQ(s,a)-\sum_aV(s)$
  - 由于状态 $s$ 下的所有动作的期望值之和等于状态价值函数：$\sum_aQ(s,a)=\sum_aV(s)=V(s)$
  - 因此得到：$\sum_aA(s,a)=V(s)-V(s)=0$

这个结果表明，动作价值函数和状态价值函数之间存在一种平衡关系，所有动作的相对优势在期望上互相抵消，导致优势值之和为零

据此，在 Dueling DQN 中，Q 网络被建模为：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)
$$
其中，$V_{\eta,\alpha}(s)$ 为状态价值函数，而 $A_{\eta,\beta}(s,a)$ 则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；$\eta$ 是状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而 $\alpha,\beta$ 分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出 $Q$ 值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到 $Q$ 值。Dueling DQN 的网络结构如下图所示：

<img src="https://hrl.boyuai.com/static/640.455bc383.png" style="zoom: 67%;" />

**将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。**

对于 Dueling DQN 中的公式 $Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)$ ，它存在对于 $V$ 值和 $A$ 值建模不唯一性的问题。例如，对于同样的 $Q$ 值，如果将 $V$ 值加上任意大小的常数 $C$ ，再将所有 $A$ 值减去 $C$，则得到的 $Q$ 值依然不变，这就导致了训练的不稳定性。为了解决这一问题，Dueling DQN 强制最优动作的优势函数的实际输出为 0，即：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\underset{a'}{max}\;A_{\eta,\beta}(s,a')
$$
此时 $V(s)=\underset{a}{max}\;A_{\eta,\beta}(s,a)$ ，可以确保 $V$​ 值建模的唯一性。在实现过程中，我们还可以用平均代替最大化操作，即：
$$
Q_{\eta,\alpha,\beta}(s,a)=V_{\eta,\alpha}(s)+A_{\eta,\beta}(s,a)-\frac{1}{|A|}\sum_{a'}A_{\eta,\beta}(s,a')
$$
此时 $V(s)=\frac{1}{|A|}\sum_{a'}A_{\eta,\beta}(s,a')$ 

Dueling DQN 会比 DQN 好的部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数 $V$ 都会被更新，这也会影响到其他动作的 $Q$ 值。而传统的 DQN 只会更新某个动作的 $Q$ 值，其他动作的 $Q$ 值就不会更新。因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数。

**Code is available at [Dueling DQN](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Double%20Deep%20Q%20network.ipynb)**

实验证明，相比于传统的 DQN，Dueling DQN 在多个动作选择下的学习更加稳定，得到的回报最大值也更大。由 Dueling DQN 的原理可知，随着动作空间的增大，Dueling DQN 相比于 DQN 的优势更为明显。此前在环境中设置的离散动作数为 11，我们可以增加离散动作数（例如 15、25 等），继续进行对比实验。



# 策略梯度算法

Q-learning、DQN 及 DQN 改进算法都是**基于价值**（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是**基于策略**（policy-based）的方法。对比两者，基于价值函数的方法主要是学习价值函数，然后根据价值函数导出一个策略，学习过程中并不存在一个显式的策略；**而基于策略的方法则是直接显式地学习一个目标策略**。策略梯度是基于策略的方法的基础。

## 策略梯度

基于策略的方法首先需要将策略参数化。假设目标策略 $\pi_{\theta}$ 是一个随机性策略，并且处处可微，其中 $\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要**寻找一个最优策略并最大化这个策略在环境中的期望回报**。我们将策略学习的目标函数定义为
$$
J(\theta)=E_{s_0}[V^{\pi_{\theta}}(s_0)]
$$
其中，$s_0$ 表示初始状态。现在有了目标函数，我们将目标函数对策略 $\theta$ 求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。

$v^{\pi}$ 表示策略 $\pi$ 下的状态访问分布。然后我们对目标函数求梯度，可以得到如下式子，
$$
\begin{align}
\nabla_{\theta} J(\theta) 
& \propto \sum_{s\in S}v^{\pi_{\theta}}(s)\sum_{a\in A}Q^{\pi_{\theta}}(s,a)\nabla_{\theta}\pi_{\theta}(a|s) \\
& = \sum_{s\in S}v^{\pi_{\theta}}(s)\sum_{a\in A}\pi_{\theta}(a|s)Q^{\pi_{\theta}}(s,a)\frac{\nabla_{\theta}\pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \\
& = E_{\pi_{\theta}}[Q^{\pi_{\theta}}(s,a)\nabla_{\theta}log\;\pi_{\theta}(a|s)]
\end{align}
$$
这个梯度可以用来更新策略。需要注意的是，因为上式中期望 $E$ 的下标是 $\pi_{\theta}$ ，所以策略梯度算法为**在线策略（on-policy）算法，即必须使用当前策略 $\pi_{\theta}$ 采样得到的数据来计算梯度**。直观理解一下策略梯度这个公式，可以发现在每一个状态下，**梯度的修改是让策略更多地去采样到带来较高 $Q$ 值的动作，更少地去采样到带来较低值的动作。**

在计算策略梯度的公式中，我们需要用到 $Q^{\pi_{\theta}}(s,a)$ ，可以用多种方式对它进行估计。

REINFORCE 算法便是采用了蒙特卡洛方法来估计 $Q^{\pi_{\theta}}(s,a)$ ，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为：
$$
\nabla_{\theta} J(\theta) = 
E_{\pi_{\theta}}
[\sum_{t=0}^T(\sum_{t'=t}^T\gamma^{t'-t}r_{t'})
\nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)
]
$$
其中，$T$ 是和环境交互的最大步数。

## REINFORCE

REINFORCE 算法的具体算法流程如下：

- 初始化策略参数 $\theta$
- for 序列 $e=1 \rightarrow E$ do :
  -  用当前策略 $\pi_{\theta}$ 采样轨迹 $\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_T,a_T,r_T\}$
  -  计算当前轨迹每个时刻 $t$ 往后的回报 $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$，记为 $\psi_{t}$
  -  对 $\theta$ 进行更新，$\theta=\theta+\alpha\sum_{t=0}^T\psi_{t}
    \nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)$
- end for

**Code is available at [REINFORCE](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/REINFORCE.ipynb)**

实验证明，随着收集到的轨迹越来越多，REINFORCE 算法有效地学习到了最优策略。不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。



# Actor-Critic 算法

基于值函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。**演员-评论员算法(Actor-Critic)既学习价值函数，又学习策略函数 。**Actor-Critic 是囊括一系列算法的整体架构，目前很多高效的前沿算法都属于 Actor-Critic 算法。需要明确的是，**Actor-Critic 算法本质上是基于策略的算法**，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。

在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE 算法用蒙特卡洛方法来估计 $Q(s,a)$ ，**Actor-Critic 算法所做的就是拟合一个值函数来指导策略进行学习**

在策略梯度中，可以把梯度写成下面这个更加一般的形式：$g=E[\sum_{t=0}^T\psi_{t}
\nabla_{\theta}log\;\pi_{\theta}(a_t|s_t)]$

其中，$\psi_{t}$ 可以有很多种形式：

1. $\sum_{t'=0}^T\gamma^{t'}r_{t'}$：轨迹的总回报
2. $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$：动作 $a_t$ 之后的回报
3. $\sum_{t'=t}^T\gamma^{t'-t}r_{t'}-b(s_t)$：基准线版本的改进
4. $Q^{\pi_\theta}(s_t,a_t)$：动作价值函数
5. $A^{\pi_\theta}(s_t,a_t)$：优势函数
6. $r_t+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t})$：时序差分残差

REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用形式(3)引入**基线函数**（baseline function）$b(s_t)$ 来减小方差。

此外，我们也可以采用 Actor-Critic 算法估计一个动作价值函数 $Q$，代替蒙特卡洛采样得到的回报，这便是形式(4)

这个时候，我们可以把状态价值函数 $V$ 作为基线，从 $Q$ 函数减去这个 $V$ 函数则得到了 $A$ 函数，我们称之为**优势函数**（advantage function），这便是形式(5)

更进一步，我们可以利用 $Q=r+\gamma V$ 等式得到形式(6)

接下来以形式(6)为例进行分析，即通过时序差分残差 $\psi_{t}=r_t+\gamma V^{\pi}(s_{t+1})-V^{\pi}(s_{t})$ 来指导策略梯度进行学习。**事实上，用 $Q$ 值或者 $V$ 值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性**。除此之外，REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 **Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制**。

- **我们将 Actor-Critic 分为两个部分：Actor（策略网络）和 Critic（价值网络）** 
  - **Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略**
  - **Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新**

<img src="https://hrl.boyuai.com/static/640.5dee5ab0.jpg" style="zoom:50%;" />

**价值模块 Critic 在策略模块 Actor 采样的数据中学习分辨什么是好的动作，什么不是好的动作，进而指导 Actor 进行策略更新。随着 Actor 的训练的进行，其与环境交互所产生的数据分布也发生改变，这需要 Critic 尽快适应新的数据分布并给出好的判别。**

Actor 的更新采用策略梯度的原则，Critic 的更新方式：将 Critic 价值网络表示为 $V_w$，参数为 $w$ 。于是，我们可以采取时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数：$L(w)=\frac{1}{2}(r+\gamma V_w(s_{t+1})-V_w(s_t))^2$

与 DQN 中一样，我们采取类似于目标网络的方法，将上式中 $r+\gamma V_w(s_{t+1})$ 作为时序差分目标，不会产生梯度来更新价值函数。由于目标网络的参数是固定的，不会随着每次更新而改变，因此时序差分不会产生梯度。

因此，价值函数的梯度为：
$$
\nabla_wL(w)=-[r+\gamma V_w(s_{t+1})-V_w(s_{t})]\nabla_wV_w(s_t)
$$
然后使用梯度下降方法来更新 Critic 价值网络参数即可。

Actor-Critic 算法的具体流程如下：

- 初始化策略网络参数，价值网络参数
- for 序列 $e=1\rightarrow E$ do :
  -  用当前策略 $\pi_\theta$ 采样轨迹 $\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots\}$
  -  为每一步数据计算: $\delta_t=r_t+\gamma V_w(s_{t+1})-V_w(s_{t})$
  -  更新价值参数 $w=w+\alpha_w\sum_t\delta_t\nabla_wV_w(s_t)$
  -  更新策略参数 $\theta=\theta+\alpha_\theta\sum_t\delta_t\nabla_\theta log\;\pi_\theta(a_t|s_t)$
- end for

**Code is available at [Actor-Critic](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Actor-Critic.ipynb)**

实验证明，Actor-Critic 算法很快便能收敛到最优策略，并且训练过程非常稳定，抖动情况相比 REINFORCE 算法有了明显的改进，这说明价值函数的引入减小了方差。



# TRPO 算法

策略梯度算法和 Actor-Critic 算法这两种基于策略的方法虽然简单、直观，但在实际应用过程中会遇到训练不稳定的情况。这样的算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。

针对以上问题，我们考虑在更新时找到一块**信任区域**（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是**信任区域策略优化**（trust region policy optimization，TRPO）算法的主要思想。

## 策略目标

假设当前策略为 $\pi_\theta$ ，参数为 $\theta$ 。我们考虑如何借助当前的 $\theta$ 找到一个更优的参数 $\theta'$，使得 $J(\theta')\geq J(\theta)$。具体来说，由于初始状态 $s_0$ 的分布和策略无关，因此上述策 $\pi_\theta$ 下的优化目标 $J(\theta)$ 可以写成在新策略 $\pi_{\theta'}$ 的期望形式：
$$
\begin{align}
J(\theta) 
& = E_{s_0}[V^{\pi_\theta}(s_0)] \\
& = E_{\pi_{\theta'}} [\sum_{t=0}^{\infty}\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^{\infty}\gamma^tV^{\pi_\theta}(s_t)] \\
& = -E_{\pi_{\theta'}} [\sum_{t=0}^{\infty}\gamma^t(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_t))]
\end{align}
$$
（......）

TRPO 算法属于在线策略学习方法，每次策略训练仅使用上一轮策略采样的数据，是基于策略的深度强化学习算法中十分有代表性的工作之一。直觉性地理解，TRPO 给出的观点是：由于策略的改变导致数据分布的改变，这大大影响深度模型实现的策略网络的学习效果，所以通过划定一个可信任的策略学习区域，保证策略学习的稳定性和有效性。



# PPO 算法

TRPO 算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是，TRPO 算法的改进版——**近端策略优化（proximal policy optimization，PPO）**算法被提出，PPO 基于 TRPO 的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法。**如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。**

TRPO 的优化目标：
$$
\underset{\theta}{max}\;E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)] \\
s.t. \quad E_{s\sim v^{\pi_{\theta_k}}}[D_{KL}(\pi_{\theta_k}(\cdot|s),\pi_{\theta}(\cdot|s))] \leq \delta
$$
TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断。

PPO 是 TRPO 的一种改进算法，它在实现上简化了 TRPO 中的复杂计算，并且它在实验中的性能大多数情况下会比 TRPO 更好，因此目前常被用作一种常用的基准算法。**需要注意的是，TRPO 和 PPO 都属于在线策略学习算法，即使优化目标中包含重要性采样的过程，但其只是用到了上一轮策略的数据，而不是过去所有策略的数据。**

## PPO-惩罚

PPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数。即：
$$
\underset{\theta}{arg\;max\;}E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}
[\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)
-\beta D_{KL}[(\pi_{\theta_k}(\cdot|s),\pi_{\theta}(\cdot|s))]]
$$
令 $d_k=D^{v^{\pi_{\theta_k}}}_{KL}(\pi_{\theta_k},\pi_{\theta})$， $\beta$ 的更新规则如下：

- 如果 $d_k < \frac{\delta}{1.5}$，那么 $\beta_{k+1}=\frac{\beta_k}{2}$
- 如果 $d_k > \frac{\delta}{1.5}$，那么 $\beta_{k+1}=\beta_k \times 2$
- 否则 $\beta_{k+1}=\beta_k$

其中，$\delta$ 是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距

## PPO-截断

PPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即：
$$
\underset{\theta}{arg\;max\;}E_{s\sim v^{\pi_{\theta_k}}} E_{a\sim \pi_{\theta_k}(\cdot|s)}
[min(
\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)
,clip(\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)},1-\epsilon,1+\epsilon
)A^{\pi_{\theta_k}}(s,a)
)]
$$
其中 $clip(x,l,r):=max(min(x,r),l)$，即把 $x$ 限制在 $[l,r]$内。上式中 $\epsilon$ 是一个超参数，表示进行截断（clip）的范围

如果 $A^{\pi_{\theta_k}}(s,a) > 0$，说明这个动作的价值高于平均，最大化这个式子会增大 $\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}$，但不会让其超过 $1+\epsilon$。反之，如果 $A^{\pi_{\theta_k}}(s,a) < 0$ ，最大化这个式子会减小 $\frac{\pi_\theta(a|s)}{\pi_{\theta_k}(a|s)}$，但不会让其超过 $1-\epsilon$。如下所示。

<img src="https://hrl.boyuai.com/static/640.1ddeb598.png" style="zoom:67%;" />

**Code is available at [PPO](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/PPO.ipynb)**

大量实验表明，PPO-截断总是比 PPO-惩罚表现得更好。



# DDPG 算法

基于策略梯度的算法 REINFORCE、Actor-Critic 以及两个改进算法——TRPO 和 PPO，它们都是在线策略算法，这意味着它们的**样本效率**（sample efficiency）比较低。

DQN 算法直接估计最优函数 Q，可以做到离线策略学习，但是它只能处理动作空间有限的环境，这是因为它需要从所有动作中挑选一个 $Q$ 值最大的动作。如果动作个数是无限的，虽然我们可以将动作空间离散化，但这比较粗糙，无法精细控制。

**深度确定性策略梯度**（deep deterministic policy gradient，DDPG）算法就是处理动作空间无限的环境并且使用的是离线策略算法，它构造一个确定性策略，用梯度上升的方法来最大化 $Q$ 值。

DDPG 也属于一种 Actor-Critic 算法。REINFORCE、TRPO 和 PPO 学习随机性策略，而 DDPG 则学习一个确定性策略。

深度确定性策略梯度算法（DDPG），它是面向连续动作空间的深度确定性策略训练的典型算法。相比于它的先期工作，即确定性梯度算法（DPG），DDPG 加入了目标网络和软更新的方法，这对深度模型构建的价值网络和策略网络的稳定学习起到了关键的作用，DDPG 算法也被引入了多智能体强化学习领域，催生了 MADDPG 算法。

随机性策略可以表示为 $a\sim \pi_\theta(\cdot|s)$；而如果策略是确定性的，则可以记为 $a=\mu_\theta(s)$​。与策略梯度定理类似，可以推导出**确定性策略梯度定理**（deterministic policy gradient theorem）：
$$
\nabla_\theta J(\pi_\theta)=E_{s\sim v^{\pi_\beta}}[\nabla_\theta \mu_\theta(s)\nabla_a Q_w^\mu(s,a)|_{a=\mu_{\theta}(s)}]
$$
其中，$\pi_\beta$ 是用来收集数据的行为策略。我们可以这样理解这个定理：假设现在已经有函数 $Q$ ，给定一个状态 $s$，但由于现在动作空间是无限的，无法通过遍历所有动作来得到 $Q$ 值最大的动作，因此我们想用策略 $\mu$ 找到使 $Q(s,a)$ 值最大的动作 $a$，即 $\mu(s)=\underset{a}{arg\;max\;}Q(s,a)$。此时，$Q$ 就是 Critic，$\mu$ 就是 Actor，这是一个 Actor-Critic 的框架

![](https://hrl.boyuai.com/static/640.a3d586c4.png)

要想得到 $\mu$，首先用 $Q$ 对 $\mu_\theta$ 求导 $\nabla_\theta Q(s,\mu_\theta(s))$，其中会用到梯度的链式法则，先对 $a$ 求导，再对 $\theta$ 求导。然后通过梯度上升的方法来最大化函数 $Q$，得到 $Q$ 值最大的动作

DDPG 要用到4个神经网络，其中 Actor 和 Critic 各用一个网络，此外它们都各自有一个目标网络。DDPG 中 Actor 也需要目标网络因为目标网络也会被用来计算目标 $Q$  值。DDPG 中目标网络的更新与 DQN 中略有不同：在 DQN 中，每隔一段时间将 $Q$  网络直接复制给目标 $Q$  网络；而在 DDPG 中，目标 $Q$  网络的更新采取的是一种软更新的方式，即让目标 $Q$  网络缓慢更新，逐渐接近 $Q$  网络，其公式为：$w^- \leftarrow\tau w+(1-\tau)w^-$。通常 $\tau$ 是一个比较小的数，当 $\tau = 1$ 时，就和 DQN 的更新方式一致了。而目标 $\mu$ 网络也使用这种软更新的方式。

另外，由于函数 $Q$ 存在 $Q$ 值过高估计的问题，DDPG 采用了 Double DQN 中的技术来更新 $Q$ 网络。但是，由于 DDPG 采用的是确定性策略，它本身的探索仍然十分有限。 DQN 算法的探索主要由 $\epsilon$-贪婪策略的行为策略产生。同样作为一种离线策略的算法，DDPG 在行为策略上引入一个随机噪声 $N$ 来进行探索。

DDPG 的算法流程如下:

- 随机噪声可以用 $N$ 来表示，用随机的网络参数 $w$ 和 $\theta$ 分别初始化 Critic 网络 $Q_w(s,a)$ 和 Actor 网络 $\mu_\theta(s)$
- 复制相同的参数 $w^- \leftarrow w$ 和 $\theta^- \leftarrow \theta$，分别初始化目标网络 $Q_{w^-}$ 和 $\mu_{\theta^-}$
- 初始化经验回放池 $R$
- for 序列 $e=1 \to E$ do :
  - 初始化随机过程 $N$ 用于动作探索
  - 获取环境初始状态 $s_1$ 
  - for 时间步 $t=1\to T$ do :
    - 根据当前策略和噪声选择动作 $a_t=\mu_\theta(s_t)+N$
    - 执行动作 $a_t$，获得奖励 $r_t$，环境状态变为 $s_{t+1}$
    - 将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池 $R$
    - 从 $R$ 中采样 $N$ 个元组 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
    - 对每个元组，用目标网络计算 $y_i=r_i+\gamma Q_{w^-}(s_{i+1},\mu_{\theta^-}(s_{i+1}))$
    - 最小化目标损失 $L=\frac{1}{N}\sum_{i=1}^N(y_i-Q_w(s_i,a_i))^2$，以此更新当前 Critic 网络
    - 计算采样的策略梯度，以此更新当前 Actor 网络：$\nabla_\theta J=\frac{1}{N}\sum_{i=1}^N[\nabla_\theta \mu_\theta(s_i)\nabla_a Q_w(s_i,a)|_{a=\mu_{\theta}(s_i)}]$
    - 更新目标网络：$w^- \leftarrow \tau w+(1-\tau)w^- \leftarrow \tau \theta +(1-\tau)\theta^-$
  - end for
- end for

在 DDPG 的原始论文中，添加的噪声符合奥恩斯坦-乌伦贝克（Ornstein-Uhlenbeck，OU）随机过程：
$$
\Delta x_t=\theta(\mu-x_{t-1})+\sigma W
$$
其中，$\mu$ 是均值，$W$ 是符合布朗运动的随机噪声，$\theta$ 和 $\sigma$ 是比例参数。可以看出，当 $x(t-1)$ 偏离均值时，$x_t$ 的值会向均值靠拢。OU 随机过程的特点是在均值附近做出线性负反馈，并有额外的干扰项。OU 随机过程是与时间相关的，适用于有惯性的系统。在 DDPG 的实践中，不少地方仅使用正态分布的噪声。

**Code is available at [DDPG](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/DDPG.ipynb)**

# SAC 算法

在线策略算法的采样效率比较低，我们通常更倾向于使用离线策略算法。然而，虽然 DDPG 是离线策略算法，但是它的训练非常不稳定，收敛性较差，对超参数比较敏感，也难以适应不同的复杂环境。2018 年，一个更加稳定的离线策略算法 **Soft Actor-Critic（SAC）**被提出。SAC 的前身是 Soft Q-learning，它们都属于**最大熵强化学习**的范畴。Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数 $Q$ 的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。

## 最大熵强化学习

**熵**（entropy）表示对一个随机变量的随机程度的度量。具体而言，如果 $X$ 是一个随机变量，且它的概率密度函数为 $p$，那么它的熵 $H$ 就被定义为：$H(X)=E_{x\sim p}[-log\; p(x)]$

在强化学习中，我们可以使用 $H(\pi(\cdot|s))$ 来表示策略 $\pi$ 在状态 $s$ 下的随机程度

**大熵强化学习**（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为
$$
\pi^*=\underset{\pi}{arg\;max\;}E_{\pi}[\sum_tr(s_t,a_t)+\alpha H(\pi(\cdot|s_t))]
$$
其中，$\alpha$ 是一个正则化的系数，用来控制熵的重要程度。熵正则化增加了强化学习算法的探索程度，$\alpha$ 越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。

## Soft 策略迭代

在最大熵强化学习框架中，由于目标函数发生了变化，其他的一些定义也有相应的变化。首先，我们看一下 Soft 贝尔曼方程：
$$
Q(s_t,a_t)=r(s_t,a_t)+\gamma E_{s_{t+1}}[V(s_{t+1})]
$$
其中，状态价值函数被写为
$$
V(s_t)=E_{a_t\sim \pi}[Q(s_t,a_t)-\alpha log \pi (a_t|s_t)]=E_{a_t\sim \pi}[Q(s_t,a_t)]+H(\pi(\cdot|s_t))
$$
于是，根据该 Soft 贝尔曼方程，在有限的状态和动作空间情况下，Soft 策略评估可以收敛到策略 $\pi$​ 的 Soft Q 函数。然后，根据如下 Soft 策略提升公式可以改进策略：
$$
\pi_{new}=\underset{\pi'}{arg\;min;}D_{KL}(\pi'(\cdot|s),\frac{exp(\frac{1}{\alpha}Q^{\pi_{old}}(s,\cdot))}
{Z^{\pi_{old}}(s,\cdot)})
$$
重复交替使用 Soft 策略评估和 Soft 策略提升，最终策略可以收敛到最大熵强化学习目标中的最优策略。但该 Soft 策略迭代方法只适用于**表格型**（tabular）设置的情况，即状态空间和动作空间是有限的情况。在连续空间下，我们需要通过参数化函数 $Q$ 和策略 $\pi$ 来近似这样的迭代。

## SAC

在 SAC 算法中，我们为两个动作价值函数 $Q$（参数分别为 $w_1$ 和 $w_2$）和一个策略函数 $\pi$（参数为 $\theta$）建模。基于 Double DQN 的思想，SAC 使用两个 $Q$ 网络，但每次用 $Q$ 网络时会挑选一个 $Q$ 值小的网络，从而缓解 $Q$ 值过高估计的问题。任意一个函数 $Q$ 的损失函数为：
$$
\begin{align}
L_Q(w) 
& = E_{(s_t,a_t,r_t,s_{t+1})\sim R} [\frac{1}{2}(Q_w(s_t,a_t)-(r_t+\gamma V_{w^-}(s_{t+1})))^2] \\
& = E_{(s_t,a_t,r_t,s_{t+1})\sim R,\;a_{t+1}\sim \pi_\theta(\cdot|s_{t+1})}
[\frac{1}{2}(Q_w(s_t,a_t)-(r_t+\gamma (\underset{j=1,2}{min\;}Q_{w_j^-}(s_{t+1},a_{t+1}) - \alpha log \pi(a_{t+1}|s_{t+1})))
)^2]
\end{align}
$$
其中，$R$ 是策略过去收集的数据，因为 SAC 是一种离线策略算法。为了让训练更加稳定，这里使用了目标 $Q$ 网络 $Q_{w^-}$，同样是两个目标 $Q$ 网络，与两个 $Q$ 网络一一对应。SAC 中目标 $Q$ 网络的更新方式与 DDPG 中的更新方式一样。

策略 $\pi$ 的损失函数由 KL 散度得到，化简后为：
$$
L_{\pi}(\theta)=E_{s_t\sim R,\;a_t\sim \pi_\theta}[\alpha\;log(\pi_\theta(a_t|s_t))-Q_w(s_t,a_t)]
$$
可以理解为最大化函数 $V$，因为有 $V(s_t)=E_{a_t\sim\pi}[Q(s_t,a_t)-\alpha \; log\; \pi(a_t|s_t)]$

对连续动作空间的环境，SAC 算法的策略输出高斯分布的均值和标准差，但是根据高斯分布来采样动作的过程是不可导的。因此，我们需要用到**重参数化技巧**（reparameterization trick）。重参数化的做法是先从一个单位高斯分布 $N$ 采样，再把采样值乘以标准差后加上均值。这样就可以认为是从策略高斯分布采样，并且这样对于策略函数是可导的。我们将其表示为 $a_t=f_\theta(\epsilon_t;s_t)$，其中 $\epsilon_t$ 是一个噪声随机变量。同时考虑到两个函数 $Q$​，重写策略的损失函数：
$$
L_\pi(\theta)=E_{s_t\sim R,\epsilon_t\sim N}[\alpha\;log(\pi_\theta(f_\theta(\epsilon_t;s_t)|s_t))-\underset{j=1,2}{min\;}Q_{w_j}(s_t,f_\theta(\epsilon_t;s_t))]
$$
在 SAC 算法中，如何选择熵正则项的系数非常重要。在不同的状态下需要不同大小的熵：在最优动作不确定的某个状态下，熵的取值应该大一点；而在某个最优动作比较确定的状态下，熵的取值可以小一点。为了自动调整熵正则项，SAC 将强化学习的目标改写为一个带约束的优化问题：
$$
\underset{\pi}{max\;}E_\pi[\sum_tr(s_t,a_t)] \quad s.t. \; E_{(s_t,a_t)\sim \rho_\pi}[-log(\pi_t(a_t|s_t))] \geq \Eta_0
$$
也就是最大化期望回报，同时约束熵的均值大于 $\Eta_0$。通过一些数学技巧化简后，得到 $\alpha$​ 的损失函数：
$$
L(\alpha)=E_{s_t\sim R,a_t\sim \pi(\cdot|s_t)}[-\alpha\; log\; \pi(a_t|s_t)-\alpha \Eta_0]
$$
即当策略的熵低于目标值 $\Eta_0$时，训练目标 $L(\alpha)$ 会使 $\alpha$ 的值增大，进而在上述最小化损失函数 $L_\pi(\theta)$ 的过程中增加了策略熵对应项的重要性；而当策略的熵高于目标值 $\Eta_0$时，训练目标 $L(\alpha)$ 会使 $\alpha$ 的值减小，进而使得策略训练时更专注于价值提升。

 SAC的具体算法流程如下：

- 用随机的网络参数 $w_1$, 和 $w_2$ 分别初始化 Critic 网络 $Q_{w_1}(s,a)$, 和 Actor 网络 $\pi_\theta(s)$
- 复制相同的参数 $w^-_1 \leftarrow w_1$ 和 $w^-_2 \leftarrow w_2$，分别初始化目标网络 $Q_{w^-_1}$ 和 $Q_{w^-_2}$
- 初始化经验回放池 $R$
- for 序列 $e=1 \to E$ do :
  - 初始化环境初始状态 $s_1$ 
  - for 时间步 $t=1\to T$ do :
    - 根据当前策略选择动作 $a_t=\mu_\theta(s_t)$
    - 执行动作 $a_t$，获得奖励 $r_t$，环境状态变为 $s_{t+1}$
    - 将 $(s_t,a_t,r_t,s_{t+1})$ 存储进回放池 $R$
    - for 训练轮数 $k=1\to K$ do:
      - 从 $R$ 中采样 $N$ 个元组 $\{(s_i,a_i,r_i,s_{i+1})\}_{i=1,\cdots,N}$
      - 对每个元组，用目标网络计算 $y_i=r_i+\gamma\;\underset{j=1,2}{min}\; Q_{w_j^-}(s_{i+1},a_{i+1})-\alpha\;log\;\pi_\theta(a_{i+1}|s_{i+1})$，其中 $a_{i+1}\sim \pi_\theta(\cdot|s_{i+1})$
      - 对两个 Critic 网络都进行如下更新：对 $j=1,2$，最小化损失函数 $L=\frac{1}{N}\sum_{i=1}^N(y_i-Q_{w_j}(s_i,a_i))^2$
      - 用重参数化技巧采样动作 $\widetilde{a}_i$，然后用以下损失函数更新当前 Actor 网络：$L_\pi(\theta)=\frac{1}{N}\sum_{i=1}^N[\alpha\;log\pi_\theta(\widetilde{a}_i|s_i)-\underset{j=1,2}{min\;}Q_{w_j}(s_i,\widetilde{a}_i)]$
      - 更新熵正则项的系数 $\alpha$
      - 更新目标网络：$w^-_1 \leftarrow \tau w_1+(1-\tau)w^-_1 \leftarrow \tau w_2 +(1-\tau)w^-_2$
    - end for
  - end for
- end for

**Code is available at [SAC](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/SAC.ipynb)**



# 模仿学习

虽然强化学习不需要有监督学习中的标签数据，但它十分依赖奖励函数的设置。有时在奖励函数上做一些微小的改动，训练出来的策略就会有天差地别。在很多现实场景中，奖励函数并未给定，或者奖励信号极其稀疏，此时随机设计奖励函数将无法保证强化学习训练出来的策略满足实际需要。

假设存在一个专家智能体，其策略可以看成最优策略，我们就可以直接模仿这个专家在环境中交互的状态动作数据来训练一个策略，并且不需要用到环境提供的奖励信号。**模仿学习**（imitation learning）研究的便是这一类问题，在模仿学习的框架下，专家能够提供一系列状态动作对 $\{(s_t,a_t)\}$，表示专家在环境 $s_t$ 下做出了的动作 $a_t$，而模仿者的任务则是利用这些专家数据进行训练，无须奖励信号就可以达到一个接近专家的策略。目前学术界模仿学习的方法基本上可以分为 3 类：

- 行为克隆（behavior cloning，BC）
- 逆强化学习（inverse RL）
- 生成式对抗模仿学习（generative adversarial imitation learning，GAIL）

逆向强化学习（IRL）假设环境的奖励函数应该使得专家轨迹获得最高的奖励值，进而学习背后的奖励函数，最后基于该奖励函数做正向强化学习，从而得到模仿策略。尽管逆强化学习有良好的学术贡献，但由于其计算复杂度较高，实际应用的价值较小。

## 行为克隆

**行为克隆**（BC）就是直接使用监督学习方法，将专家数据中 $(s_t,a_t)$ 的 $s_t$ 看作样本输入，$a_t$ 视为标签，学习的目标为
$$
\theta^*=\underset{\theta}{arg\;min\;}E_{(s,a)\sim B}[L(\pi_\theta(s),a)]
$$
其中，$B$ 是专家的数据集，$L$ 是对应监督学习框架下的损失函数。若动作是离散的，该损失函数可以是最大似然估计得到的。若动作是连续的，该损失函数可以是均方误差函数。

在训练数据量比较大的时候，BC 能够很快地学习到一个不错的策略。例如，围棋人工智能 AlphaGo 就是首先在 16 万盘棋局的 3000 万次落子数据中学习人类选手是如何下棋的，仅仅凭这个行为克隆方法，AlphaGo 的棋力就已经超过了很多业余围棋爱好者。由于 BC 的实现十分简单，因此在很多实际场景下它都可以作为策略预训练的方法。BC 能使得策略无须在较差时仍然低效地通过和环境交互来探索较好的动作，而是通过模仿专家智能体的行为数据来快速达到较高水平，为接下来的强化学习创造一个高起点。

BC 也存在很大的局限性，该局限在数据量比较小的时候犹为明显。具体来说，由于通过 BC 学习得到的策略只是拿小部分专家数据进行训练，因此 BC 只能在专家数据的状态分布下预测得比较准。然而，强化学习面对的是一个序贯决策问题，通过 BC 学习得到的策略在和环境交互过程中不可能完全学成最优，只要存在一点偏差，就有可能导致下一个遇到的状态是在专家数据中没有见过的。此时，由于没有在此状态（或者比较相近的状态）下训练过，策略可能就会随机选择一个动作，这会导致下一个状态进一步偏离专家策略遇到的的数据分布。最终，该策略在真实环境下不能得到比较好的效果，这被称为行为克隆的**复合误差**（compounding error）问题。

## 生成式对抗模仿学习

**生成式对抗模仿学习**（generative adversarial imitation learning，GAIL）是 2016 年由斯坦福大学研究团队提出的基于生成式对抗网络的模仿学习，它诠释了生成式对抗网络的本质其实就是模仿学习。GAIL 实质上是模仿了专家策略的占用度量 $\rho_E(s,a)$ ，即尽量使得策略在环境中的所有状态动作对 $(s,a)$ 的占用度量 $\rho_\pi(s,a)$ 和专家策略的占用度量一致。 

为了达成这个目标，策略需要和环境进行交互，收集下一个状态的信息并进一步做出动作。这一点和 BC 不同，BC 完全不需要和环境交互。GAIL 算法中有一个判别器和一个策略，策略 $\pi$ 就相当于是生成式对抗网络中的**生成器**（generator），给定一个状态，策略会输出这个状态下应该采取的动作，而**判别器**（discriminator）$D$ 将状态动作对 $(s,a)$ 作为输入，输出一个 0 到 1 之间的实数，表示判别器认为该状态动作对 $(s,a)$ 是来自智能体策略而非专家的概率。判别器 $D$ 的目标是尽量将专家数据的输出靠近 0，将模仿者策略的输出靠近 1，这样就可以将两组数据分辨开来。于是，判别器 $D$ 的损失函数为
$$
L(\phi)=-E_{\rho_\pi}[log\;D_\phi(s,a)]-E_{\rho E}[log(1-D_\phi(s,a))]
$$
其中 $\phi$ 是判别器 $D$ 的参数。有了判别器 $D$ 之后，模仿者策略的目标就是其交互产生的轨迹能被判别器误认为专家轨迹。于是，我们可以用判别器 $D$ 的输出来作为奖励函数来训练模仿者策略。具体来说，若模仿者策略在环境中采样到状态 $s$，并且采取动作 $a$，此时该状态动作对 $(s,a)$ 会输入到判别器 $D$ 中，输出的 $D(s,a)$ 值，然后将奖励设置为 $r(s,a)=-log\;D(s,a)$。于是，我们可以用任意强化学习算法，使用这些数据继续训练模仿者策略。最后，在对抗过程不断进行后，模仿者策略生成的数据分布将接近真实的专家数据分布，达到模仿学习的目标。GAIL 的优化目标如下图所示：

![](data:image/png;base64,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)

模仿学习的本质就是通过更新策略使其占用度量尽量靠近专家的占用度量，而这正是 GAIL 的训练目标。由于一旦策略改变，其占用度量就会改变，因此为了训练好最新的判别器，策略需要不断和环境做交互，采样出最新的状态动作对样本。

**Code is available at [Imitation learning](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Imitation%20learning.ipynb)**

实验证明在数据样本有限的情况下，BC 不能学习到最优策略，但是 GAIL 在相同的专家数据下可以取得非常好的结果。这一方面归因于 GAIL 的训练目标（拉近策略和专家的占用度量）十分贴合模仿学习任务的目标，避免了 BC 中的复合误差问题；另一方面得益于 GAIL 训练中，策略可以和环境交互出更多的数据，以此训练判别器，进而生成对基于策略“量身定做”的指导奖励信号。



# 模型预测控制

基于值函数的方法 DQN、基于策略的方法 REINFORCE 以及两者结合的方法 Actor-Critic都是**无模型**（model-free）的方法，即没有建立一个环境模型来帮助智能体决策。而在深度强化学习领域下，**基于模型**（model-based）的方法通常用神经网络学习一个环境模型，然后利用该环境模型来帮助智能体训练和决策。利用环境模型帮助智能体训练和决策的方法有很多种，例如可以用与 Dyna 类似的思想生成一些数据来加入策略训练中。而**模型预测控制**（model predictive control，MPC）算法并不构建一个显式的策略，只根据环境模型来选择当前步要采取的动作。

**Code is available at [MPC](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Model%20predictive%20control.ipynb)**

模型预测控制（MPC）方法有着其独特的优势，例如它不用构建和训练策略，可以更好地利用环境，可以进行更长步数的规划。但是 MPC 也有其局限性，例如模型在多步推演之后的准确性会大大降低，简单的控制策略对于复杂系统可能不够。MPC 还有一个更为严重的问题，即每次计算动作的复杂度太大，这使其在一些策略及时性要求较高的系统中应用就变得不太现实。

##  打靶法

假设我们在下围棋，现在根据棋盘的布局，我们要选择现在落子的位置。一个优秀的棋手会根据目前局势来推演落子几步可能发生的局势，然后选择局势最好的一种情况来决定当前落子位置。

**模型预测控制方法就是这样一种迭代的、基于模型的控制方法。**值得注意的是，MPC 方法中不存在一个显式的策略。具体而言，MPC 方法在每次采取动作时，首先会生成一些候选动作序列，然后根据当前状态来确定每一条候选序列能得到多好的结果，最终选择结果最好的那条动作序列的第一个动作来执行。因此，在使用 MPC 方法时，主要在两个过程中迭代，一是根据历史数据学习环境模型 $\widehat{P}(s,a)$，二是在和真实环境交互过程中用环境模型来选择动作。

定义模型预测方法的目标，在第 $k$ 步时，我们要想做的就是最大化智能体的累积奖励，具体来说就是：
$$
\underset{a_{k:k+H}}{arg\;max\;}\sum_{t=k}^{k+H}r(s_t,a_t) \quad s.t.\; s_{t+1}=\widehat{P}(s_t,a_t)
$$
其中 $H$ 为推演的长度，$\underset{a_{k:k+H}}{arg\;max\;}$ 表示从所有动作序列中选取累积奖励最大的序列。我们每次取最优序列中的第一个动作 $a_k$ 来与环境交互。MPC 方法中的一个关键是如何生成一些候选动作序列，候选动作生成的好坏将直接影响到 MPC 方法得到的动作。生成候选动作序列的过程我们称为**打靶**（shooting）。

**随机打靶法**（random shooting method）的做法便是随机生成 $N$ 条动作序列，即在生成每条动作序列的每一个动作时，都是从动作空间中随机采样一个动作，最终组合成 $N$ 条长度为 $H$ 的动作序列。

**交叉熵方法**（cross entropy method，CEM）是一种进化策略方法，它的核心思想是维护一个带参数的分布，根据每次采样的结果来更新分布中的参数，使得分布中能获得较高累积奖励的动作序列的概率比较高。相比于随机打靶法，交叉熵方法能够利用之前采样到的比较好的结果，在一定程度上减少采样到一些较差动作的概率，从而使得算法更加高效。对于一个与连续动作交互的环境来说，每次交互时交叉熵方法的做法如下：

- for 次数 $e=1\to E$ do:
  -  从分布 $P(A)$ 中选取 $N$ 条动作序列 $A_1,\cdots,A_N$
  -  对于每条动作序列 $A_1,\cdots,A_N$，用环境模型评估累积奖励
  -  根据评估结果保留 $M$ 条最优的动作序列 $A_{i_1},\cdots,A_{i_M}$
  -  用这些动作序列 $A_{i_1},\cdots,A_{i_M}$ 去更新分布 $P(A)$
- end for
- 计算所有最优动作序列的第一个动作的均值，作为当前时刻采取的动作

## PETS算法

**带有轨迹采样的概率集成**（probabilistic ensembles with trajectory sampling，PETS）是一种使用 MPC 的基于模型的强化学习算法。在 PETS 中，环境模型采用了集成学习的方法，即会构建多个环境模型，然后用这多个环境模型来进行预测，最后使用 CEM 进行模型预测控制。接下来，我们来详细介绍模型构建与模型预测的方法。

在强化学习中，与智能体交互的环境是一个动态系统，所以拟合它的环境模型也通常是一个动态模型。我们通常认为一个系统中有两种不确定性，分别是**偶然不确定性**（aleatoric uncertainty）和**认知不确定性**（epistemic uncertainty）。偶然不确定性是由于系统中本身存在的随机性引起的，而认知不确定性是由“见”过的数据较少导致的自身认知的不足而引起的，如下图所示。

<img src="https://hrl.boyuai.com/static/320.48871dfb.png"  />

在 PET 算法中，环境模型的构建会同时考虑到这两种不确定性。首先，我们定义环境模型的输出为一个高斯分布，用来捕捉偶然不确定性。令环境模型为 $\widehat{P}$，其参数为 $\theta$，那么基于当前状态动作对 $(s_t,a_t)$，下一个状态 $s_t$ 的分布可以写为
$$
\widehat{P}(s_t,a_t)=N(\mu_\theta(s_t,a_t),\sum_\theta(s_t,a_t))
$$
这里我们可以采用神经网络来构建 $\mu_\theta$ 和 $\sum_\theta$。

在此基础之上，我们选择用**集成**（ensemble）方法来捕捉认知不确定性。具体而言，我们构建 $B$ 个网络框架一样的神经网络，它们的输入都是状态动作对，输出都是下一个状态的高斯分布的均值向量和协方差矩阵。但是它们的参数采用不同的随机初始化方式，并且当每次训练时，会从真实数据中随机采样不同的数据来训练。

有了环境模型的集成后，MPC 算法会用其来预测奖励和下一个状态。具体来说，每一次预测会从 $B$ 个模型中挑选一个来进行预测，因此一条轨迹的采样会使用到多个环境模型，如下图所示

![](https://hrl.boyuai.com/static/640.53a1a110.png)

实验证明，PETS 算法的效果非常好，但是由于每次选取动作都需要在环境模型上进行大量的模拟，因此运行速度非常慢。与 SAC 算法的结果进行对比可以看出，PETS 算法大大提高了样本效率，在比 SAC 算法的环境交互次数少得多的情况下就取得了差不多的效果。



# 基于模型的策略优化

PETS 算法是基于模型的强化学习算法中的一种，它没有显式构建一个策略（即一个从状态到动作的映射函数）。 Dyna-Q 算法也是一种基于模型的强化学习算法，但是 Dyna-Q 算法中的模型只存储之前遇到的数据，只适用于表格型环境。

而在连续型状态和动作的环境中，我们需要像 PETS 算法一样学习一个用神经网络表示的环境模型，此时若继续利用 Dyna 的思想，可以在任意状态和动作下用环境模型来生成一些虚拟数据，这些虚拟数据可以帮助进行策略的学习。如此，通过和模型进行交互产生额外的虚拟数据，对真实环境中样本的需求量就会减少，因此通常会比无模型的强化学习方法具有更高的采样效率。

**基于模型的策略优化** (model-based policy optimization，MBPO）算法基于以下两个关键的观察： (1) 随着环境模型的推演步数变长，模型累积的复合误差会快速增加，使得环境模型得出的结果变得很不可靠； (2) 必须要权衡推演步数增加后模型增加的误差带来的负面作用与步数增加后使得训练的策略更优的正面作用，二者的权衡决定了推演的步数。

MBPO 算法在这两个观察的基础之上，提出只使用模型来从之前访问过的真实状态开始进行较短步数的推演，而非从初始状态开始进行完整的推演。这就是 MBPO 中的**分支推演**（branched rollout）的概念，即在原来真实环境中采样的轨迹上面推演出新的“短分支”。这样做可以使模型的累积误差不至于过大，从而保证最后的采样效率和策略表现。

![](https://hrl.boyuai.com/static/640.ef98f8e0.png)

MBPO 与 Dyna-Q 算法十分类似。Dyna-Q 采用的无模型强化学习部分是 Q-learning，而 MBPO 采用的是 SAC。此外，MBPO 算法中环境模型的构建和 PETS 算法中一致，都使用模型集成的方式，并且其中每一个环境模型的输出都是一个高斯分布。

MBPO 的具体算法框架如下。MBPO 算法会把真实环境样本作为分支推演的起点，使用模型进行一定步数的推演，并用推演得到的模型数据用来训练模型。

- 初始化策略 $\pi_\phi$、环境模型参数 $p_\theta$、真实环境数据集 $D_{env}$、模型数据集 $D_{model}$
- for 轮数 $n=1\to N$  do
  - 通过环境数据来训练模型参数 $p_\theta$
  -  for 时间步 $t=1\to T$ do
    -  根据策略 $\pi_\phi$ 与环境交互，并将交互的轨迹添加到 $D_{env}$ 中
    -  for 模型推演次数 $e=1\to E$ do
      -  从 $D_{env}$ 中均匀随机采样一个状态 $s_t$
      -  以 $s_t$ 为初始状态，在模型中使用策略 $\pi_\phi$ 进行步的推演，并将生成的轨迹添加到 $D_{model}$ 中
    -  end for
    -  for 梯度更新次数 $g=1\to G$ do
      -  基于模型数据 $D_{model}$，使用 SAC 来更新策略参数 $\pi_\phi$
    -  end for
  -  end for
- end for

分支推演的长度 $k$ 是平衡样本效率和策略性能的重要超参数

**Code is available at [MBPO](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Model-based%20policy%20optimization.ipynb)**

实验证明，相比无模型的强化学习算法，基于模型的方法 MBPO 在样本效率上要高很多。虽然实验效果不如 PETS 算法优秀，但是在许多更加复杂的环境中（如 Hopper 和 HalfCheetah），MBPO 的表现远远好于 PETS 算法。



# 离线强化学习

无论是**在线策略**（on-policy）算法还是**离线策略**（off-policy）算法，都有一个共同点：智能体在训练过程中可以不断和环境交互，得到新的反馈数据。二者的区别主要在于在线策略算法会直接使用这些反馈数据，而离线策略算法会先将数据存入经验回放池中，需要时再采样。然而，在现实生活中的许多场景下，让尚未学习好的智能体和环境交互可能会导致危险发生，或是造成巨大损失。

因此，**离线强化学习**（offline reinforcement learning）的目标是，在智能体不和环境交互的情况下，仅从已经收集好的确定的数据集中，通过强化学习算法得到比较好的策略。离线强化学习和在线策略算法、离线策略算法的区别如下图所示。

![](https://hrl.boyuai.com/static/960.eab822fa.png)

## 批量限制 Q-learning 算法

离线强化学习和离线策略强化学习很像，都要从经验回放池中采样进行训练，并且离线策略算法的策略评估方式也多种多样。因此，研究者们最开始尝试将离线策略算法直接照搬到离线的环境下，仅仅是去掉算法中和环境交互的部分。然而，这种做法完全失败了。研究者进行了 3 个简单的实验。第一个实验，作者使用 DDPG 算法训练了一个智能体，并将智能体与环境交互的所有数据都记录下来，再用这些数据训练离线 DDPG 智能体。第二个实验，在线 DDPG 算法在训练时每次从经验回放池中采样，并用相同的数据同步训练离线 DDPG 智能体，这样两个智能体甚至连训练时用到的数据顺序都完全相同。第三个实验，在线 DDPG 算法在训练完毕后作为专家，在环境中采集大量数据，供离线 DDPG 智能体学习。

离线 DDPG 智能体的表现都远远差于在线 DDPG 智能体，即便是第二个实验的同步训练都无法提高离线智能体的表现。在第三个模仿训练实验中，离线智能体面对非常优秀的数据样本却什么都没学到！针对这种情况，研究者指出，**外推误差**（extrapolation error）是离线策略算法不能直接迁移到离线环境中的原因。

外推误差，是指由于当前策略可能访问到的状态动作对与从数据集中采样得到的状态动作对的分布不匹配而产生的误差。为什么在线强化学习算法没有受到外推误差的影响呢？因为对于在线强化学习，即使训练是离线策略的，智能体依然有机会通过与环境交互及时采样到新的数据，从而修正这些误差。但是在离线强化学习中，智能体无法和环境交互。因此，一般来说，离线强化学习算法要想办法尽可能地限制外推误差的大小，从而得到较好的策略。

为了减少外推误差，当前的策略需要做到只访问与数据集中相似的 $(s,a)$​ 数据。满足这一要求的策略称为**批量限制策略**（batch-constrained policy）。具体来说，这样的策略在选择动作时有 3 个目标：

- 最小化选择的动作与数据集中数据的距离；
- 采取动作后能到达与离线数据集中状态相似的状态；
- 最大化函数 $Q$。

对于标准的**表格**（tabular）型环境，状态和动作空间都是离散且有限的。标准的 Q-learning 更新公式可以写为：
$$
Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha(\gamma+\gamma Q(s',\underset{a'}{arg\;max\;}Q(s',a')))
$$
这时，只需要把策略 $\pi$ 能选择的动作限制在数据集 $D$​ 内，就能满足上述 3 个目标的平衡，这样就得到了表格设定下的批量限制 Q-learning（batch-constrained Q-learning，BCQ）算法：
$$
Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha(\gamma+\gamma Q(s',\underset{a'\;s.t.(s',a')\in D}{arg\;max\;}Q(s',a')))
$$
可以证明，如果数据中包含了所有可能的 $(s,a)$ 对，按上式进行迭代可以收敛到最优的价值函数 $Q^*$

连续状态和动作的情况要复杂一些，因为批量限制策略的目标需要被更详细地定义。例如，该如何定义两个状态动作对的距离呢？BCQ 采用了一种巧妙的方法：训练一个生成模型 $G_w(s)$。对于数据集 $D$ 和其中的状态 $s$，生成模型 $G_w(s)$ 能给出与 $D$ 中数据接近的一系列动作 $a_1,\cdots,a_n$用于 $Q$ 网络的训练。更进一步，为了增加生成动作的多样性，减少生成次数，BCQ 还引入了扰动模型 $\xi_\phi(s,a,\Phi)$。输入 $(s,a)$ 时，模型给出一个绝对值最大为 $\Phi$ 的微扰并附加在动作上。这两个模型综合起来相当于给出了一个批量限制策略 $\pi$：
$$
\pi(s)=\underset{a_i+\xi_\phi(s,a,\Phi)}{arg\;max\;}Q_\theta(s,a_i+\xi_\phi(s,a,\Phi)),\quad \{a_i\sim G_w(s)\}_{i=1}^n
$$
其中，生成模型 $G_w(s)$ 用**变分自动编码器**（variational auto-encoder, VAE）实现；扰动模型直接通过确定性策略梯度算法训练，目标是使函数 $Q$​ 最大化：
$$
\phi\leftarrow argmax_\phi\sum_{(s,a)\in D}Q_\theta(s,a+\xi_\phi(s,a,\Phi))
$$

## 保守 Q-learning 算法

略



# 目标导向的强化学习

PPO、SAC 等经典的深度强化学习算法，大部分算法都能在各自的任务中取得比较好的效果，但是它们都局限在单个任务上，换句话说，对于训练完的算法，在使用时它们都只能完成一个特定的任务。如果面对较为复杂的复合任务，之前的强化学习算法往往不容易训练出有效的策略。

**目标导向的强化学习**（goal-oriented reinforcement learning，GoRL）可以学习一个策略，使其可以在不同的**目标**（goal）作为条件下奏效，以此来解决较为复杂的决策任务。

有别于一般的强化学习算法中定义的马尔可夫决策过程，在目标导向的强化学习中，使用一个扩充过的元组 $<S,A,P,r_g,G,\phi>$ 来定义 MDP，其中，$S$ 是状态空间，$A$ 是动作空间，$P$ 是状态转移函数，$G$ 是目标空间，$\phi$ 是一个将状态 $s$ 从状态空间映射为目标空间内的一个目标 $g$ 的函数，$r_g$ 是奖励函数，与目标 $g$ 有关。

在目标导向的强化学习中，任务是由目标定义的，并且目标本身是和状态 $s$ 相关的，可以将一个状态 $s$ 使用映射函数 $\phi$ 映射为目标 $\phi(s)\in G$

奖励函数不仅与状态 $s$ 和动作 $a$ 相关，在目标导向强化学习中，还与设定的目标相关，以下是其中一种常见的形式：
$$
r_g(s_t,a_t,s_{t+1}) = 
\begin{cases}
0, & ||\phi(s_{t+1})-g||_2 \leq \sigma_g \\
-1, & otherwise
\end{cases}
$$
其中，$\sigma_g$ 是一个比较小的值，表示到达目标附近就不会受到-1的惩罚。在目标导向强化学习中，由于对于不同的目标，奖励函数是不同的，因此状态价值函数 $V(s,g)$ 也是基于目标的，动作状态价值函数 $Q(s,a,g)$ 同理。

目标导向的强化学习的优化目标：定义 $v_0$ 为环境中初始状态 $s_0$ 与目标 $g$ 的联合分布，那么 GoRL 的目标为优化策略 $\pi(a|s,g)$，使以下目标函数最大化：$E_{(s_0,g)\sim v_0}[V^\pi(s_0,g)]$

可以发现目标导向的强化学习的奖励往往是非常稀疏的，由于智能体在训练初期难以完成目标而只能得到的奖励，从而使整个算法的训练速度较慢。

为了有效地利用这些“失败”的经验，**事后经验回放**（hindsight experience replay，HER）算法被提出，成为 GoRL 的一大经典方法。

假设现在使用策略 $\pi$ 在环境中以 $g$ 为目标进行探索，得到了这么一条轨迹：$s_1,s_2,\cdots,s_T$，并且 $g\neq s_1,s_2,\cdots,s_T$。这意味着这整一条轨迹上，我们得到的奖励值都是-1，这对我们的训练起到的帮助很小。虽然并没有达到目标 $g$，但是策略在探索的过程中，完成了 $s_1,s_2,\cdots,s_T$ 等对应的目标，即完成了$\phi(s_1),\phi(s_2),\cdots,\phi(s_T)$ 等目标。如果用这些目标来将原先的目标 $g$ 替换成新的目标 $g'$，重新计算轨迹中的奖励值，就能使策略从失败的经验中得到对训练有用的信息。

HER 算法的具体流程如下，值得注意的是，这里的策略优化算法可以选择任意合适的算法，比如 DQN、DDPG 等。

- 初始化策略 $\pi$ 的参数 $\theta$，初始化经验回放池 $R$
- For 序列  $e=0\to E$
  -  根据环境给予的目标 $g$ 和初始状态 $s_0$，使用 $\pi$ 在环境中采样得到轨迹 $\{s_0,a_0,r_0,\cdots,s_T,a_T,r_T,s_{T+1}\}$，将其以 $(s,a,r,s',g)$ 的形式存入 $R$ 中
  -  从 $R$ 中采样 $N$ 个 $(s,a,r,s',g)$ 元组
  -  对于这些元组，选择一个状态 $s''$，将其映射为新的目标 $g'=\phi(s'')$ 并计算新的奖励值 $r'=r_{g'}(s,a,s')$，然后用新的数据 $(s,a,r',s',g')$ 替换原先的元组
  -  使用这些新元组，对策略 $\pi$ 进行训练
- End for

对于算法中状态 $s''$ 的选择，HER 提出了 3 种不同的方案

- future： 选择与被改写的元组 $\{s,a,s',g\}$ 处于同一个轨迹并在时间上处于 $s$ 之后的某个状态作为 $s''$
- episode： 选择与被改写的元组 $\{s,a,s',g\}$ 处于同一个轨迹的某个状态作为 $s''$
- random： 选择经验回放池中的某个状态作为 $s''$

在 HER 的实验中，future 方案给出了最好的效果，该方案也最直观。

**Code is available at [GoRL](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Goal-oriented%20reinforcement%20learning.ipynb)**

通过实验，可以观察到使用 HER 算法后，效果有显著提升。这里 HER 算法的主要好处是通过重新对历史轨迹设置其目标（使用 future 方案）而使得奖励信号更加稠密，进而从原本失败的数据中学习到使“新任务”成功的经验，提升训练的稳定性和样本效率。



# 多智能体强化学习

单智能体强化学习算法，其基本假设是动态环境是**稳态的**（stationary），即状态转移概率和奖励函数不变，并依此来设计相应的算法。而如果环境中还有其他智能体做交互和学习，那么任务则上升为**多智能体强化学习**（multi-agent reinforcement learning，MARL），如下图所示。

<img src="https://hrl.boyuai.com/static/640.7a0e3ef2.png" style="zoom:50%;" />

多智能体的情形相比于单智能体更加复杂，因为每个智能体在和环境交互的同时也在和其他智能体进行直接或者间接的交互。因此，多智能体强化学习要比单智能体更困难，其难点主要体现在以下几点：

- 由于多个智能体在环境中进行实时动态交互，并且每个智能体在不断学习并更新自身策略，因此在每个智能体的视角下，环境是**非稳态的**（non-stationary），即对于一个智能体而言，即使在相同的状态下采取相同的动作，得到的状态转移和奖励信号的分布可能在不断改变；
- 多个智能体的训练可能是多目标的，不同智能体需要最大化自己的利益；
- 训练评估的复杂度会增加，可能需要大规模分布式训练来提高效率。

**Code is available at [MARL](https://github.com/CorneliusDeng/UESTC/blob/main/Dive%20Into%20RL/Multi-agent%20reinforcement%20learning.ipynb)**

## 问题建模

将一个多智能体环境用一个元组 $(N,S,A,R,P)$ 表示，其中 $N$ 是智能体的数目，$S=S_1\times \cdots \times S_N$ 是所有智能体的状态集合，$A=A_1\times \cdots \times A_N$ 是所有智能体的动作集合，$R=r_1\times \cdots \times r_N$ 是所有智能体奖励函数的集合，$P$ 是环境的状态转移概率。一般多智能体强化学习的目标是为每个智能体学习一个策略来最大化其自身的累积奖励。

## 多智能体强化学习的基本求解范式

面对上述问题形式，最直接的想法是基于已经熟悉的单智能体算法来进行学习，这主要分为两种思路。

- **完全中心化**（fully centralized）方法：将多个智能体进行决策当作一个超级智能体在进行决策，即把所有智能体的状态聚合在一起当作一个全局的超级状态，把所有智能体的动作连起来作为一个联合动作。这样做的好处是，由于已经知道了所有智能体的状态和动作，因此对这个超级智能体来说，环境依旧是稳态的，一些单智能体的算法的收敛性依旧可以得到保证。然而，这样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会导致维度爆炸，训练复杂度巨幅提升的问题往往不可解决。
- **完全去中心化**（fully decentralized）方法：与完全中心化方法相反的范式便是假设每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变。完全去中心化方法直接对每个智能体用一个单智能体强化学习算法来学习。这样做的缺点是环境是非稳态的，训练的收敛性不能得到保证，但是这种方法的好处在于随着智能体数量的增加有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。

## IPPO 算法

完全去中心化的算法这类算法被称为**独立学习**（independent learning）。由于对于每个智能体使用单智能体算法 PPO 进行训练，所因此这个算法叫作**独立 PPO**（Independent PPO，IPPO）算法。具体而言，这里使用的 PPO 算法版本为 PPO-截断，其算法流程如下：

- 对于 $N$ 个智能体，为每个智能体初始化各自的策略以及价值函数
- for 训练轮数 $k=0,1,2\cdots$  do:
  - 所有智能体在环境中交互分别获得各自的一条轨迹数据
  - 对每个智能体，基于当前的价值函数用 GAE 计算优势函数的估计
  - 对每个智能体，通过最大化其 PPO-截断的目标来更新其策略
  - 对每个智能体，通过均方误差损失函数优化其价值函数
- end for

实验证明，当智能体数量较少的时候，IPPO 这种完全去中心化学习在一定程度上能够取得好的效果，但是最终达到的胜率也比较有限。这可能是因为多个智能体之间无法有效地通过合作来共同完成目标。这时候可能就需要引入更多的算法来考虑多个智能体之间的交互行为，或者使用**中心化训练去中心化执行**（centralized training with decentralized execution，CTDE）的范式来进行多智能体训练。

## MADDPG 算法

所谓中心化训练去中心化执行是指在训练的时候使用一些单个智能体看不到的全局信息而以达到更好的训练效果，而在执行时不使用这些信息，每个智能体完全根据自己的策略直接动作以达到去中心化执行的效果。中心化训练去中心化执行的算法能够在训练时有效地利用全局信息以达到更好且更稳定的训练效果，同时在进行策略模型推断时可以仅利用局部信息，使得算法具有一定的扩展性。CTDE 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。

CTDE 算法主要分为两种：一种是基于值函数的方法，例如 VDN，QMIX 算法等；另一种是基于 Actor-Critic 的方法，例如 MADDPG 和 COMA 等。

**多智能体 DDPG**（muli-agent DDPG，MADDPG）算法从字面意思上来看就是对于每个智能体实现一个 DDPG 的算法。所有智能体共享一个中心化的 Critic 网络，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导，而执行时每个智能体的 Actor 网络则是完全独立做出行动，即去中心化地执行。

CTDE 算法的应用场景通常可以被建模为一个**部分可观测马尔可夫博弈**（partially observable Markov games）：用 $S$ 代表 $N$ 个智能体所有可能的状态空间，这是全局的信息。对于每个智能体 $i$，其动作空间为 $A_i$，观测空间为 $O_i$，每个智能体的策略 $\pi_{\theta_i}:O_i \times A_i \rightarrow [0,1]$ 是一个概率分布，用来表示智能体在每个观测下采取各个动作的概率。环境的状态转移函数为 。每个智能体的奖励函数为 $r_i:S\times A\rightarrow R$，每个智能体从全局状态得到的部分观测信息为 $o_i:S\rightarrow O_i$，初始状态分布为 $\rho:S\rightarrow [0,1] $ 。每个智能体的目标是最大化其期望累积奖励 $E[\sum^T_{t=0}\gamma^tr_i^t]$。

<img src="https://hrl.boyuai.com/static/480.c487a865.png" style="zoom:67%;" />

如上图所示，每个智能体用 Actor-Critic 的方法训练，但不同于传统单智能体的情况，在 MADDPG 中每个智能体的 Critic 部分都能够获得其他智能体的策略信息。具体来说，考虑一个有 $N$ 个智能体的博弈，每个智能体的策略参数为 $\theta=\{\theta_1,\cdots,\theta_N\}$，记 $\pi=\{\pi_1,\cdots,\pi_N\}$ 为所有智能体的策略集合，那么我们可以写出在随机性策略情况下每个智能体的期望收益的策略梯度：
$$
\nabla_{\theta_i}J(\theta_i)=E_{s\sim p^\mu,a\sim \pi_i}
[\nabla_{\theta_i}\;log\;\pi_i(a_i|o_i)Q_i^\pi(x,a_1,\cdots,a_N))]
$$
其中，$Q_i^\pi(x,a_1,\cdots,a_N)$ 就是一个中心化的动作价值函数。

对于确定性策略来说，考虑现在有 $N$ 个连续的策略 $\mu_{\theta_i}$，可以得到 DDPG 的梯度公式：
$$
\nabla_{\theta_i}J(\mu_i)=E_{x\sim D}
[\nabla_{\theta_i}\mu_i(o_i) \nabla_{a_i}Q_i^\mu(x,a_1,\cdots,a_N)|_{a_i=\mu_i(o_i)}]
$$
其中，$D$ 是用来存储数据的经验回放池，它存储的每一个数据为 $(x,x',a_1,\cdots,a_N,r_1,\cdots,r_N)$。而在 MADDPG 中，中心化动作价值函数可以按照下面的损失函数来更新：
$$
L(w_i) = E_{x,a,r,x'}[(Q_i^\mu(x,a_1,\cdots,a_N)-y)^2] \\
y = r_i+\gamma Q_i^{\mu'}(x',a'_1,\cdots,a'_N)|_{a_j^{'}=\mu_j^{'}(o_j)}
$$
其中，$\mu'=(\mu'_{\theta_1},\cdots,\mu'_{\theta_N})$ 是更新价值函数中使用的目标策略的集合，它们有着延迟更新的参数。

MADDPG 的具体算法流程如下：

- 随机初始化每个智能体的 Actor 网络和 Critic 网络
- for 序列 $e=1\to E$  do
  - 初始化一个随机过程 $N$，用于动作探索
  - 获取所有智能体的初始观测  $x$
  - for $t=1\to T$ do：
    - 对于每个智能体 $i$，用当前的策略选择一个动作 $a_i=\mu_{\theta_i}(o_i)+N_t$
    - 执行动作 $a=(a_1,\cdots,a_N)$ 并且获得奖励 $r$ 和新的观测 $x'$
    - 把 $(x,a,r,x')$ 存储到经验回放池 $D$ 中
    - 从 $D$ 中随机采样一些数据
    - 对于每个智能体 $i$，中心化训练 Critic 网络
    - 对于每个智能体 $i$，训练自身的 Actor 网络
    - 对每个智能体 $i$，更新目标 Actor 网络和目标 Critic 网络
  - end for
- end for



# MARL Survey

参考链接：[MARL Survey](https://cloud.tencent.com/developer/article/1618396)

根据综述文章 [A Survey and Critique of Multiagent Deep Reinforcement Learning](https://arxiv.org/abs/1810.05587)， MARL 算法分为以下四类：

- Analysis of emergent behaviors（行为分析）
- Learning communication（通信学习）
- Learning cooperation（协作学习）
- Agents modeling agents（智能体建模）

## Analysis of emergent behaviors

行为分析类别的算法主要是将单智能体强化学习算法（SARL）直接应用到多智能体环境之中，每个智能体之间相互独立，遵循 Independent Q-Learning 的算法思路

- [Multiagent Cooperation and Competition with Deep Reinforcement Learning](https://arxiv.org/abs/1511.08779) 这篇文章首次将 DQN 算法与 IQL 结合起来，并将其应用到 ALE 环境中的 Pong 游戏中，在这样一个环境中，每个智能体拥有独立的 Q network，独自采集数据并进行训练，都有对环境的全局观察，动作空间包含以下四个维度：上移、下移、保持不动以及击球。实验证明，在将 DQN 直接应用到多智能体环境中，也能够达到一个比较好的性能，即便 IQL 算法是一个十分简单的算法，没有办法处理环境非平稳问题，但是依旧是一个比较强的基准算法。由于是一篇比较早期的多智能体论文，因而文中还提到了一些比较有用的实验部分的细节问题：

  - Q 值的收敛与否一定程度上反映了 DQN 算法的收敛与否
  - 在训练之前首先随机在环境中采样一些 state 作为测试集，监控这些 state 对应的最大的 Q 值平均值来判断算法收敛与否

- [Cooperative Multi-Agent Control Using Deep Reinforcement Learning](https://link.springer.com/chapter/10.1007/978-3-319-71682-4_5) 本文将基于值函数的算法 DQN、基于策略梯度的算法 TRPO 以及演员-评论家算法 DDPG 与 IQL 算法以及循环神经网络（或前向神经网络）相结合，应用到局部观察的多智能体环境中。其与前一篇 paper 的区别在于，为了增加算法在大规模场景下的可扩展性，**所有的智能体都共享同一套参数**。训练时所有智能体采样得到的样本进行汇总，用来更新共享的模型参数。同时，为了进一步保证不同的智能体即使在共享参数的情况下也能够表现出不同的行为，其模型输入除了局部的观察外，还**包括自身的索引**。实验结果表明，在使用前向神经网络构建模型时，基于策略梯度的 TRPO 算法在最终性能上超越了另外两种算法；另外，使用循环神经网络构建模型时，性能超过使用前向神经网络的情况。

  最后，文章还提出了一种课程学习的训练方法：假设我们的课程学习环境是先从 2 个智能体开始训练，逐渐增加到最多 10 个智能体。

  我们首先**构建一个迪利克雷分布**，该分布的概率密度函数最大值点初始偏向于较少的智能体个数，每一次训练时都从这个分布中采样智能体的个数并进行强化学习算法的训练，直到算法在这些采样出的环境中的性能都达到了某个阈值。那么接下来我们就改变迪利克雷分布的参数，来使得其概率密度函数最大值点逐渐偏向于较多的智能体个数。循环上述过程直到算法在 10 个智能体上也能达到比较好的性能，这样我们就完成了课程学习。

## Learning communication

属于这一类别的多智能体强化学习方法显式假设智能体之间存在信息的交互，并在训练过程中学习如何根据自身的局部观察来生成信息，或者来确定是否需要通信、与哪些智能体通信等等。在训练完毕后运行的过程中，需要显式依据其余智能体传递的信息来进行决策。

- [Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://proceedings.neurips.cc/paper/2016/hash/c7635bfd99248a2cdef8249ef7bfbef4-Abstract.html) 这篇文章最先在深度多智能体强化学习中引入通信学习，其解决的强化学习问题是 Dec-POMDP 问题。换句话说，在 Dec-POMDP 中，所有智能体共享一个全局的回报函数，所以是一个完全协作环境，每个智能体只拥有自己的局部观察。文中假设通信信道是离散的，即智能体之间只能能传递离散的信息（即 one-hot 向量）。本文采用的是 CTDE 框架（即中心化训练去中心化执行，Centralized Training Decentralized Execution），在训练时不对智能体之间的信息传递进行限制（由于是中心化的训练器，所以智能体之间的信息传递完全由这个训练器接管），甚至在训练时可以使用连续的信息。但是训练完毕之后运行时，智能体之间才进行真正的通信，并且该通信信道是离散（如果训练时是连续的，则在运行时要对信息进行离散化）的。

  本文提出了两种算法，后一种是前一种的改进版本，具体名称陈列如下：

  - Reinforced Inter-Agent Learning (RIAL)：由于本文限定通信信道是离散的，因而 RIAL 算法将生成的信息也作为一个离散的动作空间来考虑。RIAL 算法将 DRQN 算法与 IQL 算法结合起来，并显式在智能体之间传输可学习的信息来增加智能体对于环境的感知，从而解决 IQL 面临的因为环境非平稳所带来的性能上的问题。RIAL 算法使用了两个 Q-network，分别输出原始的动作以及离散的信息。并且 RIAL 算法中 Q network 的输入不仅仅是局部观察，还包括**上一时间步**其余智能体传递过来的信息。另外还需提及的一点是，在多智能体环境中，采用 Experience Replay 反而会导致算法性能变差。这是因为之前收集的样本与现在收集的样本，由于智能体策略更新的原因，两者实际上是从不同的环境中收集而来，从而使得这些样本会阻碍算法的正常训练。
  - Differentiable Inter-Agent Learning (DIAL)：虽然 RIAL 算法可以在智能体之间共享参数，但它仍然没有充分利用中心化训练的优势。而且，智能体不会互相提供，有关其接收到的信息的发送方智能体的通信行为的反馈。然而人类交流富有紧密的反馈循环。例如，在面对面互动时，听众会向发言者反馈一些非语言信息（例如眼神，微动作等），表明理解和兴趣的程度。RIAL 算法缺乏这种反馈机制，但是后者对于通信学习是非常重要的。所以本文在 RIAL 的基础上提出了一种新的算法 DIAL，该算法通过通信信道讲梯度信息从信息接收方传回到信息发送方。具体来说，在中心化训练时，信息发送方的信息动作输出直接连接到信息接收方，并且为了能够实现端到端训练，此时的信息将不再是离散值而是连续值。训练完毕之后执行时，通过这个实值的正负进行 one-hot 离散化。同时为了增加算法的鲁棒性，这个信息实值是从一个拥有固定方差的高斯分布中采样而来，该分布的均值即信息发送方生成的实值。

- [Learning Multiagent Communication with Backpropagation](https://proceedings.neurips.cc/paper/2016/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html) 本文假设智能体之间传递的消息是连续变量（不像 RIAL 或者 DIAL 是离散的），文章采用的强化学习算法是 policy gradient 方法。本文解决的也同样是 Dec-POMDP 问题，遵循的是中心化训练中心化执行 CTCE（Centralized Training Centralized Execution）框架，因而在大规模的多智能体环境下，由于网络输入的数据维度过大，会给强化学习算法的训练带来困难。算法被命名为 CommNet，该算法采用的信息传递方式是采用广播的方式，文中认为可以对算法做出些许修改，让每个智能体只接收其相邻几个智能体的信息。文中还提出了两种对上述算法可以采取的改进方式：

  - 可以对模型中间的结构加上 skip connection，类似于 ResNet。这样可以使得智能体在学习的过程中同时考虑局部信息以及全局信息，类似于计算机视觉领域 multi-scale 的思想
  - 可以将其中网络结构换成 RNN-like，例如 LSTM 或者 GRU 等等，这是为了处理局部观察所带来的 POMDP 问题

- [Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) 本文提出了一个新的算法 BiCNet，同样假设智能体之间传递的信息是离散的，旨在解决 zero-sum Stochastic Game (SG) 问题。算法基于演员-评论家算法框架，使用的是 DDPG 算法，并且考虑到算法在大规模多智能体环境下的可扩展性问题，智能体之间共享模型参数，并且算法假设每个智能体都拥有同样的全局观察（全局状态），这也是本文的局限之一。另外，BiCNet 同样遵循 CTCE 框架。BiCNet 中所有的智能体都拥有独立的回报函数以及 Q-network 以及 policy network，但这些 network 中部分参数是共享的。这些智能体一起在环境中进行数据采样，最后将所有的数据集中起来，更新他们的共享部分的参数

- [Learning Attentional Communication for Multi-Agent Cooperation](https://proceedings.neurips.cc/paper/2018/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html) 前面我们讨论的三种算法，R(D)IAL、CommNet 以及 BiCNet，都是每一个时间步所有智能体之间都要进行通信，或者每个智能体与自己相邻的智能体进行通信，这在本文看来属于一个预定义的通信模式，不够灵活。本文的出发点正是基于此，希望提出一个算法，**能够让智能体在任何时刻，自己决定是否需要与其他智能体进行通信，以及与哪些智能体进行通信。**本文为了达到上述目标，提出了一个基于注意力机制的通信模型 ATOC，该模型基于智能体的局部观察，可同时适用于协作环境（共享一个全局的回报函数或者拥有各自的回报函数）以及竞争环境（实质也是协作环境，因为算法只控制一方）。其基本思想是，通过其局部观察与动作（**其实是策略网络的中间层输出**）的编码，来决定其是否需要与其视野范围内的其他智能体进行通信，以及哪些智能体进行通信。对于决定进行通信的智能体，我们称之为发起者（initiator），这个发起者从其视野范围内选择协作者共同形成一个通信群组，这个通信群组在整个 episode 中动态变化并且只在需要的时候存在。本文采用一个双向的 LSTM 网络作为通信群组之间的通信信道（类似于 BiCNet），这个 LSTM 以通信群组中各个智能体的局部观察与动作的编码（之前提到的）作为输入，输出的 higher-level 的编码信息作为各智能体策略网络的额外输入，来指导协作策略的生成。ATOC 算法采用的是演员-评论家框架，DDPG 算法，遵循 CTDE 框架，同时考虑到算法在大规模多智能体环境下的可扩展性，所有智能体共享策略网络、注意力单元以及通信信道的参数。为什么 ATOC 算法要选择视野范围之内的智能体建立通信群组，主要有以下两点原因：

  - 邻近的智能体的局部观察更加相似，因而更容易互相理解
  - 邻近的智能体之间更容易协作

  一个发起者邻近的智能体可以分为以下三种：

  - 其他发起者
  - 被其他发起者选定的智能体
  - 没有被其他发起者选定的智能体

  与 BiCNet 相比，ATOC 有以下相似以及改进点：

  - 为了进一步凸显不同智能体在特定任务中的不同定位，固定其在 LSTM 中的位置
  - 将 RNN 通信信道改为 LSTM，从而过滤掉无关信息

  最后，ATOC 在竞争环境下的训练方式是与 baseline 对抗训练，而不是分别 self-play 最后再进行比较。

- [Learning to Schedule Communication in Multi-agent Reinforcement Learning](https://openreview.net/forum?id=SJxu5iR9KQ) 与 ATOC 相比，这篇工作关注的是另外一个问题。这里同样从 R(D)IAL、CommNet 以及 BiCNet 算法出发，这三种算法在每一个时间步所有的智能体之间都参与通信，或者是类似于 R(D)IAL 这样智能体两两之间相互传递信息，或者是像 CommNet 以及 BiCNet 这样所有的智能体都将自己的信息发送到通信信道中参与高级信息的生成。

  但是在现实情况中，通信信道的带宽是有限的，如果所有智能体都往这个有限带宽的信道中发送信息，则容量一旦超出，就会出现信息丢失或者阻塞等等情况。ATOC 是通过限制每个发起者只能选择最多 个智能体加入到通信群组中，但是其选取的方式十分简单。本文将通信领域 MAC (Medium Access Control) 方法引入到多智能体强化学习中，来解决上述问题。

  本文提出的 SchedNet 算法，同样是解决 Dec-POMDP 问题，并遵循 CTDE 框架，基于演员-评论家算法。

## Learning cooperation

此类工作并不显式地学习智能体之间的通信，而是将 multi-agent learning 领域的一些思想引入到 MARL 中。而这类方案又可以分为三个类别

### 基于值函数的方法

- [Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward](https://dl.acm.org/doi/10.5555/3237383.3238080) 基于值函数的方法可以说是多智能体强化学习算法最开始的尝试（例如 Independent Q-Learning， IQL 算法）。虽然前面也提到将 IQL 算法与 DQN 算法结合能够在多智能体问题上取得比较好的效果，但是对于较为复杂的环境，IQL 还是无法很好地处理由于环境非平稳而带来的问题。而中心化的方法，即将所有智能体的状态空间以及动作空间合并，当作一个智能体来考虑的方法，虽然能够较好的处理环境非平稳性问题，但是也存在以下缺陷：

  - 在大规模多智能体环境中算法可扩展性较差
  - 由于所有智能体联合训练，一旦某个智能体较早学到一些有用的策略，则其余智能体会选择较为懒惰的策略。这是因为其余智能体由于进度较慢，从而做出的策略会阻碍已经学到一些策略的智能体，从而使得全局汇报下降

  本文提出的 VDN 方法的基本思想是，中心化地训练一个联合的 Q network，但是这个联合的网络是由所有智能体局部的 Q networks 加和得到，这样不仅可以通过中心化训练处理由于环境非平稳带来的问题，而且由于实际是在学习每个智能体的局部模型，因而解耦智能体之间复杂的相互关系。最后，由于训练完毕后每个智能体拥有只基于自己局部观察的 Q network，可以实现去中心化执行，即 VDN 遵循 CTDE 框架，并且解决的是 Dec-POMDP 问题。

- [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](http://proceedings.mlr.press/v80/rashid18a.html) QMIX 算法是 VDN 算法的后续工作，它的出发点是 VDN 做联合 Q-value 分解时只是进行简单的加和，这种做法会使得学到的局部 Q 函数表达能力有限，没有办法捕捉到智能体之间更复杂的相互关系，因而对 VDN 算法进行了修改。

- [QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning](http://proceedings.mlr.press/v97/son19a.html) QTRAN 算法是对 VDN 以及 QMIX 算法的进一步改进（所以这三个工作是一条线），本文认为直接根据局部 Q 函数采用神经网络去逼近联合 Q 函数是一件比较困难的事情，因而将整个逼近过程分为两步：首先采用 VDN 的方式得到加和的局部 Q 函数，作为联合 Q 函数的近似，接着去拟合局部 Q 函数与联合 Q 函数的差值。QTRAN 提出了两个算法 QTRAN-base 以及 QTRAN-alt。

### 基于演员-评论家的方法

将单智能体强化学习算法扩展到多智能体环境中，最简单就是 IQL 类别方法，但是此类方法在复杂环境中无法处理由于环境非平稳带来的问题；另一方面，虽然中心化方法能够处理上述问题，但是与 IQL 相比又失去了较好的可扩展性。

前面介绍的基于 value-based 方法通过 value decomposition 方式来解决可扩展性问题，那么对于基于演员-评论家方法，由于其结构的特殊性，我们可以通过中心化学习（共享/独立）评论家但是每个智能体独立的演员，来很好的处理算法可扩展性问题的同时，拥有很好的抗环境非平稳能力。

- [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html) 本文提出的 MADDPG 算法将 DDPG 算法扩展到多智能体环境中，MADDPG 算法假定每一个智能体拥有自己独立的 critic network 以及 actor network，并且假定每个智能体拥有自己独立的回报函数，这样 MADDPG 算法就可以同时解决协作环境、竞争环境以及混合环境下的多智能体问题。但是 MADDPG 算法假定每个智能体在训练时都能够获取其余所有智能体的局部观察以及动作，因而即使每个智能体的 critic network 是独立的，训练时也需要中心化训练，因而遵循 CTDE 框架。
- [Counterfactual Multi-Agent Policy Gradients](https://arxiv.org/abs/1705.08926) 本文提出的算法 COMA 旨在解决 Dec-POMDP 问题中的 multi-agent credit assignment 问题，即多智能体信用分配问题。这个问题简单概括来说，由于 Dec-POMDP 问题中所有智能体共享同一个全局回报，因而每个智能体不知道自己的行为到底对这个全局回报产生了多大的影响，这就是多智能体信用分配问题。 COMA 算法与 MADDPG 一样，遵循 CTDE 训练框架，但是因为解决的是 Dec-POMDP 问题，所以所有的智能体共享一个联合的 critic network，该 network 与 MADDPG 一样，基于所有智能体的局部观察以及动作，但是 actor network 是独立的并且只基于局部观察。COMA 与 MADDPG 在 actor network 上的不同之处在于前者使用的是 GRU 网络，为了更好的处理局部观察问题，但是后者使用的则是普通的 DNN。COMA 使用的是 vanilla 的 actor-critic 方法，其核心之处在于引入了一个 counterfactual 的 baseline 函数。这个思路是受到 difference rewards 方法启发的，该方法通过比较智能体遵循当前 actor network 进行决策得到的全局回报与遵循某个默认策略进行决策得到的全局回报，来解决多智能体信用分配问题。
- [Multiagent Soft Q-Learning](https://arxiv.org/abs/1804.09817) 本文提出了一个类似于 MADDPG 的遵循 CTDE 框架的 MASQL（论文中没有这样进行缩写） 算法，本质上是将 Soft Q-Learning 算法迁移到多智能体环境中，因而与将 DDPG 算法迁移到多智能体环境中的 MADDPG 算法类似，不过 MASQL 算法解决的是 Dec-POMDP 问题。与 MADDPG 相比，MASQL 算法并不只是换了一种单智能体算法并将其迁移到多智能体环境，而是为了解决 Dec-POMDP 中的 relative overgeneralization 问题。MASQL 算法与 MADDPG 算法一样，使用了一个中心化的 criti；但与 MADDPG 不同，每个智能体的 policy 不是只输出自己的action，而是输出所有智能体的 joint action，然后与环境交互时从中选取属于自己的 action。值从大到小意味着在训练初期会尽可能探索更大的 policy space，随着训练的进行会逐步收敛到最优的 policy 上。
- [Actor-Attention-Critic for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1810.02912) 本文提出的 MAAC 算法是在 MADDPG 上进行了一些修改，将 MADDPG 采用的 DDPG 算法替换为 SAC（soft actor-critic）算法，并将 COMA 提出的 counterfactual baseline 引入进来，因而可以同时处理协作、竞争以及混合环境，遵循 CTDE 框架。 其核心思想体现在，对于 MADDPG 算法，其每个智能体对应的 Q function 都是将其余智能体的局部观察以及动作无差别的作为输入，但是在现实场景中，智能体对于其余智能体的关注度是不一样的。为此，MAAC 算法将注意力机制引入到 Q function 的构建之中，本文注意力机制采用的是多头注意力机制。

### 基于经验回放缓存的方法

- [Stabilising experience replay for deep multi-agent reinforcement learning](http://proceedings.mlr.press/v70/foerster17b.html)  
- [Deep decentralized multi-task multi-agent reinforcement learning under partial observability](https://dl.acm.org/doi/10.5555/3305890.3305958)

由于这部分要介绍的两个工作主要聚焦于使用 ER 训练 Q-function 时增加稳定性（CommNet 甚至因为 ER 在 multi-agent 环境下的不稳定性而禁用了 ER），这两个方法前者遵循 CTDE 框架，并且类似 MADDPG 方法一样，均假设每个智能体拥有自己独立的 Q-function；后者则是完全独立的 IQL。这两个方法都是基于 Q-Learning 算法。

## Agents modeling agents

这一类方法主要聚焦于通过对其他智能体的策略、目标、类别等等建模来进行更好的协作或者更快地打败竞争对手。

- [Stabilising experience replay for deep multi-agent reinforcement learning](http://proceedings.mlr.press/v70/foerster17b.html) 除了智能体之间的协作，本文还提出了一种估计其他智能体策略的方法。这篇文章首先从 hyper Q-Learning 算法出发，该算法通过贝叶斯估计的方法来估计其他智能体的策略。但是这种方法会增加 Q function 的输入维度，使得 Q function 更难学习。上述情况在深度强化学习中更加明显。考虑如下一个简单的 idea，我们把其他智能体策略函数的参数作为额外输入 ，但是在深度强化学习中策略函数一般是 DNN，因而维度太高基本不可行。本文提出了一个十分简单的指纹表示—— **episode 索引号**。这样一个听上去过于简单的指纹在性能上确实可以带来提升。但是存在一个比较大的问题在于，当其他智能体的策略收敛之后，索引号还在不断增加。另外，本文在之前指纹的基础上还增加了一个新的指纹—— exploration rate，这个值也能够一定程度上反应 policy 的特点。
- [Machine Theory of Mind](https://www.deepmind.com/publications/machine-theory-of-mind) 这篇论文是将行为以及脑科学领域 Theory of Mind（ToM）理论引入到多智能体强化学习中，用以根据其他智能体历史行为数据来预测其未来行为等（**这里我们只关注行为**）。ToM 理论认为，预测一个智能体的未来行为，我们需要知道其性格（character）、思想（mental）以及当前状态，对于某个智能体 过去的轨迹数据，我们使用一个神经网络编码每一条轨迹，最后将这些编码加起来即可。