{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_print(N):\n",
    "    idxWithinGrid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x \n",
    "    gridStride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    # 从 idxWithinGrid 开始\n",
    "    # 每次以整个网格线程总数为跨步数\n",
    "    for i in range(idxWithinGrid, N, gridStride):\n",
    "        print(i)\n",
    "\n",
    "def main():\n",
    "    gpu_print[2, 4](32)\n",
    "    cuda.synchronize()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu vector add time 0.5130133628845215\n",
      "gpu streams vector add time 0.7255711555480957\n"
     ]
    }
   ],
   "source": [
    "# 将向量分拆成了5份，同时也创建了5个流，每个流执行1/5的“拷贝、计算、回写”操作，多个流之间异步执行\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_add(a, b, result, n):\n",
    "    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    if idx < n :\n",
    "        result[idx] = a[idx] + b[idx]\n",
    "\n",
    "def main():\n",
    "    n = 20000000\n",
    "    x = np.arange(n).astype(np.int32)\n",
    "    y = 2 * x\n",
    "    \n",
    "    start = time()\n",
    "    x_device = cuda.to_device(x)\n",
    "    y_device = cuda.to_device(y)\n",
    "    out_device = cuda.device_array(n)\n",
    "\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = math.ceil(n / threads_per_block)\n",
    "    \n",
    "    # 使用默认流\n",
    "    gpu_add[blocks_per_grid, threads_per_block](x_device, y_device, out_device, n)\n",
    "    gpu_result = out_device.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "    print(\"gpu vector add time \" + str(time() - start))\n",
    "    \n",
    "    start = time()\n",
    "\n",
    "    # 使用5个stream\n",
    "    number_of_streams = 5\n",
    "    # 每个stream处理的数据量为原来的 1/5\n",
    "    # 符号//得到一个整数结果\n",
    "    segment_size = n // number_of_streams\n",
    "    \n",
    "    # 创建5个cuda stream\n",
    "    stream_list = list()\n",
    "    for i in range (0, number_of_streams):\n",
    "        stream = cuda.stream()\n",
    "        stream_list.append(stream)\n",
    "    \n",
    "    threads_per_block = 1024\n",
    "    # 每个stream的处理的数据变为原来的1/5\n",
    "    blocks_per_grid = math.ceil(segment_size / threads_per_block)\n",
    "    streams_out_device = cuda.device_array(segment_size)\n",
    "    streams_gpu_result = np.empty(n)\n",
    "    \n",
    "    # 启动多个stream\n",
    "    for i in range(0, number_of_streams):\n",
    "        # 传入不同的参数，让函数在不同的流执行\n",
    "        x_i_device = cuda.to_device(x[i * segment_size : (i + 1) * segment_size], stream=stream_list[i])\n",
    "        y_i_device = cuda.to_device(y[i * segment_size : (i + 1) * segment_size], stream=stream_list[i])\n",
    "        \n",
    "        gpu_add[blocks_per_grid, threads_per_block, stream_list[i]](\n",
    "                x_i_device, \n",
    "                y_i_device, \n",
    "                streams_out_device,\n",
    "                segment_size)\n",
    "        \n",
    "        streams_gpu_result[i * segment_size : (i + 1) * segment_size] = streams_out_device.copy_to_host(stream=stream_list[i])\n",
    "\n",
    "    cuda.synchronize()\n",
    "    print(\"gpu streams vector add time \" + str(time() - start))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu matmul time :1.2168152332305908\n",
      "cpu matmul time :1.236732006072998\n",
      "gpu result correct\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"  矩阵乘法 C = A * B\n",
    "    \"\"\"\n",
    "    # Numba库提供了更简易的计算方法\n",
    "    # x, y = cuda.grid(2)\n",
    "    # 具体计算公式如下\n",
    "    row = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    col = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "    \n",
    "    \n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "def main():\n",
    "    # 初始化矩阵\n",
    "    M = 6000\n",
    "    N = 4800\n",
    "    P = 4000\n",
    "    A = np.random.random((M, N)) # 随机生成的 [M x N] 矩阵\n",
    "    B = np.random.random((N, P)) # 随机生成的 [N x P] 矩阵\n",
    "    \n",
    "    start = time()\n",
    "    A = cuda.to_device(A)\n",
    "    B = cuda.to_device(B)\n",
    "    C_gpu = cuda.device_array((M, P))\n",
    "\n",
    "    # 执行配置\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid_x = int(math.ceil(A.shape[0] / threads_per_block[0]))\n",
    "    blocks_per_grid_y = int(math.ceil(B.shape[1] / threads_per_block[1]))\n",
    "    blocksPerGrid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    # 启动核函数\n",
    "    matmul[blocksPerGrid, threads_per_block](A, B, C_gpu)\n",
    "\n",
    "    # 数据拷贝\n",
    "    C = C_gpu.copy_to_host()\n",
    "    cuda.synchronize()\n",
    "\n",
    "    print(\"gpu matmul time :\" + str(time() - start))\n",
    "\n",
    "    start = time()\n",
    "    C_cpu = np.empty((M, P), np.float128)\n",
    "    np.matmul(A, B, C_cpu)\n",
    "    print(\"cpu matmul time :\" + str(time() - start))\n",
    "\n",
    "    # 验证正确性\n",
    "    if np.allclose(C_cpu, C):\n",
    "        print(\"gpu result correct\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul time :0.48482823371887207\n",
      "matmul with shared memory time :0.577873945236206\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda, float32\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "# thread per block\n",
    "# 每个block有 BLOCK_SIZE x BLOCK_SIZE 个元素\n",
    "BLOCK_SIZE = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"  矩阵乘法 C = A * B\n",
    "    \"\"\"\n",
    "    row = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    col = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "    \n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_shared_memory(A, B, C):\n",
    "    \"\"\"\n",
    "    使用Shared Memory的矩阵乘法 C = A * B\n",
    "    \"\"\"\n",
    "    # 在Shared Memory中定义向量\n",
    "    # 向量可被整个Block的所有Thread共享\n",
    "    # 必须声明向量大小和数据类型\n",
    "    sA = cuda.shared.array(shape=(BLOCK_SIZE, BLOCK_SIZE), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(BLOCK_SIZE, BLOCK_SIZE), dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    row = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    col = cuda.threadIdx.y + cuda.blockDim.y * cuda.blockIdx.y\n",
    "    \n",
    "    if row >= C.shape[0] and col >= C.shape[1]:\n",
    "        # 当(x, y)越界时退出\n",
    "        return\n",
    "\n",
    "    tmp = 0.\n",
    "    # 以一个 BLOCK_SIZE x BLOCK_SIZE 为单位\n",
    "    for m in range(math.ceil(A.shape[1] / BLOCK_SIZE)):\n",
    "        sA[tx, ty] = A[row, ty + m * BLOCK_SIZE]\n",
    "        sB[tx, ty] = B[tx + m * BLOCK_SIZE, col]\n",
    "        # 线程同步，等待Block中所有Thread预加载结束\n",
    "        # 该函数会等待所有Thread执行完之后才执行下一步\n",
    "        cuda.syncthreads()\n",
    "        # 此时已经将A和B的子矩阵拷贝到了sA和sB\n",
    "\n",
    "        # 计算Shared Memory中的向量点积\n",
    "        # 直接从Shard Memory中读取数据的延迟很低\n",
    "        for n in range(BLOCK_SIZE):\n",
    "            tmp += sA[tx, n] * sB[n, ty]\n",
    "\n",
    "        # 线程同步，等待Block中所有Thread计算结束\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    # 循环后得到每个BLOCK的点积之和\n",
    "    C[row, col] = tmp\n",
    "\n",
    "def main():\n",
    "    # 初始化矩阵\n",
    "    M = 6000\n",
    "    N = 4800\n",
    "    P = 4000\n",
    "    A = np.random.random((M, N)) # 随机生成的 [M x N] 矩阵\n",
    "    B = np.random.random((N, P)) # 随机生成的 [N x P] 矩阵\n",
    "\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    C_device = cuda.device_array((M, P)) # [M x P] 矩阵\n",
    "\n",
    "    # 执行配置\n",
    "    threads_per_block = (BLOCK_SIZE, BLOCK_SIZE)\n",
    "    blocks_per_grid_x = int(math.ceil(A.shape[0] / BLOCK_SIZE))\n",
    "    blocks_per_grid_y = int(math.ceil(B.shape[1] / BLOCK_SIZE))\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    start = time()\n",
    "    matmul[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    cuda.synchronize()\n",
    "    print(\"matmul time :\" + str(time() - start))\n",
    "\n",
    "    start = time()\n",
    "    matmul_shared_memory[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    cuda.synchronize()\n",
    "    print(\"matmul with shared memory time :\" + str(time() - start))\n",
    "    C = C_device.copy_to_host()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pursue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
