{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhiyuan/miniconda3/envs/pursue/lib/python3.8/site-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu vector add time 1.1054105758666992\n",
      "cpu vector add time 0.0317842960357666\n",
      "result correct\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_add(a, b, result, n):\n",
    "    # a, b为输入向量，result为输出向量;所有向量都是n维;得到当前thread的索引\n",
    "    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    if idx < n:\n",
    "        result[idx] = a[idx] + b[idx]\n",
    "\n",
    "def main():\n",
    "    # 初始化两个2千万维的向量，作为参数传递给核函数\n",
    "    n = 20000000\n",
    "    x = np.arange(n).astype(np.int32)\n",
    "    y = 2 * x\n",
    "    gpu_result = np.zeros(n)\n",
    "    cpu_result = np.zeros(n)\n",
    "\n",
    "\n",
    "    # CUDA执行配置\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = math.ceil(n / threads_per_block)\n",
    "\n",
    "    start = time()\n",
    "    gpu_add[blocks_per_grid, threads_per_block](x, y, gpu_result, n)\n",
    "    cuda.synchronize()\n",
    "    print(\"gpu vector add time \" + str(time() - start))\n",
    "    start = time()\n",
    "    cpu_result = np.add(x, y)\n",
    "    print(\"cpu vector add time \" + str(time() - start))\n",
    "\n",
    "    if (np.array_equal(cpu_result, gpu_result)):\n",
    "        print(\"result correct\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu vector add time 0.08129477500915527\n",
      "cpu vector add time 0.1456151008605957\n",
      "result correct!\n"
     ]
    }
   ],
   "source": [
    "# 继续优化这个程序，告知GPU哪些数据需要拷贝到设备，哪些需要拷贝回主机\n",
    "\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_add(a, b, result, n):\n",
    "    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    if idx < n :\n",
    "        result[idx] = a[idx] + b[idx]\n",
    "\n",
    "def main():\n",
    "    n = 20000000\n",
    "    x = np.arange(n).astype(np.int32)\n",
    "    y = 2 * x\n",
    "\n",
    "    # 拷贝数据到设备端\n",
    "    x_device = cuda.to_device(x)\n",
    "    y_device = cuda.to_device(y)\n",
    "    # 在显卡设备上初始化一块用于存放GPU计算结果的空间\n",
    "    gpu_result = cuda.device_array(n)\n",
    "    cpu_result = np.empty(n)\n",
    "\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = math.ceil(n / threads_per_block)\n",
    "    start = time()\n",
    "    gpu_add[blocks_per_grid, threads_per_block](x_device, y_device, gpu_result, n)\n",
    "    cuda.synchronize()\n",
    "    print(\"gpu vector add time \" + str(time() - start))\n",
    "    start = time()\n",
    "    cpu_result = np.add(x, y)\n",
    "    print(\"cpu vector add time \" + str(time() - start))\n",
    "\n",
    "    if (np.array_equal(cpu_result, gpu_result.copy_to_host())):\n",
    "        print(\"result correct!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pursue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
