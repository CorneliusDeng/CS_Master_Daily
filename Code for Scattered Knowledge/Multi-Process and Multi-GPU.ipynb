{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单进程指定GPU进行计算\n",
    "\n",
    "在Pytorch框架下，单进程指定GPU进行计算核心语句：`device = torch.device(“cuda:gpu编号”)`\n",
    "例如： `device = torch.device(“cuda:1”)`\n",
    "\n",
    "GPU编号、显存大小、当前正在使用GPU的进程都可通过命令行语句 `nvidia-smi` 查看\n",
    "\n",
    "此外，还可以通过 `CUDA_VISIBLE_DEVICES` 变量指定GPU\n",
    "\n",
    "对于模型（类），使用 `.to(device)` 语句来将模型的所有参数、缓存放到指定的GPU上进行计算\n",
    "\n",
    "注：只要显存足够，多个terminal分别执行多个进程，可以指定在同一个GPU，也可以指定在不同的GPU，即可实现多进程并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Demo, 单进程指定GPU计算\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "# 1. 检查可用的GPU数量和编号\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"系统中有 {device_count} 块可用的GPU。\")\n",
    "if device_count > 0:\n",
    "    print(\"它们的编号是：\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}\")\n",
    "else:\n",
    "    print(\"没有可用的GPU。\")\n",
    "\n",
    "# 2. 随机生成两个可乘矩阵，并在指定的GPU上进行乘法运算\n",
    "if device_count > 0:\n",
    "    # 让用户输入想要使用的GPU编号\n",
    "    gpu_id = int(input(\"请输入你想要使用的GPU编号(0到{}):\".format(device_count - 1)))\n",
    "    \n",
    "    # 检查输入的GPU编号是否有效\n",
    "    if gpu_id < 0 or gpu_id >= device_count:\n",
    "        print(\"输入的GPU编号无效。\")\n",
    "    else:\n",
    "        # 指定设备\n",
    "        device = torch.device(f\"cuda:{gpu_id}\")\n",
    "        \n",
    "        # 在指定的GPU上创建两个随机的可乘矩阵\n",
    "        matrix1 = torch.rand(3, 3, device=device)\n",
    "        matrix2 = torch.rand(3, 4, device=device)\n",
    "        \n",
    "        print(f\"在GPU {gpu_id} 上的随机矩阵1:\\n{matrix1}\")\n",
    "        print(f\"在GPU {gpu_id} 上的随机矩阵2:\\n{matrix2}\")\n",
    "        \n",
    "        # 执行矩阵乘法\n",
    "        result = torch.matmul(matrix1, matrix2)\n",
    "        \n",
    "        print(f\"矩阵乘法的结果:\\n{result}\")\n",
    "else:\n",
    "    print(\"由于没有可用的GPU，无法执行GPU上的矩阵乘法。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Demo, 指定进程的GPU可见范围\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=\"1\"           Only device 1 will be seen\n",
    "CUDA_VISIBLE_DEVICES=\"0,1\"         Devices 0 and 1 will be visible\n",
    "CUDA_VISIBLE_DEVICES=\"0,1\"         Same as above, quotation marks are optional\n",
    "CUDA_VISIBLE_DEVICES=\"0,2,3\"       Devices 0, 2, 3 will be visible; device 1 is masked\n",
    "CUDA_VISIBLE_DEVICES=\"\"            No GPU will be visible\n",
    "'''\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置环境变量，不使用GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# 指定设备为GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"当前使用的设备:\", device)\n",
    "\n",
    "# 在指定的GPU上创建两个可以做乘法的矩阵\n",
    "matrix1 = torch.rand(2, 3, device=device)\n",
    "matrix2 = torch.rand(3, 2, device=device)\n",
    "\n",
    "# 打印生成的矩阵\n",
    "print(\"Matrix 1:\")\n",
    "print(matrix1)\n",
    "print(\"Matrix 2:\")\n",
    "print(matrix2)\n",
    "\n",
    "# 进行矩阵乘法\n",
    "result = torch.matmul(matrix1, matrix2)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Result of matrix multiplication:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Demo, 把模型放到GPU上做计算\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个简单的神经网络模型\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5) \n",
    "        # self.to(torch.device(\"cuda:1\")) # 定义网络时就指定设备\n",
    "\n",
    "    # 前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.relu(x) \n",
    "\n",
    "# 指定GPU设备\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleNet()\n",
    "\n",
    "# 将模型移动到指定的设备\n",
    "model.to(device)\n",
    "\n",
    "# 创建一个随机数据张量，模拟输入数据\n",
    "input_data = torch.randn(5, 10) \n",
    "\n",
    "# 将输入数据也移动到指定的设备，数据与模型在同一设备上可以提高计算效率\n",
    "input_data = input_data.to(device)\n",
    "\n",
    "# 前向传播，获取模型输出\n",
    "output = model(input_data)\n",
    "\n",
    "# 打印输出结果\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单进程跨GPU做计算\n",
    "\n",
    "在深度学习和分布式计算领域，DP 通常指的是 DataParallel。DataParallel 是一种将计算任务在多个 GPU 上并行执行的方法。它在单机多卡环境中非常有用，可以在多个 GPU 上分摊工作负载，从而加快训练速度。\n",
    "\n",
    "torch.nn.DataParallel 是 PyTorch 中的一个工具，可以让模型在多个 GPU 上并行运行。它通过将输入批次拆分成多个子批次，每个子批次发送到不同的 GPU 上，并行执行前向传播和反向传播，然后将每个 GPU 上的梯度聚合到主 GPU 上进行参数更新。\n",
    "\n",
    "使用 DataParallel 的基本步骤\n",
    "- 定义模型: 创建神经网络模型\n",
    "- 包装模型: 使用 torch.nn.DataParallel 包装模型\n",
    "- 将模型和数据迁移到 GPU: 使用 .to(device) 将模型和输入数据迁移到合适的设备上\n",
    "- 训练模型: 按照常规方式训练模型\n",
    "\n",
    "单机多卡训练策略\n",
    "- 数据拆分，模型不拆分\n",
    "- 数据不拆分，模型拆分\n",
    "- 数据拆分，模型拆分\n",
    "\n",
    "DataParallel 的局限性\n",
    "- 数据并行粒度: DataParallel 进行的是数据并行操作，每个 GPU 处理一部分数据批次。由于其自动分配负载，这可能导致 GPU 利用率不均衡，尤其是在有计算负载差异的情况下\n",
    "- 单节点限制: DataParallel 主要用于单节点多 GPU，即单机多卡。如果需要跨节点并行（分布式训练，多机多卡），应该考虑使用 torch.nn.parallel.DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0051126438193023205\n",
      "Epoch 2, Loss: 0.003030891064554453\n",
      "Epoch 3, Loss: 0.001992578152567148\n",
      "Epoch 4, Loss: 0.0015859566628932953\n",
      "Epoch 5, Loss: 0.001422481844201684\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Demo, 单机多卡, 单进程跨GPU计算\n",
    "\n",
    "最简单高效的策略: 数据拆分, 模型不拆分\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 设置环境变量，指定使用的GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# 定义设备\n",
    "globalDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义一个简单的CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, 3, 1)\n",
    "        self.fc = nn.Linear(16 * 26 * 26, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "cnn = CNN().to(globalDevice)\n",
    "\n",
    "# 检查GPU数量并设置DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    net = nn.DataParallel(cnn)\n",
    "else:\n",
    "    print(\"Using single GPU or CPU\")\n",
    "    net = cnn\n",
    "\n",
    "# 定义数据集和数据加载器\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(3, 28, 28), torch.tensor(1)\n",
    "\n",
    "dataset = SimpleDataset(1000)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=20)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 简单的训练过程\n",
    "for epoch in range(5):\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(globalDevice), labels.to(globalDevice) # 此处可指定GPU以实现手动分配负载\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pseudocode，策略一：数据拆分，模型不拆分\n",
    "\n",
    "在这种策略中，将数据拆分成多个批次，每个批次在一个GPU上进行处理。模型不会拆分，而是复制到每个GPU上\n",
    "'''\n",
    "\n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader, Dataset  \n",
    "from torch.nn.parallel import DataParallel  \n",
    "\n",
    "# 假设我们有一个自定义的数据集和模型  \n",
    "class MyDataset(Dataset):  \n",
    "    # 实现__len__和__getitem__方法  \n",
    "    pass  \n",
    "class MyModel(nn.Module):  \n",
    "    # 定义模型结构  \n",
    "    pass  \n",
    "\n",
    "# 初始化数据集和模型  \n",
    "dataset = MyDataset()  \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)  \n",
    "model = MyModel()  \n",
    "\n",
    "# 检查GPU数量  \n",
    "device_ids = list(range(torch.cuda.device_count()))  \n",
    "model = DataParallel(model, device_ids=device_ids).to(device_ids[0])  \n",
    "\n",
    "# 定义损失函数和优化器  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "# 训练循环  \n",
    "for epoch in range(num_epochs):  \n",
    "    for inputs, labels in dataloader:  \n",
    "        inputs, labels = inputs.to(device_ids[0]), labels.to(device_ids[0])  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pseudocode，策略二：数据不拆分，模型拆分\n",
    "\n",
    "在这种策略中，整个数据集在每个GPU上都会有一份副本，但模型会被拆分成多个部分，每个部分在一个GPU上运行。这种策略通常不常见，因为数据复制会消耗大量内存，而且模型拆分也可能会导致通信开销增加\n",
    "'''\n",
    "# 假设我们有一个可以拆分的模型（例如，具有多个子网络的模型）  \n",
    "class SplitModel(nn.Module):  \n",
    "    def __init__(self):  \n",
    "        super(SplitModel, self).__init__()  \n",
    "        self.subnet1 = nn.Sequential(...)  # 定义子网络1  \n",
    "        self.subnet2 = nn.Sequential(...)  # 定义子网络2  \n",
    "        # ... 其他子网络 ...  \n",
    "    def forward(self, x):  \n",
    "        # 前向传播逻辑，可能涉及跨多个设备的通信和数据传输  \n",
    "        pass  \n",
    "\n",
    "# 初始化模型和数据集（这里不实际拆分数据）  \n",
    "model = SplitModel()  \n",
    "dataset = MyDataset()  \n",
    "\n",
    "# 将模型的每个子网络分配到一个GPU上  \n",
    "model.subnet1 = model.subnet1.to('cuda:0')  \n",
    "model.subnet2 = model.subnet2.to('cuda:1')  \n",
    "\n",
    "# ... 其他子网络 ...  \n",
    "\n",
    "# 训练循环（这里省略了数据加载和批处理，因为数据没有拆分）  \n",
    "for epoch in range(num_epochs):  \n",
    "    inputs, labels = ...  # 加载数据  \n",
    "    inputs = inputs.to('cuda:0')  # 假设输入数据首先被送到第一个GPU上  \n",
    "    optimizer.zero_grad()  \n",
    "    outputs = model(inputs)  # 前向传播可能涉及跨多个GPU的通信  \n",
    "    loss = criterion(outputs, labels)  \n",
    "    loss.backward()  \n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pseudocode，策略三：数据拆分，模型拆分\n",
    "\n",
    "在这种策略中，同时使用数据并行和模型并行。数据被拆分成多个批次，每个批次在不同的GPU上进行处理，同时模型也被拆分成多个部分，每个部分在不同的GPU上运行。这通常用于非常大的模型，单个GPU无法容纳整个模型的情况\n",
    "'''\n",
    "\n",
    "import torch  \n",
    "import torch.distributed as dist  \n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler  \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP  \n",
    "\n",
    "# 自定义数据集和模型  \n",
    "class MyDataset(Dataset):  \n",
    "    # 实现__len__和__getitem__方法  \n",
    "    pass  \n",
    "class MyModel(nn.Module):  \n",
    "    # 定义模型结构，可能需要考虑如何拆分模型  \n",
    "    pass  \n",
    "\n",
    "# 初始化分布式环境  \n",
    "dist.init_process_group(backend='nccl', init_method='tcp://localhost:23456', rank=0, world_size=torch.cuda.device_count())  \n",
    "\n",
    "# 初始化数据集和模型  \n",
    "dataset = MyDataset()  \n",
    "sampler = DistributedSampler(dataset)  \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, sampler=sampler)  \n",
    "model = MyModel()  \n",
    "\n",
    "#拆分模型（这通常需要根据模型的具体结构来手动完成。例如，如果模型有两个主要部分，可以将它们分别放到不同的设备上  \n",
    "model_part1 = model.part1.to('cuda:0')  \n",
    "model_part2 = model.part2.to('cuda:1')  \n",
    "\n",
    "# 使用DistributedDataParallel包装模型  \n",
    "model = DDP(model, device_ids=[torch.cuda.current_device()])\n",
    "\n",
    "# 定义损失函数和优化器  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "# 训练循环  \n",
    "for epoch in range(num_epochs):  \n",
    "    for inputs, labels in dataloader:  \n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)  \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)  \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "# 销毁分布式进程组  \n",
    "dist.destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATSC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
