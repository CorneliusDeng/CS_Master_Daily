{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Patch Partition\n",
    "划分实质就是维度的变换\n",
    "'''\n",
    "\n",
    "num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "# (b, c=3, h, w)->(b, n_patches, patch_size**2 * c)\n",
    "patches = x.view(\n",
    "    b, c,\n",
    "    h // self.patch_h, self.patch_h, \n",
    "    w // self.patch_w, self.patch_w\n",
    ").permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Masking\n",
    "根据预设的 mask 比例采用服从均匀分布的策略随机采样一批 patches 喂给 Encoder，剩下的就 mask 掉\n",
    "'''\n",
    "\n",
    "# 根据 mask 比例计算需要 mask 掉的 patch 数量\n",
    "# num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "num_masked = int(self.mask_ratio * num_patches)\n",
    "\n",
    "# Shuffle:生成对应 patch 的随机索引\n",
    "# torch.rand() 服从均匀分布(normal distribution)\n",
    "# torch.rand() 只是生成随机数，argsort() 是为了获得成索引\n",
    "# (b, n_patches)\n",
    "shuffle_indices = torch.rand(b, num_patches, device=device).argsort()\n",
    "# mask 和 unmasked patches 对应的索引\n",
    "mask_ind, unmask_ind = shuffle_indices[:, :num_masked], shuffle_indices[:, num_masked:]\n",
    "\n",
    "# 对应 batch 维度的索引：(b,1)\n",
    "batch_ind = torch.arange(b, device=device).unsqueeze(-1)\n",
    "# 利用先前生成的索引对 patches 进行采样，分为 mask 和 unmasked 两组\n",
    "mask_patches, unmask_patches = patches[batch_ind, mask_ind], patches[batch_ind, unmask_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder 对 un-masked 的 patches 进行编码\n",
    "先对 un-masked patches 进行 emebdding 转换成 tokens，并且加上 position embeddings，从而为它们添加位置信息，然后才能是真正的编码过程。至于编码过程，实质上就是扔给 Transformer 玩 😄(query 和 key 玩一玩，玩出个 attention 后再和 value 一起玩)\n",
    "'''\n",
    "\n",
    "# 将 patches 通过 emebdding 转换成 tokens\n",
    "unmask_tokens = self.encoder.patch_embed(unmask_patches)\n",
    "# 为 tokens 加入 position embeddings \n",
    "# 注意这里索引加1是因为索引0对应 ViT 的 cls_token\n",
    "unmask_tokens += self.encoder.pos_embed.repeat(b, 1, 1)[batch_ind, unmask_ind + 1]\n",
    "# 真正的编码过程\n",
    "encoded_tokens = self.encoder.transformer(unmask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decoder\n",
    "将编码后的 tokens 和 添加了位置信息后的 mask tokens 按原先对应 patches 的次序拼起来，然后喂给 Decoder 解码\n",
    "若编码后的 tokens 维度若与 Decoder 要求的输入维度不一致，需要使用 linear projection 进行转换\n",
    "'''\n",
    "\n",
    "# 对编码后的 tokens 维度进行转换，从而符合 Decoder 要求的输入维度\n",
    "enc_to_dec_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "# 由于 mask token 实质上只有1个，因此要对其进行扩展，从而和 masked patches 一一对应\n",
    "# (decoder_dim)->(b, n_masked, decoder_dim)\n",
    "mask_tokens = self.mask_embed[None, None, :].repeat(b, num_masked, 1)\n",
    "# 为 mask tokens 加入位置信息\n",
    "mask_tokens += self.decoder_pos_embed(mask_ind)\n",
    "\n",
    "# 将 mask tokens 与 编码后的 tokens 拼接起来\n",
    "# (b, n_patches, decoder_dim)\n",
    "concat_tokens = torch.cat([mask_tokens, enc_to_dec_tokens], dim=1)\n",
    "# Un-shuffle：恢复原先 patches 的次序\n",
    "dec_input_tokens = torch.empty_like(concat_tokens, device=device)\n",
    "dec_input_tokens[batch_ind, shuffle_indices] = concat_tokens\n",
    "# 将全量 tokens 喂给 Decoder 解码\n",
    "decoded_tokens = self.decoder(dec_input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Computation\n",
    "取出解码后的 mask tokens 送入头部进行像素值预测，然后将预测结果和 masked patches 比较，计算 MSE loss\n",
    "'''\n",
    "\n",
    "# 取出解码后的 mask tokens\n",
    "dec_mask_tokens = decoded_tokens[batch_ind, mask_ind, :]\n",
    "# 预测 masked patches 的像素值\n",
    "# (b, n_masked, n_pixels_per_patch=patch_size**2 x c)\n",
    "pred_mask_pixel_values = self.head(dec_mask_tokens)\n",
    "# loss 计算\n",
    "loss = F.mse_loss(pred_mask_pixel_values, mask_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encoder ViT & Decoder Transformer\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def to_pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, net):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.net = net\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(self.norm(x), **kwargs)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dim_per_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = dim_per_head ** -0.5\n",
    "\n",
    "        inner_dim = dim_per_head * num_heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        project_out = not (num_heads == 1 and dim_per_head == dim)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, l, d = x.shape\n",
    "\n",
    "        '''i. QKV projection'''\n",
    "        # (b,l,dim_all_heads x 3)\n",
    "        qkv = self.to_qkv(x)\n",
    "        # (3,b,num_heads,l,dim_per_head)\n",
    "        qkv = qkv.view(b, l, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4).contiguous()\n",
    "        # 3 x (1,b,num_heads,l,dim_per_head)\n",
    "        q, k, v = qkv.chunk(3)\n",
    "        q, k, v = q.squeeze(0), k.squeeze(0), v.squeeze(0)\n",
    "\n",
    "        '''ii. Attention computation'''\n",
    "        attn = self.attend(\n",
    "            torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        )\n",
    "\n",
    "        '''iii. Put attention on Value & reshape'''\n",
    "        # (b,num_heads,l,dim_per_head)\n",
    "        z = torch.matmul(attn, v)\n",
    "        # (b,num_heads,l,dim_per_head)->(b,l,num_heads,dim_per_head)->(b,l,dim_all_heads)\n",
    "        z = z.transpose(1, 2).reshape(b, l, -1)\n",
    "        # assert z.size(-1) == q.size(-1) * self.num_heads\n",
    "\n",
    "        '''iv. Project out'''\n",
    "        # (b,l,dim_all_heads)->(b,l,dim)\n",
    "        out = self.out(z)\n",
    "        # assert out.size(-1) == d\n",
    "\n",
    "        return out\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, depth=6, num_heads=8, dim_per_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, SelfAttention(dim, num_heads=num_heads, dim_per_head=dim_per_head, dropout=dropout)),\n",
    "                PreNorm(dim, FFN(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for norm_attn, norm_ffn in self.layers:\n",
    "            x = x + norm_attn(x)\n",
    "            x = x + norm_ffn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self, image_size, patch_size, \n",
    "        num_classes=1000, dim=1024, depth=6, num_heads=8, mlp_dim=2048,\n",
    "        pool='cls', channels=3, dim_per_head=64, dropout=0., embed_dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        img_h, img_w = to_pair(image_size)\n",
    "        self.patch_h, self.patch_w = to_pair(patch_size)\n",
    "        assert not img_h % self.patch_h and not img_w % self.patch_w, \\\n",
    "            f'Image dimensions ({img_h},{img_w}) must be divisible by the patch size ({self.patch_h},{self.patch_w}).'\n",
    "        num_patches = (img_h // self.patch_h) * (img_w // self.patch_w)\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, f'pool type must be either cls (cls token) or mean (mean pooling), got: {pool}'\n",
    "        \n",
    "        patch_dim = channels * self.patch_h * self.patch_w\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        # Add 1 for cls_token\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=embed_dropout)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            dim, mlp_dim, depth=depth, num_heads=num_heads,\n",
    "            dim_per_head=dim_per_head, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.pool = pool\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, img_h, img_w = x.shape\n",
    "        assert not img_h % self.patch_h and not img_w % self.patch_w, \\\n",
    "            f'Input image dimensions ({img_h},{img_w}) must be divisible by the patch size ({self.patch_h},{self.patch_w}).'\n",
    "        \n",
    "        '''i. Patch partition'''\n",
    "        num_patches = (img_h // self.patch_h) * (img_w // self.patch_w)\n",
    "        # (b,c,h,w)->(b,n_patches,patch_h*patch_w*c)\n",
    "        patches = x.view(\n",
    "            b, c, \n",
    "            img_h // self.patch_h, self.patch_h, \n",
    "            img_w // self.patch_w, self.patch_w\n",
    "        ).permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n",
    "\n",
    "        '''ii. Patch embedding'''\n",
    "        # (b,n_patches,dim)\n",
    "        tokens = self.patch_embed(patches)\n",
    "        # (b,n_patches+1,dim)\n",
    "        tokens = torch.cat([self.cls_token.repeat(b, 1, 1), tokens], dim=1)\n",
    "        tokens += self.pos_embed[:, :(num_patches + 1)]\n",
    "        tokens = self.dropout(tokens)\n",
    "\n",
    "        '''iii. Transformer Encoding'''\n",
    "        enc_tokens = self.transformer(tokens)\n",
    "\n",
    "        '''iv. Pooling'''\n",
    "        # (b,dim)\n",
    "        pooled = enc_tokens[:, 0] if self.pool == 'cls' else enc_tokens.mean(dim=1)\n",
    "\n",
    "        '''v. Classification'''\n",
    "        # (b,n_classes)\n",
    "        logits = self.mlp_head(pooled)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reconstruction(Inference)\n",
    "'''\n",
    "\n",
    "@torch.no_grad\n",
    "def predict(self, x):\n",
    "    self.eval()\n",
    "\n",
    "    device = x.device\n",
    "    b, c, h, w = x.shape\n",
    "\n",
    "    '''i. Patch partition'''\n",
    "\n",
    "    num_patches = (h // self.patch_h) * (w // self.patch_w)\n",
    "    # (b, c=3, h, w)->(b, n_patches, patch_size**2*c)\n",
    "    patches = x.view(\n",
    "        b, c,\n",
    "        h // self.patch_h, self.patch_h, \n",
    "        w // self.patch_w, self.patch_w\n",
    "    ).permute(0, 2, 4, 3, 5, 1).reshape(b, num_patches, -1)\n",
    "\n",
    "    '''ii. Divide into masked & un-masked groups'''\n",
    "\n",
    "    num_masked = int(self.mask_ratio * num_patches)\n",
    "\n",
    "    # Shuffle\n",
    "    # (b, n_patches)\n",
    "    shuffle_indices = torch.rand(b, num_patches, device=device).argsort()\n",
    "    mask_ind, unmask_ind = shuffle_indices[:, :num_masked], shuffle_indices[:, num_masked:]\n",
    "\n",
    "    # (b, 1)\n",
    "    batch_ind = torch.arange(b, device=device).unsqueeze(-1)\n",
    "    mask_patches, unmask_patches = patches[batch_ind, mask_ind], patches[batch_ind, unmask_ind]\n",
    "\n",
    "    '''iii. Encode'''\n",
    "\n",
    "    unmask_tokens = self.encoder.patch_embed(unmask_patches)\n",
    "    # Add position embeddings\n",
    "    unmask_tokens += self.encoder.pos_embed.repeat(b, 1, 1)[batch_ind, unmask_ind + 1]\n",
    "    encoded_tokens = self.encoder.transformer(unmask_tokens)\n",
    "\n",
    "    '''iv. Decode'''\n",
    "    \n",
    "    enc_to_dec_tokens = self.enc_to_dec(encoded_tokens)\n",
    "\n",
    "    # (decoder_dim)->(b, n_masked, decoder_dim)\n",
    "    mask_tokens = self.mask_embed[None, None, :].repeat(b, num_masked, 1)\n",
    "    # Add position embeddings\n",
    "    mask_tokens += self.decoder_pos_embed(mask_ind)\n",
    "\n",
    "    # (b, n_patches, decoder_dim)\n",
    "    concat_tokens = torch.cat([mask_tokens, enc_to_dec_tokens], dim=1)\n",
    "    # dec_input_tokens = concat_tokens\n",
    "    dec_input_tokens = torch.empty_like(concat_tokens, device=device)\n",
    "    # Un-shuffle\n",
    "    dec_input_tokens[batch_ind, shuffle_indices] = concat_tokens\n",
    "    decoded_tokens = self.decoder(dec_input_tokens)\n",
    "\n",
    "    '''v. Mask pixel Prediction'''\n",
    "\n",
    "    dec_mask_tokens = decoded_tokens[batch_ind, mask_ind, :]\n",
    "    # (b, n_masked, n_pixels_per_patch=patch_size**2 x c)\n",
    "    pred_mask_pixel_values = self.head(dec_mask_tokens)\n",
    "\n",
    "    # 比较下预测值和真实值\n",
    "    mse_per_patch = (pred_mask_pixel_values - mask_patches).abs().mean(dim=-1)\n",
    "    mse_all_patches = mse_per_patch.mean()\n",
    "\n",
    "    print(f'mse per (masked)patch: {mse_per_patch} mse all (masked)patches: {mse_all_patches} total {num_masked} masked patches')\n",
    "    print(f'all close: {torch.allclose(pred_mask_pixel_values, mask_patches, rtol=1e-1, atol=1e-1)}')\n",
    "        \n",
    "    '''vi. Reconstruction'''\n",
    "\n",
    "    recons_patches = patches.detach()\n",
    "    # Un-shuffle (b, n_patches, patch_size**2 * c)\n",
    "    recons_patches[batch_ind, mask_ind] = pred_mask_pixel_values\n",
    "    # 模型重建的效果图\n",
    "    # Reshape back to image \n",
    "    # (b, n_patches, patch_size**2 * c)->(b, c, h, w)\n",
    "    recons_img = recons_patches.view(\n",
    "        b, h // self.patch_h, w // self.patch_w, \n",
    "        self.patch_h, self.patch_w, c\n",
    "    ).permute(0, 5, 1, 3, 2, 4).reshape(b, c, h, w)\n",
    "\n",
    "    mask_patches = torch.randn_like(mask_patches, device=mask_patches.device)\n",
    "    # mask 效果图\n",
    "    patches[batch_ind, mask_ind] = mask_patches\n",
    "    patches_to_img = patches.view(\n",
    "        b, h // self.patch_h, w // self.patch_w, \n",
    "        self.patch_h, self.patch_w, c\n",
    "    ).permute(0, 5, 1, 3, 2, 4).reshape(b, c, h, w)\n",
    "\n",
    "    return recons_img, patches_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "simple pipline\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 读入图像并缩放到适合模型输入的尺寸\n",
    "from PIL import Image\n",
    "\n",
    "img_raw = Image.open('test.jpg')\n",
    "h, w = img_raw.height, img_raw.width\n",
    "ratio = h / w\n",
    "print(f\"image hxw: {h} x {w} mode: {img_raw.mode}\")\n",
    "\n",
    "img_size, patch_size = (224, 224), (16, 16)\n",
    "img = img_raw.resize(img_size)\n",
    "rh, rw = img.height, img.width\n",
    "print(f'resized image hxw: {rh} x {rw} mode: {img.mode}')\n",
    "img.save('resized_test.jpg')\n",
    "\n",
    "# 将图像转换成张量\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "img_ts = ToTensor()(img).unsqueeze(0).to(device)\n",
    "print(f\"input tensor shape: {img_ts.shape} dtype: {img_ts.dtype} device: {img_ts.device}\")\n",
    "\n",
    "# 实例化模型并加载训练好的权重\n",
    "encoder = ViT(img_size, patch_size, dim=512, mlp_dim=1024, dim_per_head=64)\n",
    "decoder_dim = 512\n",
    "mae = MAE(encoder, decoder_dim, decoder_depth=6)\n",
    "weight = torch.load('mae.pth', map_location='cpu')\n",
    "mae.to(device)\n",
    "\n",
    "# 推理\n",
    "# 模型重建的效果图，mask 效果图\n",
    "recons_img_ts, masked_img_ts = mae.predict(img_ts)\n",
    "recons_img_ts, masked_img_ts = recons_img_ts.cpu().squeeze(0), masked_img_ts.cpu().squeeze(0)\n",
    "\n",
    "# 将结果保存下来以便和原图比较\n",
    "recons_img = ToPILImage()(recons_img_ts)\n",
    "recons_img.save('recons_test.jpg')\n",
    "\n",
    "masked_img = ToPILImage()(masked_img_ts)\n",
    "masked_img.save('masked_test.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
